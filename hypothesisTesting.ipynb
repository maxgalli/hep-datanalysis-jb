{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypotheses Testing {#ChapterHypothesisTesting}\n",
    "==================\n",
    "\n",
    "In the previous chapters we used experimental data to estimate\n",
    "parameters. Here we will use data to test hypotheses. A typical example\n",
    "is to test whether the data are compatible with the theoretical\n",
    "prediction or to choose among different hypothesis which one best\n",
    "represents the data.\n",
    "\n",
    "Hypotheses and tests statistics\n",
    "-------------------------------\n",
    "\n",
    "Let's begin by defining some terminology that we will need in the\n",
    "following. The goal of a statistical test is to make a statement about\n",
    "how well the observed data stand in agreement (accept) or not (reject)\n",
    "with a given predicted distribution, i.e. a hypothesis. The typical\n",
    "naming for the hypothesis under test is the **null hypothesis** or\n",
    "$H_{0}$. The **alternative hypothesis**, if there is one, is usually\n",
    "called $H_{1}$. If there are several alternative hypotheses they are\n",
    "labeled $H_{1}, H_{2}, \\ldots$ The hypothesis can be **simple** if the\n",
    "p.d.f. of the random variable under test is completely specified (e.g.\n",
    "the data are drawn from a Gaussian p.d.f. with specified normalization,\n",
    "mean ad width) or **composite** if at least one of the parameters is not\n",
    "specified (e.g. the data are drawn from a Poisson with mean $>$3).\\\n",
    "In order to tell in a quantitative way what it means to *test a\n",
    "hypothesis* we need to build a function of the measured variables\n",
    "$\\vec{x}$, called *test statistic* $t(\\vec{x})$. If we build it in a\n",
    "clever way, the test statistic will be distributed differently depending\n",
    "on the hypothesis under test: $g(t(\\vec{x})|H_0)$ or\n",
    "$g(t(\\vec{x})|H_1)$. This pedantic notation is used here to stress that\n",
    "the test statistic is a function of the data and that it is the\n",
    "distribution of the test statistic values that is different under the\n",
    "different hypotheses (the lighter notation $g(t|H_i)$ will be used from\n",
    "now on). Comparing the value of the test statistic computed on the\n",
    "actual data, with the value(s) obtained computing it under different\n",
    "hypotheses, we can quantitatively state the level of agreement. That's\n",
    "the general idea. The way this is implemented in practice will be\n",
    "explained in the next sections.\\\n",
    "The test statistic can be any function of the data: it can be a\n",
    "multidimensional vector $\\vec{t}(\\vec{x})$ or a single real number\n",
    "$t(\\vec{x})$. Even the data themselves $\\{\\vec{x}\\}$ can be used as a\n",
    "test statistic. Collapsing all the information about the data into a\n",
    "single meaningful variable is particularly helpful in visualizing the\n",
    "test statistic and the separation between the two hypothesis. There is\n",
    "no general rule about the choice of the test statistic. The specific\n",
    "choice will depend on the particular case at hand. Different test\n",
    "statistic will give in general different results and it is up to the\n",
    "physicist to decide which is the most appropriate for the specific\n",
    "problem.\\\n",
    "\\\n",
    "**Example**: In order to better understand the terminology we can use a\n",
    "specific example based on particle identification. The average specific\n",
    "ionization $dE/dx$ of two charged particle with the same momentum\n",
    "passing through matter will be different depending on their masses (see\n",
    "Fig. [1.2](#fig:dEdx){reference-type=\"ref\" reference=\"fig:dEdx\"},\n",
    "$\\beta\\gamma = p / m$). Because of this dependence $dE/dx$ can be used\n",
    "as a particle identification tool to distinguish particle types. For\n",
    "example the ionization of electrons with momenta in the range of few GeV\n",
    "tend to be larger than the one of pions of the same momentum range.\n",
    "\n",
    "![Left: the specific ionization for some particle types (in green pions\n",
    "and in red electrons; other particles species are shown with different\n",
    "colors); Right: the projections of the left plot on the y-axis, i.e. the\n",
    "measured specific ionization for pions and\n",
    "electrons.](Section8Bilder/BetheBloch.png){#fig:dEdx\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "![Left: the specific ionization for some particle types (in green pions\n",
    "and in red electrons; other particles species are shown with different\n",
    "colors); Right: the projections of the left plot on the y-axis, i.e. the\n",
    "measured specific ionization for pions and\n",
    "electrons.](Section8Bilder/BetheBlochProjection.png){#fig:dEdx\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "If we want to distinguish an electron from a pion with a given momentum\n",
    "we can use the specific ionization itself as test statistic\n",
    "$t(\\vec{x}) = dE/dx$. This is a typical case where the data itself is\n",
    "used as test statistic. The test statistics will then be distributed\n",
    "differently under the two following hypotheses (see\n",
    "Fig. [1.2](#fig:dEdx){reference-type=\"ref\" reference=\"fig:dEdx\"} right):\n",
    "\n",
    "-   null hypothesis $g(t|H_0) = P\\left(\\frac{dE}{dx}|e^{\\pm}\\right)$\n",
    "\n",
    "-   alternative hypothesis\n",
    "    $g(t|H_1) = P\\left(\\frac{dE}{dx}|\\pi^{\\pm}\\right)$\n",
    "\n",
    "\\\n",
    "**Example**: When testing data for the presence of a signal, we define\n",
    "the null hypothesis as the \"background only\" hypothesis and the\n",
    "alternative hypothesis as the \"signal+background\" hypothesis.\\\n",
    "**Example**: Fig. [1.3](#fig:L3SM){reference-type=\"ref\"\n",
    "reference=\"fig:L3SM\"} shows the cross section\n",
    "$\\sigma(e^+e^-)\\to W^+W^-(\\gamma)$ measured by the L3 collaboration at\n",
    "different centre of mass energies. In this case the test statistic is\n",
    "the cross-section as a function of energy. The measured values are then\n",
    "compared with different theoretical models (different hypothesis). We\n",
    "haven't explained yet how to quantitatively accept or reject an\n",
    "hypothesis, but already at a naive level we can see that data clearly\n",
    "prefer one of the models.\n",
    "\n",
    "![Analysis of the cross section of $e^+e^-\\to W^+W^-(\\gamma)$ as a\n",
    "function of the centre of mass energy (L3 detector at LEP).\n",
    "](Section8Bilder/L3SM.png){#fig:L3SM width=\"\\\\textwidth\"}\n",
    "\n",
    "\\\n",
    "\\\n",
    "The p.d.f. describing the test statistic corresponding to a certain\n",
    "hypothesis $g(\\vec{t}|H)$ is usually built from a data set that has\n",
    "precisely the characteristic associated to that hypothesis. In the\n",
    "particle identification example discussed before, the expected data used\n",
    "to build the p.d.f. for the two hypotheses were pure sample of electrons\n",
    "and pure samples of pions. For example you can get a pure sample of\n",
    "electrons selecting tracks from photon conversions $\\gamma \\to e^+ e^-$\n",
    "and a pure sample of pions from the self-tagging decays of charmed\n",
    "mesons\n",
    "$D^{*+}\\to \\pi^+ D^0; D^0\\to K^- \\pi^+ (D^{*-}\\to \\pi^- D^0; D^0\\to K^+ \\pi^-)$\n",
    "(self-tagging means that by knowing the charge of the $\\pi$ in the first\n",
    "decay you can unambiguously assign the pion/kaon hypothesis to the\n",
    "positive/negative charge of the second decay). In other cases the p.d.f.\n",
    "are built from dedicated measurement (e.g. a test beam[^1]) or from\n",
    "Monte Carlo simulations.\\\n",
    "\n",
    "Significance, power, consistency and bias\n",
    "-----------------------------------------\n",
    "\n",
    "In order to accept or reject a null hypothesis we partition the space of\n",
    "the test statistics values into a **critical region** and its\n",
    "complementary the **acceptance region** (see\n",
    "Fig. [1.4](#fig:acceptReject){reference-type=\"ref\"\n",
    "reference=\"fig:acceptReject\"}). The value of the test statistics chosen\n",
    "to define the two regions is called **decision boundary**: \"$t_{cut}$\".\n",
    "If the value of the test statistic computed on the data sample under\n",
    "test falls in the rejection region, then the null hypothesis is\n",
    "discarded, otherwise it is accepted (or more precisely not rejected).\n",
    "\n",
    "![[\\[fig:acceptReject\\]]{#fig:acceptReject label=\"fig:acceptReject\"}A\n",
    "test statistic in red, where we defined an acceptance $x\\le 1$ and\n",
    "rejection region\n",
    "$x>1$.](Section8Bilder/acceptReject.png){#fig:acceptReject width=\"40%\"}\n",
    "\n",
    "Given a test statistic, some parameters are usually defined when sizing\n",
    "a rejection region. The first one is the **significance level** of the\n",
    "test (see Fig. [1.6](#AcceptRejectRegions){reference-type=\"ref\"\n",
    "reference=\"AcceptRejectRegions\"}). It is defined as the integral of the\n",
    "null hypothesis p.d.f. above the decision boundary:\n",
    "$$\\alpha = \\int_{t_{cut}}^\\infty g(t|H_0)dt$$ The probability $\\alpha$\n",
    "can be read as the probability to reject $H_0$ even if $H_0$ is in\n",
    "reality correct. This is called an **error of the first kind**.\\\n",
    "If we have an alternative hypothesis $H_1$, an error of the **second\n",
    "kind** occurs when $H_0$ is accepted but the correct hypothesis is in\n",
    "reality the alternative one $H_1$. The integral of the alternative\n",
    "hypothesis p.d.f. below $t_{cut}$ is called the **power of the test** to\n",
    "discriminate against the alternative hypothesis $H_1$ (see\n",
    "Fig. [1.6](#AcceptRejectRegions){reference-type=\"ref\"\n",
    "reference=\"AcceptRejectRegions\"}):\n",
    "$$\\beta = \\int_{-\\infty}^{t_{cut}} g(t|H_1)dt$$\n",
    "\n",
    "![Illustration of the acceptance and rejection region both for the\n",
    "hypothesis $H_{0}$ (on the left hand side) and the alternative $H_{1}$\n",
    "(on the right hand side) under the same choice of decision\n",
    "boundary.](Section8Bilder/nullHP.png){#AcceptRejectRegions\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "![Illustration of the acceptance and rejection region both for the\n",
    "hypothesis $H_{0}$ (on the left hand side) and the alternative $H_{1}$\n",
    "(on the right hand side) under the same choice of decision\n",
    "boundary.](Section8Bilder/alternativeHP.png){#AcceptRejectRegions\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "A good test has both $\\alpha$ and $\\beta$ small, which is equivalent to\n",
    "say high significance and high power. This means that $H_{0}$ and\n",
    "$H_{1}$ are *well separated*.\n",
    "Tab. [1.7](#tab:typeErrors){reference-type=\"ref\"\n",
    "reference=\"tab:typeErrors\"} summarize the different ways to mistakenly\n",
    "interpret the data in terms of errors of the first and second kind.\n",
    "While errors of the first type can be controlled by choosing $\\alpha$\n",
    "sufficiently small, errors of the second type, depending on the\n",
    "separation between the two hypothesis, are not as easily controllable.\n",
    "In HEP searches we typically speak of \"evidence\" when\n",
    "$\\alpha \\leq 3\\cdot 10^{-3}$, and of \"discovery\" when\n",
    "$\\alpha \\leq 3\\cdot 10^{-7}$ (corresponding to the probability outside 3\n",
    "$\\sigma$ and 5 $\\sigma$ respectively in a single sided tail Gaussian);\n",
    "these numbers are purely conventional and they don't have any scientific\n",
    "ground. They are defined this way to set a high threshold for such\n",
    "important claims about the observation of new phenomena.[^2]\\\n",
    "\n",
    "![Example of errors of the first and second kind\n",
    "(Wikipedia).](Section8Bilder/Type1Type2.png){#tab:typeErrors\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "\\\n",
    "\\\n",
    "**Example**  Consider a machine BM1 which is used for bonding wires of\n",
    "Si-detector modules. The produced detectors had a scrap rate of\n",
    "$P_{0} = 0.2$. BM1 should be replaced with a newer bonding machine\n",
    "called BM2, if (and only if) the new machine can produce detector\n",
    "modules with a lower scrap rate $P_{1}$. In a test run we produce $n=30$\n",
    "modules. To verify $P_{1} < P_{0}$ statistically, we use the hypothesis\n",
    "test discussed above. Define the two hypotheses $H_{0}$ and $H_{1}$ as:\n",
    "$$H_{0} : P_{1} \\geq 0.2; \\quad H_{1}: P_{1} < 0.2.$$ We choose\n",
    "$\\alpha = 0.05$ and our test statistic $t$ is the number of\n",
    "malfunctioning detector modules. This quantity is distributed according\n",
    "to a binomial distribution, with the total number of produced modules\n",
    "$n=30$ and a probability $P$. The rejection region for $H_{0}$ is\n",
    "constructed out of\n",
    "$$\\sum_{i=0}^{n_c}{n \\choose i}P_0^i(1-P_0)^{n-i}<\\alpha.$$ Here, the\n",
    "critical value is denoted by $n_{c}$, and it is the maximal number of\n",
    "malfunctioning modules produced by BM2 which still implies a rejection\n",
    "of $H_{0}$ with CL $1-\\alpha$. By going through the calculation we find\n",
    "that for $n_{c} =2$ the value of $\\alpha$ is still just below 0.05. This\n",
    "means that if we find two or less malfunctioning modules produced by BM2\n",
    "we should replace BM1 by the new machine BM2.\\\n",
    "\\\n",
    "Once the test statistics is defined there is a trade-off between\n",
    "$\\alpha$ and $\\beta$, the smaller you make $\\alpha$ the larger $\\beta$\n",
    "will be; it's up to the experimenter to decide what is acceptable and\n",
    "what is not.\\\n",
    "\\\n",
    "**Example** Suppose we want to distinguish $K-p$ elastic scattering\n",
    "events from inelastic scattering events where a $\\pi^0$ is produced.\n",
    "$H_0 : K^- p \\to K^- p$ ; $H_1 : K^- p \\to K^- p \\pi^0$. The detector\n",
    "used for this experiment is a spectrometer capable of measuring the\n",
    "momenta of all the charged particles ($K^-$, $p$) but it is blind to\n",
    "neutral particles ($\\pi^0$). The considered test statistic is the\n",
    "\"missing mass\" $M$ defined as the difference between the initial and\n",
    "final visible mass. The true value of the missing mass is $M=0$ under\n",
    "the null hypothesis $H_0$ (no $\\pi^0$ produced) and\n",
    "$M_{\\pi^0}=135~MeV/c^2$ under the alternative hypothesis $H_1$ (a\n",
    "$\\pi^0$ is produced). The critical region can be defined as $M>M_c$. The\n",
    "value of $M_c$ depends on the significance and power we want to obtain\n",
    "(see Fig. [1.8](#fig:inelastic){reference-type=\"ref\"\n",
    "reference=\"fig:inelastic\"}): a high value of $M_c$ will correspond to a\n",
    "high significance at the expenses of the power, while low values of\n",
    "$M_c$ will result in a high power but low significance.\\\n",
    "\\\n",
    "\n",
    "![Top: the p.d.f. for the test statistic $M$ under the null hypothesis\n",
    "of elastic scattering $H_0$ centred at $M=0$; bottom the p.d.f. for the\n",
    "test statistic under the alternative hypothesis of inelastic scattering\n",
    "$H_1$ centred at $M=m_{\\pi^0}$. $M_c$ defines the critical\n",
    "region.](Section8Bilder/inelastic.png){#fig:inelastic\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "\\\n",
    "Some caution is necessary when using $\\alpha$. Suppose you have 20\n",
    "researchers looking for a new phenomenon which \"in reality\" does not\n",
    "exist. Their $H_0$ hypothesis is that what they see is only background.\n",
    "One of them could reject $H_0$ with $\\alpha = 5\\%$, while the other 19\n",
    "will not. This is part of the game and therefore, before rushing for\n",
    "publication, that researcher should balance the claim with what the\n",
    "others don't see. That's the main reason why anytime there is a\n",
    "discovery claim, we always need to have the results to be corroborated\n",
    "by independent measurements. We will come back to this point when we\n",
    "will talk about the look-elsewhere effect.\\\n",
    "\\\n",
    "**Example**  Let's use again the example of the electron/pion\n",
    "separation. As already shown before the specific ionization $dE/dx$ of a\n",
    "charged particle can be used as a test statistic to distinguish particle\n",
    "types, for example electrons ($e$) from pions ($\\pi$)(see\n",
    "Fig. [1.2](#fig:dEdx){reference-type=\"ref\" reference=\"fig:dEdx\"}). The\n",
    "selection efficiency is defined as the probability for a particle to\n",
    "pass the selection cut:\n",
    "$$\\epsilon_e = \\int_{-\\infty}^{t_{cut}}g(t|e) dt = 1-\\alpha \\qquad  \\epsilon_\\pi = \\int_{-\\infty}^{t_{cut}}g(t|\\pi) dt = \\beta$$\n",
    "By moving the value of $t_{cut}$ you can change the composition of your\n",
    "sample. The lower the value of $t_{cut}$ the larger the electron\n",
    "efficiency but the higher the contamination from pions and vice-versa.\n",
    "In general, one can set a value of $t_{cut}$, select a sample and work\n",
    "out what is the fraction of electrons $N_{acc}$ present in the initial\n",
    "sample (before the requirement $t<t_{cut}$). The number of accepted\n",
    "particles in the sample is composed by:\n",
    "$$N_{acc}=\\epsilon_e N_e + \\epsilon_\\pi N_\\pi = \\epsilon_e N_e + \\epsilon_\\pi(N_{tot} - N_e)$$\n",
    "which gives\n",
    "$$N_{e}=\\frac{N_{acc} - \\epsilon_\\pi N_{tot}}{\\epsilon_e - \\epsilon_\\pi}$$\n",
    "From this, one can immediately notice that the $N_e$ can only be\n",
    "calculated if $\\epsilon_e \\neq \\epsilon_\\pi$, i.e. $N_e$ can only be\n",
    "extracted if there is any separation power at all. If there are\n",
    "systematic uncertainties in $\\epsilon_e$ or $\\epsilon_\\pi$ these will\n",
    "translate into an uncertainty on $N_e$. One should try to select the\n",
    "critical region $t_{cut}$ such that the total error on $N_e$ is\n",
    "negligible.\\\n",
    "The other side of the problem is to estimate the **purity** $p_e$ of the\n",
    "sample of candidates passing the requirement $t<t_{cut}$:\n",
    "$$\\begin{aligned}\n",
    "p_e &=& \\frac{\\#electrons~with~t<t_{cut}}{\\#particles~with~t<t_{cut}}\\\\\n",
    "    &=& \\frac{\\int_{-\\infty}^{t_{cut}}a_e g(t|e) dt }{\\int_{-\\infty}^{t_{cut}}(a_e g(t|e) + (1-a_e)g(t|\\pi) ) dt }\\\\\n",
    "    &=& \\frac{a_e\\epsilon_e N_{tot}}{N_{acc}}\\end{aligned}$$\\\n",
    "Historically, in high energy physics a parallel nomenclature has been\n",
    "defined to express the same concepts we have encounter in this section:\n",
    "\n",
    "-   *signal efficiency* = 1-$\\alpha$ (a test is significant at a level\n",
    "    1-$\\alpha$ %)\n",
    "\n",
    "    -   $\\alpha$ = probability of a type I error\n",
    "\n",
    "-   *background efficiency* = $\\beta$ = probability of a type II error\n",
    "\n",
    "    -   1-$\\beta$ = power of the test = background rejection\n",
    "\n",
    "Is there a signal ?\n",
    "-------------------\n",
    "\n",
    "A typical application of hypothesis testing in high energy physics is to\n",
    "test for the presence of a signal in data. The easiest case is\n",
    "represented by counting experiments. In this type of experiments the\n",
    "detector is used to count the number of events satisfying some selection\n",
    "criteria (slang: \"cut-and-count\"). The number of expected events in case\n",
    "of background only hypothesis is compared with the measured number. The\n",
    "signal would typically appear as an excess over the expected\n",
    "background[^3].\\\n",
    "\\\n",
    "**Example** Let $n$ be a number of events which is the sum of some\n",
    "signal and some background events $n = n_s + n_b$. Each of the\n",
    "components can be treated as a Poisson variable $\\nu_s$ (signal) and\n",
    "$\\nu_b$ (background) and so the total $\\nu = \\nu_s + \\nu_b$ is also a\n",
    "Poisson variable. The probability to observe $n$ events is:\n",
    "$$f(n; \\nu_s, \\nu_b) = \\frac{(\\nu_s+\\nu_b)^n}{n!}e^{-(\\nu_s+\\nu_b)}$$\n",
    "Suppose you measure $n_{obs}$ events. To quantify our degree of\n",
    "confidence in the discovery of a new phenomenon, i.e. $\\nu_s\\ne 0$, we\n",
    "can compute *how likely it is to find $n_{obs}$ events or more from\n",
    "background alone*.\n",
    "$$P(n\\ge n_{obs}) = \\sum_{n=n_{obs}}^\\infty f(n; \\nu_s = 0, \\nu_b) = 1 - \\sum_{n=0}^{n_{obs}-1} f(n; \\nu_s = 0, \\nu_b) =\n",
    "1- \\sum_{n=0}^{n_{obs}-1} \\frac{\\nu_b^n}{n!}e^{-\\nu_b}.$$ For example,\n",
    "if we expect $\\nu_b = 0.5$ background events and we observe\n",
    "$n_{obs} = 5$, then the *p*-value is $1.7 \\cdot 10^{-4}$. *This is not\n",
    "the probability of the hypothesis $\\nu_s = 0$ ! It is rather the\n",
    "probability, under the assumption $\\nu_s = 0$, of obtaining as many\n",
    "events as observed or more.*\\\n",
    "You should be very careful with a common pitfall. Often the result of a\n",
    "measurement is given as the estimated value of a number of events plus\n",
    "or minus one standard deviation. Since the standard deviation of a\n",
    "Poisson variable is equal to the square root of its mean, from the\n",
    "previous example, we have $5 \\pm \\sqrt{5}$ for an estimate of $\\nu$,\n",
    "i.e. after subtracting the expected background, $4.5 \\pm 2.2$ for our\n",
    "estimate of $\\nu_s$. This is very misleading: it is only two standard\n",
    "deviations from zero, and it gives the impression that $\\nu_s$ is not\n",
    "very incompatible with zero, but we have seen from the *p*-value that it\n",
    "is not the case. The subtlety is that we need to ask for the probability\n",
    "that a Poisson variable of mean $\\nu_b$ will fluctuate up to $n_{obs}$\n",
    "or higher, not for the probability that a Poisson variable with mean\n",
    "$n_{obs}$ will fluctuate down to $\\nu_b$ or lower.\\\n",
    "Another important point is that usually $\\nu_b$ is known within some\n",
    "uncertainty. If we set $\\nu = 0.8$ rather than 0.5, the *p*-value would\n",
    "increase by almost an order of magnitude to $1.4 \\cdot 10^{-3}$. It is\n",
    "therefore crucial to quantify the systematic uncertainty of the\n",
    "background when evaluating the significance of a new effect.\\\n",
    "\\\n",
    "In other types of searches the signal would reveal itself as a\n",
    "resonance, i.e. an excess of data in a localized region of a mass\n",
    "spectrum (slang: \"bump hunt\"), or as an excess of events in the tail of\n",
    "a distribution. Two examples are show in\n",
    "Fig. [1.9](#fig:searches){reference-type=\"ref\"\n",
    "reference=\"fig:searches\"}. In these cases the signal is extracted from\n",
    "the background using a fit (more on this will be developed in the next\n",
    "sections). In this case on top of using the number of expected events,\n",
    "we add the information about the way they are distributed (slang: \"shape\n",
    "analysis\").\n",
    "\n",
    "![Left: Higgs boson search in 2011. Right: search for an excess of\n",
    "events at high missing transverse energy. In both cases the data are\n",
    "well described by the background only\n",
    "hypothesis.](Section8Bilder/searches.png){#fig:searches\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "\\\n",
    "\\\n",
    "\n",
    "Neyman Pearson Lemma {#SectionNeymanPearson}\n",
    "--------------------\n",
    "\n",
    "We haven't addressed up to now the choice of $t_{cut}$. The only thing\n",
    "we know up to now is that it affects the efficiency and the purity of\n",
    "the sample under study. Ideally what we want is to set the desired\n",
    "efficiency and for that value get the best possible purity.\\\n",
    "Take the case of a *simple hypothesis* $H_0$ and allow for an\n",
    "alternative *simple hypothesis* $H_1$ (e.g. the typical situation of\n",
    "signal and background). The *Neyman Pearson lemma* states that the\n",
    "acceptance region giving the highest power (i.e. the highest purity) for\n",
    "a given significance level $\\alpha$, is the region of space such that:\n",
    "$$\\frac{g(\\vec{t} | H_{0})}{g(\\vec{t} | H_{1})} > c,$$ where $c$ is the\n",
    "knob we can tune to achieve the desired efficiency and\n",
    "$g(\\vec{t}|H_{i})$ is the distribution of $\\vec{t}$ under the hypothesis\n",
    "$H_{i}$.\\\n",
    "Basically what the lemma says is that there is function $r$ defined as\n",
    "$$\\begin{aligned}\n",
    "r = \\frac{g(\\vec{t} | H_{0})}{g(\\vec{t} | H_{1})}\\end{aligned}$$ that\n",
    "brings the problem to a 1 dimensional case and that gives the best\n",
    "purity given a fixed efficiency. The function $r$ is called the\n",
    "*likelihood ratio* for the simple hypotheses $H_0$ and $H_1$ (in the\n",
    "likelihood the data are fixed, the hypothesis is what is changed). The\n",
    "corresponding acceptance region is given by $r>c$. Any monotonic\n",
    "function of $r$ will be good too and will lead to the same test.\\\n",
    "The main draw back of the Neyman-Pearson lemma is that is is valid if\n",
    "and only if both $H_0$ and $H_1$ are simple hypothesis (and that is\n",
    "pretty rare). Even in those cases in order to determine c one needs to\n",
    "know $g(\\textbf{t}|H_{0})$ and $g(\\textbf{t}| H_{1})$. These have to be\n",
    "determined by Monte Carlo simulations, or data driven techniques, for\n",
    "both data and background. The simplest way to represent the p.d.f.'s is\n",
    "to use a multidimensional histogram. This can cause some troubles when\n",
    "the dimensionality of the problems is high. Say we have $M$ bins for\n",
    "each of the $n$ dimension of the test statistics $\\textbf{t}$, then the\n",
    "total number of bins is $M^n$, i.e. $M^n$, parameters must be determined\n",
    "from Monte Carlo or data and this can quickly become impractical. We\n",
    "will see later (Ch. [\\[ChapterMVA\\]](#ChapterMVA){reference-type=\"ref\"\n",
    "reference=\"ChapterMVA\"}) a different way to model the p.d.f. using\n",
    "Multi-Variate techniques.\n",
    "\n",
    "Goodness of Fit {#SectionGoodnessOfFit}\n",
    "---------------\n",
    "\n",
    "A typical application of hypothesis testing is to assess the goodness of\n",
    "a fit, i.e. quantifying how well the null hypothesis $H_0$ describes a\n",
    "sample of data, *without any specific reference to an alternative\n",
    "hypothesis*. The test statistic has to be constructed such that it\n",
    "reflects the level of agreement between the observed data and the\n",
    "predictions of $H_0$.\\\n",
    "The typical quantitative way to assess the agreement is to use the\n",
    "concept of $p-$value. As we have already seen\n",
    "(Eq. [\\[eq:pvalue\\]](#eq:pvalue){reference-type=\"ref\"\n",
    "reference=\"eq:pvalue\"}) the $p-$value is the probability, under the\n",
    "assumption of H, to observe data with equal or less compatibility with\n",
    "$H_0$, relative to the data we got.\\\n",
    "N.B.: the $p-$value does not give the probability for $H_0$ to be true!\n",
    "As a frequentist the probability of the hypothesis is not even defined:\n",
    "the probability is defined on the data. As Bayesian the probability of\n",
    "the hypothesis is a different thing and it is defined through the Bayes\n",
    "theorem using the prior hypothesis.\n",
    "\n",
    "### The $\\chi^2$-Test\n",
    "\n",
    "We have already encountered the $\\chi^2$ as a goodness of fit test in\n",
    "section\n",
    "Sec. [\\[sec:chi2GoodnessOfFit\\]](#sec:chi2GoodnessOfFit){reference-type=\"ref\"\n",
    "reference=\"sec:chi2GoodnessOfFit\"}. The $\\chi^{2}$-test is by far the\n",
    "most commonly used goodness of fit test. The first application we\n",
    "discuss is with a set of measurements $x_i$ and $y_i$, where the $x_i$\n",
    "are supposed to be exact (or at least with negligible uncertainty) and\n",
    "the $y_i$ are known with an uncertainty $\\sigma_i$. We want to test the\n",
    "function $f(x)$ which we believe it gives (predicts) the correct value\n",
    "of $y_i$ for each value of $x_i$; to do so we define the $\\chi^2$ as:\n",
    "$$\\chi^{2} = \\sum_{i=1}^{N} \\frac{[y_{i}-f(x_{i})]^{2}}{\\sigma_{i}^{2}}.$$\n",
    "If the uncertainties on the $y_i$ measurements are correlated, the above\n",
    "formula becomes (with the lighter matrix notation, see\n",
    "Sec. [\\[sec:matrixNotation\\]](#sec:matrixNotation){reference-type=\"ref\"\n",
    "reference=\"sec:matrixNotation\"}):\n",
    "$$\\chi^{2} = (\\bf{y}-\\bf{f})^T \\bf{V}^{-1} (\\bf{y} - \\bf{f})$$ where\n",
    "$\\bf{V}$ is the covariance matrix. A function that correctly describes\n",
    "the data will give a small difference between the values predicted by\n",
    "the function $f$ and the measurements $y_i$. This difference reflects\n",
    "the statistical uncertainty on the measurements, so for $N$ measurements\n",
    "the $\\chi^2$ should be roughly $N$.\\\n",
    "Recalling the p.d.f. of the $\\chi^2$ distribution (see\n",
    "Sec. [\\[SubSectionChi2\\]](#SubSectionChi2){reference-type=\"ref\"\n",
    "reference=\"SubSectionChi2\"}): $$\\label{Chi2EquationAgain}\n",
    "  P(\\chi^{2};N) = \\frac{2^{\\frac{-N}{2}}}{\\Gamma(\\frac{N}{2})} \\chi^{N-2} e ^{\\frac{-\\chi^{2}}{2}}$$\n",
    "(where the expectation value of this distribution is $N$, and so\n",
    "$\\chi^{2} / N \\sim 1$), we can base our *decision boundary* on the\n",
    "goodness-of-fit, by defining the $p-$value: $$\\label{Chi2Probability}\n",
    "  p = \\text{Prob}(\\chi^{2};N) = \\int_{\\chi^{2}}^{\\infty} P(\\chi'^{2}; N) d\\chi'^{2}$$\n",
    "which is called the *$\\chi^{2}$ probability*. This expression gives the\n",
    "probability that the function describing the $N$ measured data points\n",
    "gives a $\\chi^{2}$ *as large or larger than* the one we obtained from\n",
    "our measurement.\\\n",
    "\\\n",
    "**Example** Suppose you compute a $\\chi^2$ of 20 for N=5 points. The\n",
    "feeling is that the function is a very poor model of the data\n",
    "($20/5 =4 \\gg 1$). To quantify that we compute the $\\chi^2$ probability\n",
    "$\\int_{20}^{\\infty}P(\\chi^2,5)d\\chi^2$. In `ROOT` you can compute this\n",
    "as `TMath::Prob(20,5) = 0.0012`. The $\\chi^2$ probability is indeed very\n",
    "small and the $H_0$ hypothesis should be discarded.\\\n",
    "\\\n",
    "You have to be careful when using the $\\chi^2$ probability to take\n",
    "decisions. For instance if the $\\chi^2$ is large, giving a very small\n",
    "$\\chi^2$ probability, it could be both that the function $f$ is a bad\n",
    "representation of the data or that the uncertainties are underestimated.\n",
    "On the other hand if you obtain a very small value for the $\\chi^2$, the\n",
    "function cannot be blamed[^4], so you might have overestimated the\n",
    "uncertainties. It's up to you to interpret correctly the meaning of the\n",
    "probability. A very useful tool for this scope is the **pull\n",
    "distribution** (see\n",
    "Sec. [\\[sec:MLremarks\\]](#sec:MLremarks){reference-type=\"ref\"\n",
    "reference=\"sec:MLremarks\"}), where each entry is defined as\n",
    "(measured-predicted)/uncertainty = $(y_i - f(x_i))/ \\sigma_i$; if\n",
    "everything is done correctly (i.e. the model is correct and the\n",
    "uncertainties are computed correctly) the pull will result in a normal\n",
    "distribution centred at 0 with width 1. If the pull is not centred at 0\n",
    "(bias) the model is incorrect, if the pull has a width larger than 1\n",
    "either the uncertainties are underestimated or the model is wrong, if\n",
    "the pull has a width smaller than 1 the uncertainties are\n",
    "overestimated.\\\n",
    "\n",
    "### Degrees of freedom for $\\chi^2$-test on fitted data\n",
    "\n",
    "The concept of $\\chi^2$ developed above only works if you are given a\n",
    "set of data points and a function (model). If the function comes out\n",
    "*from a fit to the data* then, by construction, you will get a $\\chi^2$\n",
    "which is the smallest, because you fit the parameters of the function in\n",
    "order to minimize it.\\\n",
    "This problem turns out to be very easy to treat. You just need to remove\n",
    "degrees of freedom from the computation. For example, suppose you have\n",
    "$N$ points and you fitted $m$ parameters of your function to minimize\n",
    "the $\\chi^2$ sum; then all you have to do to compute the new $\\chi^2$\n",
    "probability is to reduce the number of d.o.f. to $n=N-m$.\\\n",
    "\\\n",
    "**Example**  You have a set of 20 points, you consider as function f(x)\n",
    "a straight line and you get $\\chi^2 = 36.3$. If you use a parabola you\n",
    "get $\\chi^2 = 20.1$. The straight line has 2 degrees of freedom (slope\n",
    "and intercept), so the number of d.o.f. of the problem is 20-2=18; the\n",
    "$\\chi^2$ probability is `TMath::Prob(36.3,18) = 0.0065` which makes the\n",
    "hypothesis that data are described by a straight line improbable. If you\n",
    "now fit it with a parabola you get `TMath::Prob(20.1,17) = 0.27` which\n",
    "means that you can't reject the hypothesis that the data are distributed\n",
    "according to a parabolic shape.\\\n",
    "\\\n",
    "Notes on the $\\chi^{2}$-test:\n",
    "\n",
    "-   For large values of d.o.f. the distribution of $\\sqrt{2\\chi^2}$ can\n",
    "    be approximated with a Gaussian distribution with mean $\\sqrt{2n-1}$\n",
    "    and standard deviation $1$. When in the past the integrals were\n",
    "    extracted from tables this was a neat trick; still it is a useful\n",
    "    simplification when the the $\\chi^2$ is used in some explicit\n",
    "    calculation.\n",
    "\n",
    "-   The $\\chi^{2}$-test can also be used as a goodness of fit test for\n",
    "    binned data. The number of events in bin $i$ ($i = 1, 2, \\ldots, n$)\n",
    "    are $y_{i}$, with bin $i$ having mean value $x_{i}$. The predicted\n",
    "    number of events is thus $f(x_{i})$. The errors are given by Poisson\n",
    "    statistics in the bin ($\\sqrt{f(x_i)}$, see\n",
    "    [\\[sec:binnedchi2\\]](#sec:binnedchi2){reference-type=\"ref\"\n",
    "    reference=\"sec:binnedchi2\"} for the use of the Poisson\n",
    "    uncertainties) and the $\\chi^{2}$ is\n",
    "    $$\\chi^{2} = \\sum_{i=1}^{n}\\frac{[y_{i}-f(x_{i})]^{2}}{f(x_{i})},$$\n",
    "    where the number of degrees of freedom $n$ is given by the number of\n",
    "    bins minus the number of fitted parameters (do not forget the\n",
    "    overall normalization of the model when counting the number of\n",
    "    fitted parameters).\n",
    "\n",
    "-   when binning data, you should try to have enough entries per bin\n",
    "    such that the computation of the $\\chi^2$ is actually meaningful; as\n",
    "    a rule of thumb you should have *at least 5 entries per bin*. Most\n",
    "    of the results for binned data are only true asymptotically.\n",
    "\n",
    "### Run test\n",
    "\n",
    "The $\\chi^2$ collapses in one number the level of agreement between a\n",
    "hypothesis and a set of data. There are cases where behind a\n",
    "$\\chi^2\\sim 1$ hides in reality a very poor agreement between the data\n",
    "and the model. Consider the situation which is illustrated in\n",
    "Fig. [1.10](#RunTestPicture){reference-type=\"ref\"\n",
    "reference=\"RunTestPicture\"}. The 12 data points are fitted by a straight\n",
    "line, which clearly does not describe the data adequately. Nevertheless,\n",
    "in this example, the $\\chi^{2}$ is 12.0 and thus $\\chi^{2} / n = 1$.\n",
    "\n",
    "![[\\[RunTestPicture\\]]{#RunTestPicture label=\"RunTestPicture\"}Example\n",
    "for the application of the run test. The dashed line is the hypothesized\n",
    "fit (a straight line), whereas the crosses are the actual\n",
    "data.](Section8Bilder/RunTestPicture.pdf){#RunTestPicture width=\"35%\"}\n",
    "\n",
    "In cases such as this one the **run test** provides important extra\n",
    "information. The run test works like this: every time the measured data\n",
    "point lies *Above* the function, we write an $A$ in a sequence, and\n",
    "every time the data lies *Below* the function, we write a $B$. If the\n",
    "data are distributed according to the hypothesis function, then they\n",
    "should fluctuate up and down creating very short sequences of A's and\n",
    "B's (*runs*). The sequence in the pictures reads $AAABBBBBBAAA$, making\n",
    "only three runs and possibly pointing to a poor description of the\n",
    "data.\\\n",
    "The probability of the A's and B's giving a particular number of runs\n",
    "can be calculated. Suppose there are $N_A$ points above and $N_B$ below\n",
    "with $N=N_A+N_B$. The total number of possible combinations without\n",
    "repetitions is given by (see chapter 1):\n",
    "$${N \\choose N_A} = \\frac{N!}{N_A!~N_B!}$$ this will be our denominator.\n",
    "For the numerator suppose that $r$ is even and the sequence starts with\n",
    "an A. There are $N_A$ A-points and $r/2-1$ divisions between them\n",
    "(occupied by B's). With $N_A$ point you can place $N_A-1$ dividing line,\n",
    "in the next step $N_A-2$ and so on, giving ${N_A-1 \\choose r/2-1}$\n",
    "different A arrangements. The same argument can be made for the B's. So\n",
    "we find for the probability of $r$ runs:\n",
    "$$P_{r} = 2 \\frac{{N_{A}-1 \\choose r/2-1} \\cdot {N_{B}-1 \\choose r/2 -1}}{{N \\choose N_{A}}},$$\n",
    "where the extra factor of 2 is there because we chose to start with an A\n",
    "and we could have started with a B. When $r$ is odd you get:\n",
    "$$P_{r} = 2 \\frac{{N_{A}-1 \\choose (r-3)/2} \\cdot {N_{B}-1 \\choose (r-1)/2} + {N_{A}-1 \\choose (r-1)/2} \\cdot {N_{B}-1 \\choose (r-3)/2}} {{N \\choose N_{A}}}$$\n",
    "These are probabilities to get $r$ runs given a sequence of A's and\n",
    "B's.\\\n",
    "It can be shown that $$\\begin{aligned}\n",
    "\\langle r \\rangle &=& 1 + \\frac{2N_{A}N_{B}}{N}\\\\\n",
    "V(r) & = & \\frac{2N_{A}N_{B}(2N_{A}N_{B}-N)}{N^{2}(N-1)}\\end{aligned}$$\n",
    "In the example above the number of expected runs is\n",
    "$\\langle r \\rangle = 1 +2 \\cdot 6 \\cdot 6/12 = 7$ with $\\sigma = 1.65$.\n",
    "The deviation between the expected and the observed is $7-3 = 4$ and it\n",
    "constitutes 2.4 standard deviations which is significant at the 1% level\n",
    "for the one-sided test. Thus the straight line fit could be rejected\n",
    "despite the (far too) good $\\chi^{2}$ value.\\\n",
    "The run test does not substitute the $\\chi^2$ test, it's in a sense\n",
    "complementary; the $\\chi^2$ test ignore the sign of the fluctuations,\n",
    "while the run test only looks at them.\n",
    "\n",
    "Unbinned tests {#sec:KS}\n",
    "--------------\n",
    "\n",
    "Unbinned tests are used when the binning procedure would result in a too\n",
    "large loss of information (e.g. when the data set is small). They are\n",
    "all based on the comparison of the cumulative distribution function\n",
    "(c.d.f.) $F(x)$ of the model $f(x)$ under some hypothesis $H_0$ and the\n",
    "c.d.f. for the data. To define a c.d.f. on data we define an order\n",
    "statistics, i.e. a rule to order the data[^5] and then define on it the\n",
    "*Empirical Cumulative Distribution Function* e.c.d.f.: $$S_n(x) =\n",
    "\\left\\{\n",
    "\\begin{array}{rll}\n",
    "  0 &\\mbox{,} & x <x_1 \\\\\n",
    "  \\frac{r}{n} &\\mbox{,} & x_r \\le x  < x_{r+1} \\\\\n",
    "  1 &\\mbox{,} & x_n <x \n",
    "\\end{array}\n",
    "\\right.$$ This is just the fraction of events not exceeding x (which is\n",
    "a staircase function from 0 to 1), see\n",
    "Fig. [\\[fig:KS\\]](#fig:KS){reference-type=\"ref\" reference=\"fig:KS\"}.\\\n",
    "\\\n",
    "The first unbinned test we describe is the **Smirnov-Cramér-von Mises**\n",
    "test. We define a measure of the distance between $S_n(x)$ and F(x) as:\n",
    "$$W^2 = \\int_0^1 [S_n(x) - F(x)]^2 dF(x)$$ (dF(x) can be in general a\n",
    "non decreasing weight). Inserting the explicit expression of $S_n(x)$ in\n",
    "this definition we get:\n",
    "$$nW^2 = \\frac{1}{12n} + \\sum_{i=1}^n \\left( F(x_i) - \\frac{2i-1}{2n}   \\right)^2$$\n",
    "From the asymptotic distribution of $nW^2$ the critical regions can be\n",
    "computed: frequently used test sizes are given in the\n",
    "Tab. [1.11](#fig:skvm){reference-type=\"ref\" reference=\"fig:skvm\"}. The\n",
    "asymptotic distribution is reached remarkably rapidly (in this table the\n",
    "asymptotic limit is reached for $n \\ge 3$).\\\n",
    "\n",
    "![[\\[fig:skvm\\]]{#fig:skvm label=\"fig:skvm\"}Rejection regions for the\n",
    "Smirnov-Cramér-von Mises test the for some typical test\n",
    "sizes.](Section8Bilder/smirnovCramer.png){#fig:skvm width=\"30%\"}\n",
    "\n",
    "\\\n",
    "The **Kolmogorov-Smirnov** test follows the same idea of comparing the\n",
    "model c.d.f. with the data e.c.d.f. but it defines a different metric\n",
    "for the distance between the two. The test statistic is\n",
    "$d := D \\sqrt{N}$ where $D$ is the maximal vertical difference between\n",
    "$F_{n}(x)$ and $F(x)$ (see\n",
    "Fig. [\\[fig:KS\\]](#fig:KS){reference-type=\"ref\" reference=\"fig:KS\"}):\n",
    "$$D := \\max_{x} |S_{n}(x)-F(x)|$$ The hypothesis $H_{0}$ corresponding\n",
    "to the function $f(x)$ is rejected if $d$ is larger than a given\n",
    "critical value. The probability $P(d\\le t_0)$ can be calculated in\n",
    "`ROOT` by `TMath::KolmogorovProb(t0)`. Some values are reported in\n",
    "Tab. [1.1](#KSTable){reference-type=\"ref\" reference=\"KSTable\"}.\\\n",
    "\\\n",
    "\n",
    "![[\\[fig:KS\\]]{#fig:KS label=\"fig:KS\"}Example of c.d.f. and e.c.d.f..\n",
    "The arrow indicates the largest distance used by the Kolmogorov-Smirnov\n",
    "test.](Section8Bilder/KS_Example.png){#Kolmogorov1 width=\"50%\"}\n",
    "\n",
    "::: {#KSTable}\n",
    "  $\\alpha$              99%    95%    50%    32%     5%     1%    0.1%\n",
    "  ------------------ ------ ------ ------ ------ ------ ------ -------\n",
    "  $P(d\\leq t_{0})$       1%    5 %    50%    68%    95%    99%   99.9%\n",
    "  $t_{0}$              0.44   0.50   0.83   0.96   1.36   1.62    1.95\n",
    "\n",
    "  : [\\[KSTable\\]]{#KSTable label=\"KSTable\"}Critical values $t_{0}$ for\n",
    "  various significances $1-\\alpha$.\n",
    ":::\n",
    "\n",
    "\\\n",
    "The Kolmogorov-Smirnov test can also be used to test if *two data sets*\n",
    "have been drawn from the same parent distribution. Take the two\n",
    "histograms corresponding to the data to be compared and normalize them\n",
    "(such that the cumulative plateaus at 1). Then compare the e.c.d.f. for\n",
    "the two histograms and compute the maximum distance as before (in `ROOT`\n",
    "use `h1.KolmogorovTest(h2)`).\\\n",
    "\\\n",
    "Notes on the Kolmogorov-Smirnov test:\n",
    "\n",
    "-   the test is more sensitive to departures of the data from the median\n",
    "    of $H_0$ than to departures from the width (more sensitive to the\n",
    "    core than to the tails of the distributions)\n",
    "\n",
    "-   the test becomes meaningless if the $H_0$ p.d.f. is a fit to the\n",
    "    data. This is due to the fact that there is no equivalent of the\n",
    "    number of degrees of freedom as in the $\\chi^{2}$-test, hence it\n",
    "    cannot be corrected for.\n",
    "\n",
    "Two-sample problem {#SectionDistributionDependingTests}\n",
    "------------------\n",
    "\n",
    "In this section we will look at the problem of telling if two samples\n",
    "are compatible with each other, i.e. if both are drawn from the same\n",
    "parent distribution. Clearly the complication is that, even if they are,\n",
    "they will exhibit differences coming from statistical fluctuations. In\n",
    "the following we will examine some typical examples of two-sample\n",
    "problems (see Tab. [1.2](#hypo2){reference-type=\"ref\"\n",
    "reference=\"hypo2\"}).\n",
    "\n",
    "### Two Gaussians, known $\\sigma$ {#sec:knownsigma}\n",
    "\n",
    "Suppose you have two random variables $X$ and $Y$ distributed as\n",
    "Gaussians of known width. Typical situations are when you have two\n",
    "measurements taken with the same device with a known resolution; or two\n",
    "samples are taken under different conditions where the variances of the\n",
    "parent distribution are known (you have the two means\n",
    "$\\langle X \\rangle$, $\\langle Y \\rangle$ and the uncertainty on the\n",
    "means $\\sigma_x/\\sqrt{N_x}$ and $\\sigma_y/\\sqrt{N_y}$).\\\n",
    "This problem is equivalent to check if $X-Y$ is compatible with $0$. The\n",
    "variance of $X-Y$ is $V(X-Y) = \\sigma_x^2 +\\sigma_y^2$ and so the\n",
    "problem boils down to how many $\\sigma$ the difference is from 0:\n",
    "$(X-Y)/ \\sqrt{\\sigma_x^2 + \\sigma_y^2}$.\\\n",
    "More generally what you are doing is defining a test statistics\n",
    "$\\frac{|\\langle x \\rangle - \\mu_0|}{\\sigma/\\sqrt{N}}$ (in the previous\n",
    "case $\\mu_0 = 0$) and a double sided rejection region. This means that\n",
    "you choose the significance of your test ($\\alpha$) and set as rejection\n",
    "region the (symmetric) values $u_{1-\\alpha/2}$ on the corresponding\n",
    "Gaussian as:\n",
    "$$\\int_{-\\infty}^{u_{1-\\alpha/2}} G(x;\\mu_0,\\sigma)dx  = \\int_{u_{1-\\alpha/2}}^{\\infty} G(x;\\mu_0,\\sigma)dx = \\frac{\\alpha}{2}$$\n",
    "If the measured difference ends up in the rejection region (either of\n",
    "the two tails) then the two samples are to be considered different.\\\n",
    "You can also test whether $X>Y$ (or $Y>X$). In this case the test\n",
    "statistic is $\\frac{(\\langle x \\rangle - \\mu_0)}{\\sigma/\\sqrt{N}}$ and\n",
    "the rejection region becomes single sided $(u_{1-\\alpha},\\infty)$ (or\n",
    "$(-\\infty,u_{1-\\alpha})$).\n",
    "\n",
    "### Two Gaussians, unknown $\\sigma$ {#student-t}\n",
    "\n",
    "The problem is similar to the previous one, you're comparing two\n",
    "Gaussian distributions with means $\\langle X \\rangle$ and\n",
    "$\\langle Y \\rangle$, but this time you don't know what are the parents'\n",
    "standard deviations. All you can do is to estimate them from the samples\n",
    "at hand:\n",
    "$$s^2_x = \\frac{\\sum(x_i -\\langle x\\rangle)^2}{N_x-1} \\qquad ; \\qquad s^2_y = \\frac{\\sum(y_i -\\langle y\\rangle)^2}{N_y-1}.$$\n",
    "Because we're using the estimated standard deviation we have to build a\n",
    "Student's $t$ variable to test the significance, and not the Gaussian\n",
    "p.d.f. as we did in the previous case. As we have seen in\n",
    "Sec. [\\[SectionStudentT\\]](#SectionStudentT){reference-type=\"ref\"\n",
    "reference=\"SectionStudentT\"} the $t$-variable comes from the ratio of a\n",
    "Gaussian and a $\\chi^2$ variable; the trick was to cancel out in the\n",
    "ratio our ignorance about $\\sigma$.\\\n",
    "For the numerator, the expression\n",
    "$$\\frac{\\langle x \\rangle - \\langle y \\rangle}{\\sqrt{(\\sigma_x^2/N_x)+(\\sigma_y^2/N_y)}}$$\n",
    "under the null hypothesis that the two distributions have the same mean\n",
    "($\\mu_x = \\mu_y$) is a Gaussian centred at zero with standard deviation\n",
    "one.\\\n",
    "For the denominator, the sum\n",
    "$$\\frac{(N_x-1)s_x^2}{\\sigma_x^2} + \\frac{(N_y-1)s_y^2}{\\sigma_y^2}$$ is\n",
    "a $\\chi^2$ (to convince yourself just plugin the $s^2_x$ and $s^2_y$\n",
    "written above) with $N_x+N_y-2$ d.o.f, because we used them to compute\n",
    "the means.\\\n",
    "If we assume that the unknown parent standard deviation is the same for\n",
    "the two samples $\\sigma_x = \\sigma_y$, that will do the trick: $\\sigma$\n",
    "cancels out in the ratio. The definition for the $t$-distribution\n",
    "becomes:\n",
    "$$t = \\frac{\\langle x \\rangle - \\langle y \\rangle}{S \\sqrt{(1/N_x)+(1/N_y)}}$$\n",
    "where $$S^2 = \\frac{(N_x-1)s_x^2 + (N_y-1)s_y^2}{N_x +N_x -2}$$ The\n",
    "variable $t$ is distributed as a Student's $t$ with $N_x +N_x -2$ d.o.f.\n",
    "With this variable we can now use the same testing procedure (double or\n",
    "single sided rejection regions) used in the case shown\n",
    "above [1.7.1](#sec:knownsigma){reference-type=\"ref\"\n",
    "reference=\"sec:knownsigma\"}, substituting the c.d.f of the Gaussian with\n",
    "the c.d.f. of the student's t.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### F-test {#sec:Ftest}\n",
    "\n",
    "The $F$-test is used to test whether the variances of two samples with\n",
    "size $n_{1}$ and $n_{2}$, respectively, are compatible. Because the true\n",
    "variances are not known, the sample variances $V_{1}$ and $V_{2}$ are\n",
    "used to build the ratio $F = \\frac{V_{1}}{V_{2}}$. Recalling the\n",
    "definition of the sample variance, we can write:\n",
    "$$F= \\frac{V_{1}}{V_{2}}=\\frac{\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{1_{i}}-\\bar{x}_{1})^{2}}{\\frac{1}{n_{2}-1}\\sum_{i=1}^{n_{2}}(x_{2_{i}}-\\bar{x}_{2})^{2}}$$\n",
    "(by convention the bigger sample variance is at the numerator, such that\n",
    "$F \\geq 1$). Intuitively the ratio will be close to 1 if the two\n",
    "variances are similar, while it will go to a large value if they are\n",
    "not. When you divide the variance by $\\sigma^2$ you obtain a random\n",
    "variable which is distributed as a $\\chi^2$ with $N-1$ d.o.f. Given that\n",
    "the random variable $F$ is the ratio of two such variables, the\n",
    "$\\sigma^2$ cancels and we are left with the ratio of two $\\chi^2$\n",
    "distributions with $f_1 = N_1-1$ d.o.f. for the numerator and\n",
    "$f_2=N_2-1$ d.o.f. for the denominator.\\\n",
    "The variable $F$ follows the $F$-distribution with $f_1$ and $f_2$\n",
    "degrees of freedom: $F(f_{1},f_{2})$:\n",
    "$$P(F) = \\frac{\\Gamma((f_1+f_2)/2)}{\\Gamma(f_1/2)\\Gamma(f_2/2)}\\sqrt{f_1^{f_1} f_2^{f_2}}\\frac{F^{f_1/2-1}}{(f_1+f_2F)^{(f_1+f_2)/2}}$$\n",
    "For large numbers, the variable $$Z  = \\frac{1}{2} \\log{F}$$ converges\n",
    "to a Gaussian distribution with mean $\\frac{1}{2}(1/f_2-1/f_1)$ and\n",
    "variance $\\frac{1}{2}(1/f_2+1/f_1)$. In any case you can test the\n",
    "compatibility by using e.g. the `ROOT` function\n",
    "`TMath::fdistribution_pdf`.\\\n",
    "\\\n",
    "**Example** Background model for the $H\\to \\gamma\\gamma$ search: The\n",
    "collected diphoton events are divided in several categories (based on\n",
    "resolution and S/B to optimize the analysis sensitivity). Once a model\n",
    "for the background is chosen (e.g. a polynomial) the number of d.o.f.\n",
    "for that model (e.g. the order of the polynomial) can be chosen using an\n",
    "F-test. The main idea is gradually increase the number of d.o.f. until\n",
    "you don't see any decrease in the variance (see snapshot of the text\n",
    "here below).\n",
    "\n",
    "![image](Section8Bilder/FtestHgg.png){#fig:FtestHgg width=\"100%\"}\n",
    "\n",
    "[\\[fig:FtestHgg\\]]{#fig:FtestHgg label=\"fig:FtestHgg\"}\n",
    "\n",
    "\\\n",
    "\\\n",
    "\n",
    "::: {#hypo2}\n",
    "              $H_0$                         $H_1$                              Test Statistic                             Rejection Region                                                    Comment\n",
    "  ----------------------------- ----------------------------- ------------------------------------------------- ------------------------------------- ---------------------------------------------------------------------------------------\n",
    "                                                                                                                                                      \n",
    "     $\\mu_1-\\mu_2\\le \\delta$       $\\mu_1 -\\mu_2 >\\delta$       $\\frac{\\bar{x_1}-\\bar{x_2}-\\delta}{\\sigma_d}$          $(u_{1-\\alpha};\\infty)$                              $\\bar{x_i}$: arithmetic mean of sample $i$\n",
    "     $\\mu_1-\\mu_2\\ge \\delta$       $\\mu_1 -\\mu_2 <\\delta$       $\\frac{\\bar{x_1}-\\bar{x_2}-\\delta}{\\sigma_d}$         $(-\\infty;-u_{1-\\alpha})$                        $\\sigma_d:=\\frac{\\sigma_1}{n_1}+\\frac{\\sigma_2}{n_2}$\n",
    "      $\\mu_1-\\mu_2=\\delta$        $\\mu_1 -\\mu_2 \\ne\\delta$     $\\frac{|\\bar{x_1}-\\bar{x_2}-\\delta|}{\\sigma_d}$        $(u_{1-\\alpha/2};\\infty)$       \n",
    "                                                                                                                                                      \n",
    "     $\\mu_1-\\mu_2\\le \\delta$       $\\mu_1 -\\mu_2 >\\delta$         $\\frac{\\bar{x_1}-\\bar{x_2}-\\delta}{S_d}$            $(t_{f;1-\\alpha};\\infty)$        $S_d=\\sqrt{\\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}\\sqrt{\\frac{n_1+n_2}{n_1n_2}}$\n",
    "     $\\mu_1-\\mu_2\\ge \\delta$       $\\mu_1 -\\mu_2 <\\delta$         $\\frac{\\bar{x_1}-\\bar{x_2}-\\delta}{S_d}$           $(-\\infty;-t_{f;1-\\alpha})$                                           $f=n_1+n_2-2$\n",
    "      $\\mu_1-\\mu_2=\\delta$        $\\mu_1 -\\mu_2 \\ne\\delta$       $\\frac{|\\bar{x_1}-\\bar{x_2}-\\delta|}{S_d}$          $(t_{f;1-\\alpha/2};\\infty)$                                Calculate by non-central $t$-dist.\n",
    "                                                                                                                                                      \n",
    "   $\\sigma_1^2 \\le \\sigma^2_2$    $\\sigma_1^2 > \\sigma^2_2$                     $S_1^2/S_2^2$                    $A_1=(F_{N_1;N_2;1-\\alpha};\\infty)$                                        $N_i=n_i-1$\n",
    "   $\\sigma_1^2\\ge \\sigma^2_2$     $\\sigma_1^2 < \\sigma^2_2$                     $S_1^2/S_2^2$                       $A_2=(0;F_{N_1;N_2;\\alpha})$      \n",
    "     $\\sigma_1^2=\\sigma^2_2$     $\\sigma_1^2 \\ne \\sigma^2_2$                    $S_1^2/S_2^2$                         $A_1 \\, \\text{and} \\,A_2$       \n",
    "\n",
    "  : [\\[hypo2\\]]{#hypo2 label=\"hypo2\"}Summary of the hypothesis tests for\n",
    "  the two-sample problem.\n",
    ":::\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matched and correlated samples\n",
    "\n",
    "In the previous sections we've seen how to compare two samples under\n",
    "different hypothesis. The tests are more discriminating the smaller are\n",
    "their variances. Correlations between the two samples can be used to our\n",
    "advantage to reduce the variance. Take as test statistic:\n",
    "$$\\sum_i (x_i-y_i)$$ where each of the data point of the first sample is\n",
    "paired to a corresponding one in the second sample. The variance of this\n",
    "distribution is:\n",
    "$$V(x-y) = \\sigma_x^2 + \\sigma_y^2 - 2\\rho \\sigma_x \\sigma_y$$ Now, if\n",
    "the two samples are correlated ($\\rho > 0$) then the variance is reduced\n",
    "and will make the test more discriminating.\\\n",
    "\\\n",
    "**Example**  A consumer magazine is testing a widget claimed to increase\n",
    "fuel economy. Here are the data on seven cars are reported in\n",
    "Tab. [1.14](#fig:cars){reference-type=\"ref\" reference=\"fig:cars\"}. Is\n",
    "there evidence for any improvement?\n",
    "\n",
    "![[\\[fig:cars\\]]{#fig:cars label=\"fig:cars\"}Data from seven\n",
    "cars.](Section8Bilder/cars.png){#fig:cars width=\"100%\"}\n",
    "\n",
    "If you ignore the matching, the means are $38.6 \\pm 3.0$ and\n",
    "$35.6 \\pm 2.3$ for the samples with and without the widget. The\n",
    "improvement of 3 m.p.g. is within the statistical uncertainties. Now\n",
    "look at the differences. Their average is 3.0. The estimated standard\n",
    "deviation s is 3.6, so the error on the estimated average is\n",
    "$3.6/ \\sqrt{7} = 1.3$, and t is $3.6/1.3 = 2.0$. This is significant at\n",
    "the 5% level using Student's t (one-tailed test, 6 degrees of freedom,\n",
    "$t_{critic}= 1.943$).\n",
    "\n",
    "### The most general test\n",
    "\n",
    "As we already said the more precisely the test can be formulated, the\n",
    "more discriminant it will be. The most general two-sample test, makes no\n",
    "assumptions at all on the two distributions, it just asks whether the\n",
    "two are the same.\\\n",
    "You can apply an unbinned test like the the Kolmogorov-Smirnov (as\n",
    "explained in Sec. [1.6](#sec:KS){reference-type=\"ref\"\n",
    "reference=\"sec:KS\"}) by ordering the two samples and computing the\n",
    "maximal distance between the two e.c.d.f. Or you can approach the\n",
    "problem ordering together both samples and then apply a run-test. If the\n",
    "two samples are drawn from the same parent distribution there will be\n",
    "several very short runs; if on the other hand the two samples are from\n",
    "different parent distributions you will have long runs from both\n",
    "samples. This test can be tried only if he number of points in sample A\n",
    "is similar to the one in sample B.\\\n",
    "\\\n",
    "**Example**  Two samples A and B from the same parent distribution will\n",
    "give something like: $AABBABABAABBAABABAABBBA$.\\\n",
    "Two samples from two narrow distributions with different means will give\n",
    "something like:\\\n",
    "$AAAAAAAAAABBBBBBBBBBBBB$.\\\n",
    "\\\n",
    "\n",
    "ANOVA\n",
    "-----\n",
    "\n",
    "The analysis of variance (ANOVA) is a technique rarely used in high\n",
    "energy physics, but because of its use in natural sciences a brief\n",
    "introduction will be given.\\\n",
    "The idea is a generalization of the two sample problem: instead of two\n",
    "samples you are confronted with several. The obvious approach to compare\n",
    "the different samples in pairs doesn't work. Suppose as an example that\n",
    "you have 15 samples and you have to decide if they are sampled from the\n",
    "same parent distribution (i.e. they are \"compatible\"). If you start\n",
    "comparing them in pairs you will end up with $\\binom{15}{2}$ = 105\n",
    "pairs. If you now set the significance at the 99% the probability to\n",
    "have all of them passing is $(1-0.01)^{105}$ = 0.348, i.e. the\n",
    "probability to make a type 1 error (reject the true hypothesis that they\n",
    "all come from the same parent distribution) is 1-0.348 = 0.652.\\\n",
    "To expose the ANOVA method we take the simple case where all samples\n",
    "(\"groups\" as they are generally called in ANOVA) are Gaussian of the\n",
    "same unknown variance and we want to check if their means are compatible\n",
    "with the hypothesis of being all samples taken from the same parent\n",
    "distribution.\\\n",
    "To fix the notation we set $(\\mu,\\sigma)$ the true mean and width of the\n",
    "parent distribution; $n$ the number of groups $g$ each with a number of\n",
    "events $N_g$ and mean and variance $(\\bar{x}_g,V_g)$ and true (unknown)\n",
    "sample mean $\\mu_g$. The total number of events is $N$ and the mean and\n",
    "variance of the sample made summing together all groups are\n",
    "$(\\bar{x},V)$.\\\n",
    "The null hypothesis we want to test is $H_0$ all groups are compatible\n",
    "(all $\\mu_g$ are the same and equal to $\\mu$), the alternative\n",
    "hypothesis $H_1$ is that there are differences.\\\n",
    "To test if the groups are compatible means that we will have to say\n",
    "whether the variations we see from one group to the other are just\n",
    "statistical fluctuations or they are indeed the expression of coming\n",
    "from different parent distributions. Let's define the \"spread *between*\n",
    "the groups\" as the spread of their means $\\bar{x_g}$ which follows the\n",
    "$\\sigma$ of the parent distribution. Clearly we don't know $\\sigma$ but\n",
    "it can be estimated from the data itself looking at the variation\n",
    "\"*within* the groups\". Stated in this way the problem can be solved\n",
    "using the $F-$test we ecountered in the previous section, by comparing\n",
    "the variances \"between\" and \"within\" the groups:\n",
    "$$F=\\frac{V_b}{V_w} = \\frac{between}{within}$$ The numerator, the\n",
    "variance between the groups, can be taken as:\n",
    "$$\\frac{1}{n-1} \\sum_g N_g (\\bar{x}_g - \\bar{x})^2.$$ There are $n-1$\n",
    "degrees of freedom because the mean is taken as $\\bar{x}_g$. As for the\n",
    "denominator we can take the estimate of $\\sigma$ from the different\n",
    "groups: $$\\frac{1}{N-n}\\sum_g \\sum_{i\\in g} (x_i - \\bar{x}_g)^2$$ where\n",
    "$g$ is the group and $i$ is the element within that group.\\\n",
    "By taking the ratio of numerator and denominator we obtain an $F-$test.\n",
    "Now we just need to set the critical value for the desidered\n",
    "significance level and proceed as in the $F-$ test. Note that the ANOVA\n",
    "method formally reduces to the Student's $t$ discussed\n",
    "in [1.7.2](#student-t){reference-type=\"ref\" reference=\"student-t\"} when\n",
    "you have only two samples.\\\n",
    "\\\n",
    "**Example**  As an example we take the shares of industrial, financial,\n",
    "and textile sectors for a particular day see\n",
    "Tab. [1.3](#tab:anovaEx){reference-type=\"ref\" reference=\"tab:anovaEx\"}.\n",
    "Is there any difference in the behaviour of the different sectors ?\\\n",
    "\n",
    "::: {#tab:anovaEx}\n",
    "  ------------ --- ---- --- ---- --- ---- --- ---- --- ---- ---- ---\n",
    "  Industrial    0   1    1   -1   2   -1   0   -4   4   -1   2    0\n",
    "  Financial     3   5    1   3    0   0    1   -1   0   -7   -3  \n",
    "  Textiles      0   -2   8   3    7   -7                         \n",
    "  ------------ --- ---- --- ---- --- ---- --- ---- --- ---- ---- ---\n",
    "\n",
    "  : [\\[tab:anovaEx\\]]{#tab:anovaEx label=\"tab:anovaEx\"}The shares of\n",
    "  industrial, financial, and textile sectors for a particular day see.\n",
    ":::\n",
    "\n",
    "We can compute:\n",
    "\n",
    "-   total average: $\\bar{x}$ = 0.48\n",
    "\n",
    "-   average for group 1: $\\bar{x}_1$ = 0.25; $\\bar{x}_1 - \\bar{x}$ =0.23\n",
    "\n",
    "-   average for group 2: $\\bar{x}_2$ = 0.18; $\\bar{x}_2 - \\bar{x}$ =0.30\n",
    "\n",
    "-   average for group 3: $\\bar{x}_3$ = 1.5; $\\bar{x}_3 - \\bar{x}$ =1.02\n",
    "\n",
    "Computing the numerator we get\n",
    "$1/2 (12\\times 0.23^2 + 11 \\times 0.30^2 + 6\\times 1.02^2) = 3.93$.\n",
    "Computing the variances within the groups, for the denominator we have\n",
    "$(44.3 + 103.6 + 161.5) / 26 = 11.9$.\\\n",
    "The numerator (between) is even smaller than the denominator (within),\n",
    "so we can say that there is no evidence for different behaviour in the\n",
    "different sectors. If the numerator was larger than the denominator we\n",
    "should have used the critical values for the $F-$test.\n",
    "\n",
    "Resampling techniques {#sec:resampling}\n",
    "---------------------\n",
    "\n",
    "Rasampling is a technique used for non-parametric estimation of the\n",
    "statistical uncertainty (bias and variance) of a statistical estimator.\n",
    "To get an idea of non-parametric statistics see e.g. wiki. In this\n",
    "section we will review the two most used: jackknife and bootstrap\n",
    "[@ResamplingArticle].\\\n",
    "\\\n",
    "Non parametric techniques allow to *generalize the concept of\n",
    "uncertainty to any estimator*. Take as an example a random sample of\n",
    "size $n$, $\\{ x_i\\}$ with $i=1,\\ldots,n$, taken from an unknown parent\n",
    "distribution $F$. Typically you will characterize the sample by\n",
    "computing the estimated mean and its standard deviation:\n",
    "$$\\bar{x} = \\frac{\\sum_i x_i}{n} \\qquad; \\qquad \\hat{\\sigma}=\\left( \\frac{1}{n-1} \\sum_i (x_i - \\bar{x})^2  \\right)^{1/2}$$\n",
    "The issue with this expression for the estimated standard deviation is\n",
    "that it does not generalize to any other estimator but the mean. For\n",
    "instance there is no way to extend it to compute the the standard\n",
    "deviation of the median. The resampling techniques allow to make this\n",
    "generalization.\\\n",
    "\n",
    "### Jackknife\n",
    "\n",
    "Let's first introduce some notation. Let\n",
    "$$\\bar{x}_{(i)} = \\frac{n\\bar{x} -x_i}{n-1} = \\frac{1}{n-1}\\sum_{j \\neq i} x_j$$\n",
    "be the average of the dataset constructed by the initial dataset, but\n",
    "removing the i-th element (the new sample will have n-1 elements), also\n",
    "known as the \"deleted average\". Let\n",
    "$$\\bar{x}_{(\\cdot)} = \\sum_i \\frac{\\bar{x}_{(i)}}{n}$$ be the average of\n",
    "the sample at hand, i.e. the average of the averages computed on the\n",
    "samples where we have removed the i-th element (the average of the\n",
    "deleted averages).\\\n",
    "The **jackkinfe estimate of the standard error** is defined as:\n",
    "$$\\bar{\\sigma}_J = \\left[\\frac{n-1}{n} \\sum_{i=1}^n (\\bar{x}_{(i)} - \\bar{x}_{(\\cdot)})^2   \\right]^{1/2}$$\n",
    "which is a different way of rewriting the standard deviation, i.e.\n",
    "$$\\hat{\\sigma} = \\bar{\\sigma}_J.$$ The advantage of this expression is\n",
    "that it can trivially be generalized to any estimator $\\hat{\\theta}$\n",
    "function of the $\\{x_i\\}$ dataset:\n",
    "$\\hat{\\theta} = \\theta(x_1,\\ldots,x_n)$. Just replace $$\\begin{aligned}\n",
    "        \\bar{x}_{(i)}     &\\to& \\hat{\\theta}_{(i)} = \\hat{\\theta}(x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n)\\\\\n",
    "        \\bar{x}_{(\\cdot)} &\\to& \\hat{\\theta}_{(\\cdot)} = \\frac{\\sum_i \\hat{\\theta}_{(i)}}{n}\\end{aligned}$$\n",
    "The jackknife in general perform less well than the bootstrap method\n",
    "that we will see in the next section, but it is often preferred because\n",
    "computationally less expensive.\n",
    "\n",
    "### Bootstrap\n",
    "\n",
    "The bootstrap represents a generalization of the jackknife. Again let's\n",
    "start with some notation. Let $\\hat{F}$ be the empirical cumulative\n",
    "probability distribution function (ECDF) of the data: $$\\hat{F} (x) = \n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    " 0           & , & x<x_{1}\\\\\n",
    " \\frac{r}{n} & , & x_{r} \\leq x < x_{r+1}\\\\\n",
    " 1           & , & x_{n} \\leq x\\\\\n",
    "\\end{array}\n",
    "\\right.$$ and let's define $\\{x_i^*\\} = \\{x_1^*, \\ldots x_n^*\\}$ a\n",
    "random sample taken from $\\hat{F}$, i.e. the $x_i^*$ are drawn\n",
    "independently with repetition from $\\{x_i\\} = \\{x_1, \\ldots x_n\\}$.\\\n",
    "For each of these samples we can compute the average and the variance:\n",
    "$$\\bar{x}^* = \\frac{\\sum_i x^*}{n} \\qquad ; \\qquad var_\\cdot \\bar{x}^* = \\frac{1}{n^2} \\sum_i (x_i - \\bar{x})^2$$\n",
    "where the dot represents the sample at hand.\\\n",
    "As before with the jackknife this definition can be generalized to any\n",
    "estimator $\\bar{x}_{(\\cdot)} \\to \\hat{\\theta}_{(i)}$ as\n",
    "$$\\hat{\\sigma}_B = \\left[ var_\\cdot \\bar{\\theta}(x_1^*,\\ldots,x_n^*)\\right]^{1/2}$$\n",
    "and it can be verified that\n",
    "$$\\sqrt{\\frac{n}{n-1}}~\\sigma_B = \\hat{\\sigma}.$$ To clarify the\n",
    "procedure let's take the correlation coefficient $\\rho$ as an example\n",
    "for which we have the expression for its variance:\n",
    "$\\hat{\\sigma} = \\frac{1-\\hat{\\rho}^2}{\\sqrt{n-3}}$. In\n",
    "Fig. [1.15](#fig:dataBootstrap){reference-type=\"ref\"\n",
    "reference=\"fig:dataBootstrap\"} is reported a dataset made of 15 points,\n",
    "each representing the average LSAT and GPA [^6] scores for students\n",
    "coming from school $i$ entering the american law school in 1973. This\n",
    "plot shows the correlation between the results of the admission exam\n",
    "LSAT and the average grades of undergraduate students from a school.\n",
    "From these data, we can estimate the sample correlation\n",
    "$\\hat{\\rho}=0.776$ and its uncertainty $\\hat{\\sigma} = 0.115$.\n",
    "\n",
    "![[\\[fig:dataBootstrap\\]]{#fig:dataBootstrap\n",
    "label=\"fig:dataBootstrap\"}Data taken from\n",
    "[@ResamplingArticle]](Section8Bilder/dataBootstrap.png){#fig:dataBootstrap\n",
    "width=\"50%\"}\n",
    "\n",
    "Now let's compute the sample correlation and its uncertainty using the\n",
    "bootstrap method writing esplicitly the algorithm.\n",
    "\n",
    "-   Build the ECDF $\\hat{F}$ on the bivariate distribution from the\n",
    "    dataset given above.\n",
    "\n",
    "-   Draw a \"bootstrap sample\" $(x_1^*,\\ldots,x_n^*)$, i.e. take $n$\n",
    "    random draws with repetition from $(x_1,\\ldots,x_n)$\n",
    "\n",
    "-   Compute the value of the statistics (in this case the correlation\n",
    "    coefficient) from the bootstrapped sample\n",
    "    $\\hat{\\rho}^* = \\hat{\\rho}(x_1^*,\\ldots,x_n^*)$\n",
    "\n",
    "-   Repeat steps 2) and 3) a large number of times $B$ obtaining $B$\n",
    "    estimations of the correlation coefficient:\n",
    "    $\\hat{\\rho}^{*1}, \\hat{\\rho}^{*2},\\ldots,\\hat{\\rho}^{*B}$\n",
    "\n",
    "-   Finally compute\n",
    "    $\\hat{\\sigma}_B = \\left( \\sum_{b=1}^B(\\hat{\\rho}^{*b} - \\hat{\\rho}^{*\\cdot} )^2/(B-1)  \\right)^{1/2}$;\n",
    "    where $\\hat{\\rho}^{*\\cdot} = \\frac{\\sum \\hat{\\rho}^{*b}}{B}$\n",
    "\n",
    "Fig. [1.16](#fig:bootstrap){reference-type=\"ref\"\n",
    "reference=\"fig:bootstrap\"} shows the ditribution of\n",
    "$\\hat{\\rho}^* - \\hat{\\rho}$, i.e. $\\hat{\\rho}^* -0.776$ the residual\n",
    "between the bootstrapped estimations and the $\\rho$ computed on the\n",
    "dataset, using the esplicit formula. The plot has been obtained using\n",
    "$B = 1000$, i.e. computing\n",
    "$\\hat{\\rho}^{*1}, \\hat{\\rho}^{*2},\\ldots,\\hat{\\rho}^{*1000}$. Using the\n",
    "algorithm above we obtained $\\hat{\\sigma}_B = 0.127$ to be compared with\n",
    "the $\\hat{\\sigma} = 0.115$.\\\n",
    "\\\n",
    "\n",
    "![[\\[fig:bootstrap\\]]{#fig:bootstrap label=\"fig:bootstrap\"} Histogram of\n",
    "$B=1000$ bootstrap replication of $\\hat{\\rho}^*$ for the law school\n",
    "data. The normal theory density curve is overlapped\n",
    "[@ResamplingArticle].](Section8Bilder/bootstrap.png){#fig:bootstrap\n",
    "width=\"70%\"}\n",
    "\n",
    "\\\n",
    "Another application of the resampling techniques is the estimation of\n",
    "the bias (this is what the resampling was originally designed for). Let\n",
    "$\\hat{\\theta} = \\theta(\\hat{F})$ and define the bias\n",
    "$\\beta = E[\\theta(\\hat{F}) - \\theta(F)]$, where F is the unknown parent\n",
    "distribution and $\\hat{F}$ is the sampled distribution. We can consider\n",
    "the bias just as another estimator and so we \"bootstrap the bias\" i.e.\n",
    "compute the bootstrap estimate of the bias.\n",
    "$$\\hat{\\beta}_B = \\beta(\\hat{F}) = E_*[\\theta(\\hat{F}^*) - \\theta(\\hat{F})]$$\n",
    "where $E_*$ is the expectation value on the bootstrap sampling and\n",
    "$\\hat{F}^*$ is the ECDF of the bootstrapped sample.\\\n",
    "$\\beta_B$ is computed with the same algorithm we used before to compute\n",
    "$\\sigma_B$:\n",
    "$$\\beta_B = \\hat{\\theta}^{*\\cdot} - \\hat{\\theta} = \\frac{1}{B}\\sum_{b=1}^B (\\hat{\\theta}^{*\\cdot} - \\hat{\\theta})$$\\\n",
    "**Example** A common application of the resampling techniques is when\n",
    "you apply two different estimators of the same parameter to the dataset\n",
    "at hand and you ask yourself if the two estimates are compatible. We\n",
    "will follow an example from literature [@jackknifeHgg].\\\n",
    "The estimates of the best fit of the signal strength modifier of the\n",
    "Higgs decaying into two photons are: for the first method\n",
    "$0.54^{+0.31}_{-0.26}$ for the second $0.96^{+0.37}_{-0.33}$.\n",
    "Fig. [1.18](#fig:jackknifeHgg){reference-type=\"ref\"\n",
    "reference=\"fig:jackknifeHgg\"} is the excerpt from the paper discussing\n",
    "the compatibility.\n",
    "\n",
    "![[\\[fig:jackknifeHgg\\]]{#fig:jackknifeHgg label=\"fig:jackknifeHgg\"}\n",
    "$H\\to\\gamma\\gamma$ compatibility\n",
    "studies.](Section8Bilder/jackknifeHgg1.png \"fig:\"){#fig:jackknifeHgg\n",
    "width=\"70%\"} ![[\\[fig:jackknifeHgg\\]]{#fig:jackknifeHgg\n",
    "label=\"fig:jackknifeHgg\"} $H\\to\\gamma\\gamma$ compatibility\n",
    "studies.](Section8Bilder/jackknifeHgg2.png \"fig:\"){#fig:jackknifeHgg\n",
    "width=\"70%\"}\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "-   G. Cowan [@CowanBook], \"Statistical Data Analysis\",Ch. 4\n",
    "\n",
    "-   R. Barlow [@Barlow], \" A guide to the use of statistical methods in\n",
    "    the physical sciences\". Ch. 8\n",
    "\n",
    "-   W. Metzger [@Metzger], \"Statistical Methods in Data Analysis\": Ch.10\n",
    "\n",
    "-   B. Efron and G. Gong , \"A Leisurely Look at the Bootstrap, the\n",
    "    Jackknife, and Cross-Validation\", The American Statistician, Vol.\n",
    "    37, No. 1, (Feb., 1983), pp. 36-48\n",
    "\n",
    "-   CMS collaboration, \"Updated measurements of the Higgs boson at 125\n",
    "    GeV in the two photon decay channel\", CMS-PAS-HIG-13-001\n",
    "\n",
    "[^1]: In a test beam, a beam of particles is prepared in a well defined\n",
    "    condition (particle type, energy, etc\\...) and it is typically used\n",
    "    to test a device under development. This configuration inverts the\n",
    "    typical experimental conditions where a device with known properties\n",
    "    is used to characterize particles in a beam or from collisions.\n",
    "\n",
    "[^2]: The typical criticism coming with this choice is on how we can be\n",
    "    sure that we control the test statistics to such extreme tails. The\n",
    "    typical answer is that such small values allow to a certain extent\n",
    "    to also cover for possible mis-estimations of systematic\n",
    "    uncertainties. It's all hand-waving. The choice of the threshold to\n",
    "    claim a discovery remains subjective.\n",
    "\n",
    "[^3]: The signal doesn't always appear as an excess of events. In case\n",
    "    of test for neutrino oscillations the signal can appear as a deficit\n",
    "    of events.\n",
    "\n",
    "[^4]: Here the function is given, not fitted on data.\n",
    "\n",
    "[^5]: In 1-D the ordering is trivial, ascending/descending, in n-D it is\n",
    "    arbitrary, you have to choose a convention and map it to a 1D\n",
    "    sequence.\n",
    "\n",
    "[^6]: LSAT = Law School Admission Test, GPA = undergraduate Grade Point\n",
    "    Average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
