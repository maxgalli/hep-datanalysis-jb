
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parameter Estimation - Likelihood {#ChapterParameterEstimations} &#8212; Statistical Methods and Data Analysis Techniques</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Parameter Estimation - Least Squares {#sec:chi2}" href="leastSquares.html" />
    <link rel="prev" title="Statistical inference" href="inference.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Methods and Data Analysis Techniques</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probabilityDistributions.html">
   Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="errors.html">
   Measurements uncertainties {#ch:errors}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="monteCarlo.html">
   Monte Carlo methods {#sec:MC}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Statistical inference
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Parameter Estimation - Likelihood  {#ChapterParameterEstimations}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="leastSquares.html">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hypothesisTesting.html">
   Hypotheses Testing {#ChapterHypothesisTesting}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="confidenceIntervals.html">
   Confidence Intervals {#ChapterConfidenceLimits}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mva.html">
   Multivariate Analysis Methods {#ChapterMVA}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unfolding.html">
   Unfolding {#ch:Unfolding}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/likelihood.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties-of-the-estimators-sec-propestimator">
   Properties of the estimators {#sec:propEstimator}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-of-the-mean">
   Estimation of the Mean
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-of-the-variance">
   Estimation of the Variance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-method-sec-likelihood-sec-likelihood-label-sec-likelihood">
   Maximum Likelihood Method[[sec:likelihood]]{#sec:likelihood label=”sec:likelihood”}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimum-variance-bound">
   Minimum Variance Bound
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information">
     Information
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rao-cramer-frechet-inequality-sec-rcf">
     Rao-Cramér-Frechet inequality {#sec:RCF}
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uncertainty-for-ml-estimators-sec-mlunc">
   Uncertainty for ML estimators {#sec:MLunc}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binned-maximum-likelihood">
   Binned Maximum Likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extended-maximum-likelihood-method-sec-emlf">
   Extended Maximum Likelihood Method {#sec:EMLF}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combination-of-measurements-with-the-ml-method">
   Combination of Measurements with the ML Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constraining-parameters">
   Constraining parameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-general-remarks-concerning-ml-estimators-sec-mlremarks">
   Some general remarks concerning ML estimators {#sec:MLremarks}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="parameter-estimation-likelihood-chapterparameterestimations">
<h1>Parameter Estimation - Likelihood  {#ChapterParameterEstimations}<a class="headerlink" href="#parameter-estimation-likelihood-chapterparameterestimations" title="Permalink to this headline">¶</a></h1>
<p>Take a random variable <span class="math notranslate nohighlight">\(x\)</span> described by a pdf <span class="math notranslate nohighlight">\(f(x)\)</span>: the <strong>sample
space</strong> is defined to be the set of all possible values of <span class="math notranslate nohighlight">\(x\)</span>. The set
of <span class="math notranslate nohighlight">\(n\)</span> independent measurements of the random variable <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(\{x_i\}\)</span> is
called a <strong>sample of size</strong> <span class="math notranslate nohighlight">\(n\)</span>.<br />
In theory of probability, from the pdf <span class="math notranslate nohighlight">\(f(x)\)</span> we can compute all sorts
of quantities (mean, moments, etc…). In statistical inference we are
concerned about the opposite problem: take the distribution of a
quantity measured in data and infer its parent pdf <span class="math notranslate nohighlight">\(f(x)\)</span>. In the
simplest case of data distributed following a known pdf which depends on
a parameter <span class="math notranslate nohighlight">\(\theta\)</span> (i.e. <span class="math notranslate nohighlight">\(f(x,\theta)\)</span>), the statistical inference is
reduced to the extraction of the best estimate of the parameter from
data.<br />
Often when talking about an estimate we use the adjectives “accurate”
and “precise”. In what follows we mean (see
Fig. <a class="reference external" href="#fig:PrecAcc">1.1</a>{reference-type=”ref” reference=”fig:PrecAcc”}):</p>
<ul class="simple">
<li><p><em>accuracy:</em> how close is the estimated value to the true reference
value</p></li>
<li><p><em>precision:</em> how reproducible the measurements are.</p></li>
</ul>
<p>This means for instance that a poorly calibrated device can give you
high precision but poor accuracy.\</p>
<p><img alt="Meaning of &quot;accuracy&quot; and &quot;precision&quot;[[fig:PrecAcc]]{#fig:PrecAcclabel=&quot;fig:PrecAcc&quot;}" src="Section6Bilder/PrecAcc.pdf" />{#fig:PrecAcc
width=”60%”}\</p>
<p>Any function of the observed measurements is called a <strong>statistic</strong>. A
statistic used to estimate some parameter of a distribution is called
<strong>estimator</strong>. We will generally denote an estimator of a parameter by
adding a circumflex (<span class="math notranslate nohighlight">\(\;\hat{ }\;\)</span>) to the symbol of the parameter:
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span>.<br />
You can build several estimators for any parameter. As an example take
the estimation of the average height of all students enrolled at ETH. Be
<span class="math notranslate nohighlight">\(h_i\)</span> the outcome of the measurement of each of the <span class="math notranslate nohighlight">\(N\)</span> students, then
any of the following procedures would produce an estimate:</p>
<ul class="simple">
<li><p>add all <span class="math notranslate nohighlight">\(h_i\)</span> and divide by N</p></li>
<li><p>add only the first 15, divide by 15; ignore the rest</p></li>
<li><p>add all <span class="math notranslate nohighlight">\(h_i\)</span> and divide by N-1</p></li>
<li><p>just quote it to be 1.82 m</p></li>
<li><p>multiply all <span class="math notranslate nohighlight">\(h_i\)</span> and take <span class="math notranslate nohighlight">\(N^{th}\)</span>-root of result</p></li>
<li><p>choose the most popular height (mode)</p></li>
<li><p>take shortest and tallest and divide by 2</p></li>
<li><p>add 2<span class="math notranslate nohighlight">\(^{nd}\)</span>, 4<span class="math notranslate nohighlight">\(^{th}\)</span>, 6<span class="math notranslate nohighlight">\(^{th}\)</span>,… and divide by N/2 [ or
(N-1)/2 if N odd]</p></li>
<li><p>take only <span class="math notranslate nohighlight">\(h_i\)</span> of students with brown hair, divide by M</p></li>
</ul>
<p>All these are by definition estimators. Some appear to be clearly better
than others, but how do we define what is a better/worse estimator? To
answer this question we define some general properties of the
estimators: bias, consistency, efficiency and robustness.</p>
<div class="section" id="properties-of-the-estimators-sec-propestimator">
<h2>Properties of the estimators {#sec:propEstimator}<a class="headerlink" href="#properties-of-the-estimators-sec-propestimator" title="Permalink to this headline">¶</a></h2>
<p>The estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> being a function of random variables (data)
is itself a random variable and it will be distributed according to a
pdf <span class="math notranslate nohighlight">\(g(\hat{\theta}|\theta)\)</span>, which will clearly depend on the parameter
<span class="math notranslate nohighlight">\(\theta\)</span>. We define the following properties for an estimator (see
Fig. <a class="reference external" href="#fig:estProp">1.2</a>{reference-type=”ref” reference=”fig:estProp”}):</p>
<p><img alt="Some estimator properties[[fig:estProp]]{#fig:estProplabel=&quot;fig:estProp&quot;}" src="Section6Bilder/estProp.pdf" />{#fig:estProp
width=”60%”}\</p>
<ul class="simple">
<li><p>An estimator is called <em>unbiased</em> if its expectation value is equal
to the true value: <span class="math notranslate nohighlight">\(&lt;\hat{\theta}&gt;=\theta\)</span>. Thus an estimator is
biased if <span class="math notranslate nohighlight">\(b_n = &lt;\hat{\theta}&gt; - \theta \ne 0\)</span>. The number <span class="math notranslate nohighlight">\(b_n\)</span> is
called the bias of the estimator. We include the subscript <span class="math notranslate nohighlight">\(n\)</span> in
this definition since we will see that some estimators are unbiased
only asymptotically, i.e. only for <span class="math notranslate nohighlight">\(n\to \infty\)</span>. An example of an
unbiased estimator is the mean (<span class="math notranslate nohighlight">\(\langle \bar{\mu} \rangle = \mu\)</span>);
the third in the list of the previous section is asymptotically
unbiased (<span class="math notranslate nohighlight">\(\langle \hat{\mu} \rangle = n/(n-1)\mu\)</span>) and so
<span class="math notranslate nohighlight">\(b_n(\hat{\mu}) \to 0\)</span> for <span class="math notranslate nohighlight">\(n\to \infty\)</span>. If we know the bias, we
can construct an unbiased estimator by correcting it.</p></li>
<li><p>An estimator is called <em>consistent</em> if <span class="math notranslate nohighlight">\(\forall \epsilon &gt; 0\)</span>,
<span class="math notranslate nohighlight">\(\lim_{n \to \infty} P(|\hat{\theta}-\theta| \ge \epsilon) = 0\)</span>. For
instance if <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the average of data distributed
according to a p.d.f. where we can apply the CLT, then
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a consistent estimator because
<span class="math notranslate nohighlight">\(N(\bar{x}; \mu, \sigma^2/n)\)</span> tends to a delta function for
<span class="math notranslate nohighlight">\(n \to \infty\)</span>. In the list of the previous section for example, the
first and the third are consistent the second is not.</p></li>
<li><p>An estimator is called <em>efficient</em> if it has the smallest possible
variance of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> (see later in this section the minimum
variance bound). The efficiency <span class="math notranslate nohighlight">\(\epsilon\)</span> is defined as
<span class="math notranslate nohighlight">\(\epsilon=\frac{{\rm minimal\, Variance\, of\,} \hat{\theta}}{{\rm Variance\, of\,} \hat{\theta}}\)</span>.</p></li>
<li><p>An estimator is called <em>robust</em> if it is insensitive to wrong data
or wrong assumptions, especially in the tails of a distribution.</p></li>
</ul>
</div>
<div class="section" id="estimation-of-the-mean">
<h2>Estimation of the Mean<a class="headerlink" href="#estimation-of-the-mean" title="Permalink to this headline">¶</a></h2>
<p>The estimator for the mean <span class="math notranslate nohighlight">\(\mu\)</span> obtained from <span class="math notranslate nohighlight">\(n\)</span> independent
measurements <span class="math notranslate nohighlight">\(x_{i}\)</span> is: $<span class="math notranslate nohighlight">\(\label{estimate_mean}
\hat{\mu}=\frac{1}{n}\sum_i x_i.\)</span><span class="math notranslate nohighlight">\( This estimator is unbiased, i.e.
\)</span>\langle\hat{\mu}\rangle=\langle \frac{1}{n}\sum_i x_i\rangle=\frac{1}{n}\sum_i\langle x_i\rangle = \mu<span class="math notranslate nohighlight">\(.
Furthermore it is consistent because of the CLT. Its variance is given
by \)</span><span class="math notranslate nohighlight">\(V(\hat{\mu})=\frac{1}{n}\sigma^2.\)</span><span class="math notranslate nohighlight">\( Whether this estimator is
efficient or not depends on the p.d.f. of the parent distribution. For
instance, given a uniform distribution the mean is not the most
efficient estimator; the estimator \)</span>\hat{\mu}=0.5(x_{max}+x_{min})$ has
a smaller variance. The robustness for the sample mean is increased if
the truncated mean is used. This means that the largest and smallest
values are trimmed (truncated). This more robust mean is less sensitive
to outliers, but unless the parent distribution is symmetric it will be
biased. An example for a truncated mean can be found in sports rating
when only 4 out of 6 grades are used to form the final grade.</p>
</div>
<div class="section" id="estimation-of-the-variance">
<h2>Estimation of the Variance<a class="headerlink" href="#estimation-of-the-variance" title="Permalink to this headline">¶</a></h2>
<p>An estimator for the variance of a parent distribution <span class="math notranslate nohighlight">\(\sigma^2\)</span>, when
we know the true mean <span class="math notranslate nohighlight">\(\langle x \rangle = \mu\)</span> is:
$<span class="math notranslate nohighlight">\(s_1^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2.\)</span><span class="math notranslate nohighlight">\( This estimator is
unbiased \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
&lt;s_1^2&gt; &amp;=&amp; \frac{1}{n}\langle\sum(x_i - \mu)^2\rangle  \\
        &amp;=&amp; \frac{1}{n}\left(\langle \sum x_i^2 \rangle - 2\mu\langle \sum x_i \rangle + n \mu^2\right) \\
        &amp;=&amp; \frac{1}{n}\left( n \langle  x^2 \rangle - 2n\mu\langle x \rangle + n \mu^2\right)  \qquad (independent~x_i~so~: \langle\sum x_i^2\rangle= n\langle x^2 \rangle)\\
        &amp;=&amp; \langle x^2 \rangle - 2\mu^2 + \mu^2 \\
        &amp;=&amp; \sigma^2 -\mu^2 + \mu^2  \qquad (\sigma^2 = \langle x^2 \rangle - \mu^2) \\
        &amp;=&amp; \sigma^2        \\\end{aligned}\)</span><span class="math notranslate nohighlight">\( So \)</span>s_1^2<span class="math notranslate nohighlight">\( is an unbiased
estimator of the variance of the parent p.d.f \)</span>\sigma^2<span class="math notranslate nohighlight">\(, when \)</span>\mu<span class="math notranslate nohighlight">\( is
known.\
When it is not known, we use the estimate \)</span>\bar{x} = \hat{\mu}<span class="math notranslate nohighlight">\( and
define:
\)</span><span class="math notranslate nohighlight">\(s_x^2 = \frac{1}{n}\sum(x_i-\bar{x})^2 = \bar{x^2} - \bar{x}^2\)</span><span class="math notranslate nohighlight">\( The
expectation value of \)</span>s_x^2<span class="math notranslate nohighlight">\( is: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\langle s_x^2 \rangle= \frac{1}{n}\left( \langle\sum x_i^2\rangle- \frac{1}{n}\langle\left( \sum x_i  \right)^2  \rangle\right).\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Substituting: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 \langle\sum x_i^2\rangle&amp;=&amp; n\langle x^2 \rangle\\
 \sigma^2 &amp;=&amp; \langle x^2\rangle- \mu^2\\
 V\left( \sum x_i \right) &amp;=&amp; \langle\left( \sum x_i\right)^2\rangle- \left( \langle\sum x_i\rangle\right)^2\end{aligned}\)</span><span class="math notranslate nohighlight">\(
we get
\)</span><span class="math notranslate nohighlight">\(\langle s_x^2\rangle= \frac{1}{n}\left( n(\sigma^2+\mu^2) - \frac{1}{n}\left( \langle\left( \sum x_i\right)^2 \rangle+ \langle\sum x_i\rangle^2 \right)  \right)\)</span><span class="math notranslate nohighlight">\(
and using \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 V(\langle\sum x_i\rangle) &amp;=&amp; \sum V(x_i) = nV(x) = n\sigma^2 \\
\langle\sum x_i\rangle&amp;=&amp; n \langle x \rangle= n \mu\end{aligned}\)</span><span class="math notranslate nohighlight">\( we
finally have
\)</span><span class="math notranslate nohighlight">\(\langle s_x^2 \rangle= \frac{1}{n}\left( n\sigma^2 + n \mu^2 - \frac{1}{n}(n\sigma^2 + (n\mu)^2 ) \right) = \frac{1}{n}(n-1) \sigma^2 .\)</span><span class="math notranslate nohighlight">\(
This means that \)</span>s_x^2<span class="math notranslate nohighlight">\( is a biased estimator of \)</span>\sigma^2<span class="math notranslate nohighlight">\(. The reason
is that we used the sample mean \)</span>\bar{x}<span class="math notranslate nohighlight">\( as an estimator of the true
mean \)</span>\mu<span class="math notranslate nohighlight">\(. The spread of the data around the sample mean is less than
the spread around the true mean and since the variance is the spread
around the true mean \)</span>s_x^2<span class="math notranslate nohighlight">\( underestimates the true variance.\
The formula for the variance we are used to is the one with the
corrected bias:
\)</span><span class="math notranslate nohighlight">\(s^2 = \frac{n}{n-1} s_x^2 = \frac{n}{n-1}(&lt;x^2&gt; - &lt;x&gt;^2) = \frac{1}{n-1}\sum (x_i -&lt;x&gt;)^2\)</span><span class="math notranslate nohighlight">\(\
In the same way as we got to the unbiased estimator of the variance, we
obtain the expression for the unbiased estimator of the covariance
\)</span>V_{xy}<span class="math notranslate nohighlight">\( of two random variables \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\( with unknown (but
estimated) means
\)</span><span class="math notranslate nohighlight">\(\hat{V}_{xy}=\frac{1}{n-1}\sum(x_i-&lt;x&gt;)(y_i-&lt;y&gt;)=\frac{n}{n-1}(&lt;xy&gt;-&lt;x&gt;&lt;y&gt;).\)</span><span class="math notranslate nohighlight">\(
The correlation coefficient is then given by
\)</span><span class="math notranslate nohighlight">\(\rho_{xy}=\frac{\hat{V}_{xy}}{s_xs_y}.\)</span>$</p>
</div>
<div class="section" id="maximum-likelihood-method-sec-likelihood-sec-likelihood-label-sec-likelihood">
<h2>Maximum Likelihood Method[[sec:likelihood]]{#sec:likelihood label=”sec:likelihood”}<a class="headerlink" href="#maximum-likelihood-method-sec-likelihood-sec-likelihood-label-sec-likelihood" title="Permalink to this headline">¶</a></h2>
<p>Assume we have <span class="math notranslate nohighlight">\(n\)</span> measurements of a random variable <span class="math notranslate nohighlight">\(x\)</span>, distributed
according to a known probability density function <span class="math notranslate nohighlight">\(f(x|\theta)\)</span>. Where
<span class="math notranslate nohighlight">\(\theta\)</span> stands for one parameter of the p.d.f. (e.g. the mean). Note
that here you assume you know what is the correct pdf to fit and you are
“only” interested in finding the value of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> that
allows the model to best fit the data.<br />
<br />
Can we find a <em>general</em> method to build an estimator for <span class="math notranslate nohighlight">\(\theta\)</span> ( i.e.
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span> ) ? The procedure we present here goes under the name of
<strong>maximum likelihood method</strong> and it is the most intuitive way to set up
such an estimator.<br />
<br />
To understand the maximum likelihood method for parameter estimation
(sometimes abbreviated as ML method) we start from the probability
<span class="math notranslate nohighlight">\(f(x|\theta)dx\)</span> (i.e. the probability to observe <span class="math notranslate nohighlight">\(x\in (x,x+dx)\)</span> given
<span class="math notranslate nohighlight">\(\theta\)</span>). With this we can compute the probability to observe a certain
set of data <span class="math notranslate nohighlight">\(\{x_i\}\)</span> <em>given</em> the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, as the joint
probability:
$<span class="math notranslate nohighlight">\(P= f(x_1|\theta)dx_1\cdot f(x_2|\theta)dx_2\cdot \ldots \cdot f(x_n|\theta)dx_n\)</span><span class="math notranslate nohighlight">\(
The **likelihood function**[^1] is then defined as:
\)</span><span class="math notranslate nohighlight">\(L(\theta)=f(x_1|\theta)\cdot f(x_2|\theta)\cdots f(x_n|\theta)=\prod_{i = 1}^{Nevts} f(x_i|\theta).\)</span><span class="math notranslate nohighlight">\(
where the product runs over all the events in the data sample \)</span>{x_i}<span class="math notranslate nohighlight">\(
and \)</span>L(\theta)<span class="math notranslate nohighlight">\( is normalized to 1 for all values of \)</span>\theta<span class="math notranslate nohighlight">\(.\
\
The function \)</span>L(\theta)<span class="math notranslate nohighlight">\( is, for a given data sample, a function of only
the parameter \)</span>\theta<span class="math notranslate nohighlight">\( and it gives us the probability to get with this
choice of the parameter \)</span>\theta<span class="math notranslate nohighlight">\( the measured values \)</span>{x_{i}}<span class="math notranslate nohighlight">\(. The
likelihood function is *not* a probability density function in the
parameters \)</span>\theta<span class="math notranslate nohighlight">\( (if that was the case we would calculate explicitly
the expectation value of \)</span>\theta<span class="math notranslate nohighlight">\( and all its higher moments).\
\
The **ML principle** states that the best estimate of \)</span>\theta<span class="math notranslate nohighlight">\( is given
by the estimator \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( which maximizes \)</span>L(\theta)<span class="math notranslate nohighlight">\(, i.e. the
value which maximizes the probability to obtain the observed set of
observed data \)</span>{x_{i}}<span class="math notranslate nohighlight">\( given \)</span>\theta<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\hat{\theta} = max_\theta~L(\theta)\)</span><span class="math notranslate nohighlight">\( The maximum is then computed by
differentiating \)</span>dL(\theta) / d\theta = 0<span class="math notranslate nohighlight">\(.\
The concept is trivially generalized to several parameters \)</span>a_{k}<span class="math notranslate nohighlight">\(
requiring: \)</span>\partial L/\partial a_k=0 \ \ \forall k<span class="math notranslate nohighlight">\(.\
\
In practice we often work with the (natural) logarithm of the likelihood
function \)</span>l(\theta) = \ln L(\theta)<span class="math notranslate nohighlight">\( (slang: the *log-likelihood*). The
reason for this is that likelihoods are usually calculated with
computers and the product of several probabilities (i.e. numbers smaller
than 1) will hit the numerical precision of the machine. The logarithm
transforms the product into a sum which does not create precision
problems. Since the logarithm is a monotonic rising function, the value
that maximizes \)</span>L(\theta)<span class="math notranslate nohighlight">\( also maximizes \)</span>\ln L(\theta)<span class="math notranslate nohighlight">\(, and our
condition becomes:
\)</span><span class="math notranslate nohighlight">\(l(\theta)=\ln L(\theta)=\sum_{i=1}^{Nevts} \ln f(x_i|\theta) = \mbox{Maximum}.\)</span><span class="math notranslate nohighlight">\(
Any monotonic transformation of the likelihood will leave its maximum
unchanged.\
It has to be stressed again that the ML estimation yields a value
\)</span>\hat{\theta}<span class="math notranslate nohighlight">\(, for which the observed data are the most &quot;likely\&quot;
(compared to other parameter values), and not vice-versa! It is not to
be confused with the statement that the parameter \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( is the
most probable value.\
\
**Example** Take the probability density given by
\)</span>f(x|\theta) = 1 + \theta(x-0.5)<span class="math notranslate nohighlight">\( with \)</span>x<span class="math notranslate nohighlight">\( between 0 and 1. The provided
sample data \)</span>{x_{i}}= {0.89, 0.03, 0.5, 0.36, 0.49}<span class="math notranslate nohighlight">\(. The
log-likelihood function is then given by \)</span><span class="math notranslate nohighlight">\(\label{eq2_sect4}
  l(\theta)=\sum_{i=1}^5 \ln(1+\theta(x_i-0.5))\)</span>$ (see
Fig. <a class="reference external" href="#fig1_sect4">1.3</a>{reference-type=”ref” reference=”fig1_sect4”}).</p>
<p><img alt="The log-likelihood function fromEq. [eq2_sect4]{reference-type=&quot;ref&quot;reference=&quot;eq2_sect4&quot;}[[fig1_sect4]]{#fig1_sect4label=&quot;fig1_sect4&quot;}" src="Section6Bilder/LogLikelihoodJPG.jpg" />{#fig1_sect4
width=”80%”}\</p>
<p>The maximum of the log-likelihood function can be determined graphically
to be <span class="math notranslate nohighlight">\(\hat{\theta} = -0.6\)</span>.<br />
<br />
The numerical libraries in use to solve optimization problems search for
minima, not maxima. To use a minimization program to find a maximum you
just need to flip the sign of the likelihood: the problem is trivially
moved from a maximization to a minimization. In the following we will
typically work with the <span class="math notranslate nohighlight">\(-\ln L(\theta)\)</span> (slang: negative log-likelihood
or NLL). See App. <a class="reference external" href="#app:MIGRAD">[app:MIGRAD]</a>{reference-type=”ref”
reference=”app:MIGRAD”} for the description of a minimization algorithm
called gradient descent.<br />
<br />
<strong>Example</strong>  <em>Exponential decay.</em> Let’s derive the ML estimator for a
particle’s lifetime. The proper decay time of an unstable particle with
lifetime <span class="math notranslate nohighlight">\(\tau\)</span> follows the exponential distribution:
$<span class="math notranslate nohighlight">\(f(t;\tau)=\frac{1}{\tau}e^{-t/\tau}\)</span><span class="math notranslate nohighlight">\( Given a set of \)</span>n<span class="math notranslate nohighlight">\( measurements
\)</span>{t_i}<span class="math notranslate nohighlight">\( of the proper-decay time, we can write the log-likelihood as:
\)</span><span class="math notranslate nohighlight">\(l(\tau)=\ln L(\tau)= \ln \prod_i f(t_i;\tau) = \sum_i \ln f(t_i;\tau)=\sum_i\left(\ln\frac{1}{\tau}-\frac{t_i}{\tau}\right).\)</span><span class="math notranslate nohighlight">\(
Maximizing the log-likelihood with respect to \)</span>\tau<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\frac{\partial l}{\partial \tau}= \sum_i \left(-\frac{1}{\tau} + \frac{t_i}{\tau^2} \right) = -\frac{n}{\tau}+\frac{1}{\tau^2}\sum t_i = 0\)</span><span class="math notranslate nohighlight">\(
we obtain the ML estimator \)</span>\hat{\tau}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\hat{\tau}=\frac{1}{n}\sum_i t_i\)</span><span class="math notranslate nohighlight">\( Hence we get the mean as the ML
estimator of the lifetime! Furthermore it shows that the ML estimator is
asymptotically unbiased: increasing the number of measurements the bias
decreases.\
\
**Example**  *Gaussian distribution.* The Gaussian probability
distribution function is given by
\)</span><span class="math notranslate nohighlight">\(f(x_i;\mu)=\frac{1}{\sqrt{2\pi}}\frac{1}{\sigma_i}\cdot e^{-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma_i}\right)^2}.\)</span><span class="math notranslate nohighlight">\(
To get a ML estimator for the mean \)</span>\hat{\mu}<span class="math notranslate nohighlight">\( we construct again the
log-likelihood function: \)</span><span class="math notranslate nohighlight">\(\label{eq_gauss_llh}
l(\mu)=\ln L(\mu)= \ln \prod_i f(t_i;\mu) = \sum_i \ln f(t_i;\mu) = 
\sum_i\left( \ln\frac{1}{\sqrt{2\pi}} - \ln \sigma_i - \frac{1}{2} \left( \frac{x_i-\mu}{\sigma_i} \right)^2 \right)\)</span><span class="math notranslate nohighlight">\(
Differentiating with respect to \)</span>\mu<span class="math notranslate nohighlight">\(, the determination of the maximum
yields: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\frac{dl(\mu)}{d\mu}&amp;=&amp; \frac{d}{d\mu} \sum_i -\frac{1}{2}\left( \frac{x_i-\mu}{\sigma_i}\right)^2   = \sum_i\frac{x_i-\mu}{\sigma_i^2}=0 \\
\hat{\mu}&amp;=&amp;\frac{\sum_i x_i/\sigma_i^2 }{\sum_i 1/\sigma_i^2}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Which is the weighted mean of the sample \)</span>{x_i}<span class="math notranslate nohighlight">\( and it simplifies to
\)</span>\hat{\mu} = \frac{1}{n} \sum_{i}x_{i}<span class="math notranslate nohighlight">\( if all the \)</span>x_{i}<span class="math notranslate nohighlight">\( have the same
\)</span>\sigma_{i}<span class="math notranslate nohighlight">\(. In this case (\)</span>\sigma_{i} = \sigma , \forall i<span class="math notranslate nohighlight">\() we can
use the likelihood method to get an estimate for the variance
\)</span>\hat{\sigma}^{2}<span class="math notranslate nohighlight">\(. The ML method yields
\)</span><span class="math notranslate nohighlight">\(\hat{\sigma}^{2}=\frac{1}{n}\sum_i(x_i-\hat{\mu})^2.\)</span><span class="math notranslate nohighlight">\( which is, as
already discussed, asymptotically unbiased.\
\
**Example**  *Poisson distribution:* Consider a set of data \)</span>{r_i}<span class="math notranslate nohighlight">\(
which we assume to be distributed according to a Poisson distribution
with parameter \)</span>\lambda<span class="math notranslate nohighlight">\(. We use the ML method to find an estimator for
\)</span>\lambda<span class="math notranslate nohighlight">\(. The log-likelihood function for the Poisson distribution is
given by
\)</span><span class="math notranslate nohighlight">\(l(\lambda) = \sum_i\ln \frac{\lambda^{r_i}}{r_i!}e^{-\lambda} = \sum_i \ln \lambda^{r_i} - n\lambda - \sum_i \ln r_i! =
\ln\lambda\cdot\sum_i r_i-n\lambda - \sum_i \ln r_i!\)</span><span class="math notranslate nohighlight">\( Differentiating
\)</span>l(\lambda)<span class="math notranslate nohighlight">\( w.r.t. \)</span>\lambda<span class="math notranslate nohighlight">\( and equating it to zero gives as estimator
for the mean of a Poisson distribution
\)</span>\hat{\lambda} = \frac{1}{n} \sum_{i} r_{i}<span class="math notranslate nohighlight">\(, which is again the mean of
the sample.\
\
**Example**  *Binomial distribution:* As for the Poisson case above,
consider a set of data \)</span>{s_i, r_i}<span class="math notranslate nohighlight">\( (for the measurement \)</span>i<span class="math notranslate nohighlight">\( we obtain
\)</span>s_i<span class="math notranslate nohighlight">\( successes and \)</span>r_i<span class="math notranslate nohighlight">\( failures, \)</span>n_i = s_i +r_i<span class="math notranslate nohighlight">\() which we assume to
be distributed according to a binomial distribution
\)</span>B(p) = {n \choose s} p^{s} (1-p)^{r}<span class="math notranslate nohighlight">\( with \)</span>s + r = n<span class="math notranslate nohighlight">\(. We use the ML
method to find an estimator for \)</span>p<span class="math notranslate nohighlight">\(. The log-likelihood function for the
binomial distribution is given by:
\)</span><span class="math notranslate nohighlight">\(l(p)=\ln B(p) = \ln{n \choose s} + s\ln p + r\ln(1-p)\)</span><span class="math notranslate nohighlight">\( The
requirement \)</span>\frac{\partial l(p)}{\partial p} = 0<span class="math notranslate nohighlight">\( yields
\)</span>\frac{s}{p} - \frac{r}{1-p}=0<span class="math notranslate nohighlight">\( and hence \)</span>\hat{p}=s/n$, which is the
fraction of successes given n trials.\</p>
</div>
<div class="section" id="minimum-variance-bound">
<h2>Minimum Variance Bound<a class="headerlink" href="#minimum-variance-bound" title="Permalink to this headline">¶</a></h2>
<div class="section" id="information">
<h3>Information<a class="headerlink" href="#information" title="Permalink to this headline">¶</a></h3>
<p>We introduce here the concept of information following Fisher’s
definition. Any information definition should fulfil the following
criteria:</p>
<ul class="simple">
<li><p>the information should increase if we make more observations - add
more data</p></li>
<li><p>data, which are irrelevant to the estimation of the parameters we
wish to estimate or to the hypothesis we wish to test, should
contain no information</p></li>
<li><p>the precision of the estimation should be greater if we have more
information</p></li>
</ul>
<p>The <strong>Fisher information</strong> (information for short in the following) on a
parameter <span class="math notranslate nohighlight">\(\theta\)</span> given by a data set <span class="math notranslate nohighlight">\(\{\vec{x}\}\)</span> of the random
variable <span class="math notranslate nohighlight">\(x\)</span> is defined as the expectation value:
$<span class="math notranslate nohighlight">\(I_{\vec{x}}(\theta) = \langle\left( \frac{\partial \ln L(\vec{x};\theta)}{\partial \theta}  \right)^2  \rangle=\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(= \langle\left( \frac{\partial l}{ \partial \theta }  \right)^2  \rangle= \int \left( \frac{\partial \ln L(\vec{x};\theta)}{\partial \theta} \right)^2 L(\vec{x}; \theta) d\vec{x}\)</span><span class="math notranslate nohighlight">\(
To have a more compact notation we define the **score** of one
measurement as the random variable:
\)</span><span class="math notranslate nohighlight">\(S_1 = \frac{\partial}{\partial \theta}\ln f(x;\theta)\)</span><span class="math notranslate nohighlight">\( The score of a
sample is the sum of the score of each measurement:
\)</span><span class="math notranslate nohighlight">\(S(\vec{x},\theta) = \sum_{i=1}^{n} S_1(x_i;\theta)\)</span><span class="math notranslate nohighlight">\( and it is equal
to the derivative of the log-likelihood w.r.t to the parameter of
interest:
\)</span><span class="math notranslate nohighlight">\(S(\vec{x},\theta) = \frac{\partial \ln L(\vec{x},\theta)}{\partial \theta}\)</span><span class="math notranslate nohighlight">\(
So the definition of the information of the sample \)</span>\vec{x}<span class="math notranslate nohighlight">\( on the
parameter \)</span>\theta<span class="math notranslate nohighlight">\(, can be rewritten as the expectation value of the
square of the score:
\)</span><span class="math notranslate nohighlight">\(I_{\vec{x}}(\theta) = \langle S^2(\vec{x};\theta) \rangle\)</span><span class="math notranslate nohighlight">\( If
\)</span>\ln L(\vec{x},\theta)<span class="math notranslate nohighlight">\( is twice differentiable w.r.t. \)</span>\theta<span class="math notranslate nohighlight">\(, then
the Fisher information can be rewritten as:
\)</span><span class="math notranslate nohighlight">\(I_{\vec{x}}(\theta)  = \langle\; \left( \frac{\partial}{\partial \theta} \ln L \right)^2 \;\rangle
= -\langle\; \frac{\partial^2}{\partial \theta^2} \ln L \;\rangle\)</span><span class="math notranslate nohighlight">\(
*Proof*: \)</span><span class="math notranslate nohighlight">\(\frac{\partial^2}{\partial\theta^2}\ln L =
\frac{\partial}{\partial\theta}\left( \frac{1}{L} \frac{\partial L}{\partial \theta} \right) =
\frac{-(\partial L / \partial \theta )^2}{L^2} + \frac{1}{L}\frac{\partial^2 L}{\partial\theta^2}\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(-\langle\frac{\partial^2 \ln L}{\partial\theta^2}\rangle=
\langle\left(\frac{\partial L / \partial \theta }{L} \right)^2\rangle- \langle\frac{ \partial^2  L / \partial\theta^2}{L} \rangle=
\langle\left(\frac{\partial \ln L}{\partial \theta}\right)^2 \rangle-
\int \left( \frac{\partial^2 L }{\partial \theta^2} \right) \frac{1}{L} L dx =\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(\langle\left(\frac{\partial \ln L}{\partial \theta}\right)^2 \rangle= I_{\vec{x}} (\theta)\)</span><span class="math notranslate nohighlight">\(
the last is because
\)</span>\int \left( \frac{\partial^2 L }{\partial \theta^2} \right) \frac{1}{L} L dx = \frac{\partial^2}{\partial \theta^2} \int L dx = \frac{\partial^2}{\partial \theta^2} 1 = 0<span class="math notranslate nohighlight">\(.\
\
With these definitions we can check that the Fisher information fulfils
the requirements shown above.\
\
\)</span>\rightarrow 1)<span class="math notranslate nohighlight">\( The information should increase if we make more
observations. For n measurements:
\)</span><span class="math notranslate nohighlight">\(I(\theta) = \langle\left( \sum_{i=1}^n S_1 (x_i ; \theta \right)^2 \rangle\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(I(\theta) = \langle\left( \sum_{i=1}^n S_1(x_i; \theta)  \right)^2\rangle
= V\left( \sum_{i=1}^n S_1(x_i; \theta) \right) + \langle\sum_{i=1}^n S_1(x_i; \theta)  \rangle^2\)</span><span class="math notranslate nohighlight">\(
where we used \)</span>V(a) = \langle a^2\rangle- \langle a \rangle^2<span class="math notranslate nohighlight">\(. Assuming
that the single measurements \)</span>x_i<span class="math notranslate nohighlight">\( are independent, the variance of the
sum is the sum of the variances. And since all the measurements are
taken from the same p.d.f., the variance is the same for all \)</span>i<span class="math notranslate nohighlight">\(. A
similar argument applies to the second term. So:
\)</span><span class="math notranslate nohighlight">\(I(\theta) = n V\left( S_1(x; \theta) \right) + n^2 \langle S_1(x; \theta)  \rangle^2\)</span><span class="math notranslate nohighlight">\(
which shows that the information increases with the number of
observations.\
\
\)</span>\rightarrow 2)<span class="math notranslate nohighlight">\( Irrelevant data carry no information. For irrelevant
data the p.d.f. will not depend on \)</span>\theta<span class="math notranslate nohighlight">\(; the score will be 0 adding
no information.\
\
\)</span>\rightarrow 3)<span class="math notranslate nohighlight">\( The precision should be greater if we have more
information. This comes from the definition of Fisher's information:
\)</span><span class="math notranslate nohighlight">\(\sigma^2(\hat{\theta}) = \frac{1}{d^2 L/d\theta^2|_{\theta = \hat{\theta}}} = \frac{1}{I(\hat{\theta})}\)</span><span class="math notranslate nohighlight">\(
The variance is the inverse of the second derivative of the likelihood,
i.e. the inverse of the information. The larger the information the
smaller the variance. Another way to look at it: think about the second
derivative computed at the best estimate of the parameter
(\)</span>\hat{\theta}$) as the curvature of the likelihood at that point. The
larger the curvature, the more pronounced the minimum, the larger the
information in the data set (the smaller the uncertainty on the
parameter see <a class="reference external" href="#sec:MLunc">1.6</a>{reference-type=”ref”
reference=”sec:MLunc”}). Now go to the other extreme: a likelihood that
does not depend on a parameter will be flat with respect to it, so the
curvature and the information will be zero, and the variance infinite.</p>
</div>
<div class="section" id="rao-cramer-frechet-inequality-sec-rcf">
<h3>Rao-Cramér-Frechet inequality {#sec:RCF}<a class="headerlink" href="#rao-cramer-frechet-inequality-sec-rcf" title="Permalink to this headline">¶</a></h3>
<p>The “Rao-Cramér-Frechet inequality” (1945) tells that any estimator
will never have a variance smaller than a given number that depends on
the information contained in the dataset and the bias of the estimator.<br />
To see how this is possible, let’s compute the covariance among the
score and the MLE (both are random variables):
$<span class="math notranslate nohighlight">\(\mbox{cov}[S(\vec{x}; \hat{\theta}(\vec{x})), \hat{\theta}(\vec{x})] = \langle S(\vec{x}; \hat{\theta}(\vec{x}))\hat{\theta}(\vec{x})\rangle-\langle S(\vec{x}; \hat{\theta}(\vec{x}))\rangle\langle\hat{\theta}(\vec{x})\rangle\)</span><span class="math notranslate nohighlight">\(
Let's compute it taking \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( as an estimator of \)</span>\theta<span class="math notranslate nohighlight">\( with
bias \)</span>b_n(\hat{\theta}) = \langle\hat{\theta} \rangle- \theta<span class="math notranslate nohighlight">\( and
assume that its variance is finite and that the range of \)</span>x<span class="math notranslate nohighlight">\( does not
depend on \)</span>\theta<span class="math notranslate nohighlight">\(. Then we can write the first term as:
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\langle\hat{\theta} S(\vec{x},\theta) \rangle&amp;=&amp; \int\ldots\int \hat{\theta}\left( \frac{\partial}{\partial \theta} \ln L(\vec{x},\theta) \right) L(\vec{x},\theta) dx_1\ldots dx_n\\
&amp;=&amp; \int\ldots\int \hat{\theta}\left( \frac{1}{L(\vec{x},\theta)}\frac{\partial}{\partial \theta} L(\vec{x},\theta) \right) L(\vec{x},\theta) dx_1\ldots dx_n\\
&amp;=&amp; \int\ldots\int \hat{\theta}\left( \frac{\partial}{\partial \theta} L(\vec{x},\theta) \right)dx_1\ldots dx_n\\
&amp;=&amp; \int\ldots\int \hat{\theta}\frac{\partial}{\partial \theta}\left( \prod_{i=1}^nf(x_i;\theta)dx_i \right)\\
&amp;=&amp; \int\ldots\int \frac{\partial}{\partial \theta}\left(\hat{\theta} \prod_{i=1}^nf(x_i;\theta)dx_i \right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
the last step follows because \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( is a statistic (a function
of the data only) and therefore does not depend on \)</span>\theta<span class="math notranslate nohighlight">\(. Then we
change the order of integration and differentiation: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\langle\hat{\theta} S(\vec{x},\theta) \rangle&amp;=&amp;  \frac{\partial}{\partial \theta}\int\ldots\int \hat{\theta} \prod_{i=1}^n f(x_i;\theta)dx_i \\
&amp;=&amp; \frac{\partial}{\partial \theta} \langle\hat{\theta}\rangle= \frac{\partial}{\partial \theta} (\theta + b_n(\hat{\theta}))\\
&amp;=&amp; 1 + \frac{\partial}{\partial \theta} b_n(\hat{\theta})\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The second term is zero because
\)</span>\langle S(\vec{x};\theta)\rangle= \sum \langle S_1 (x_i;\theta)\rangle<span class="math notranslate nohighlight">\(
and \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 \langle S_1(x;\theta)  \rangle&amp;=&amp; \langle\frac{\partial}{\partial \theta} \ln f(x;\theta) \rangle\\
 &amp;=&amp; \int \left(  \frac{\partial}{\partial \theta} \ln f(x;\theta)  \right) f(x; \theta) dx \\
 &amp;=&amp; \int \frac{1}{f(x;\theta)} \left( \frac{\partial}{\partial \theta} f(x;\theta) \right) f(x;\theta) dx \\
 &amp;=&amp;  \int \frac{\partial}{\partial \theta} f(x;\theta) dx\\
 \end{aligned}\)</span><span class="math notranslate nohighlight">\( interchanging the order of integration and
differentiation (this usually holds for smooth distributions encountered
in physics):
\)</span><span class="math notranslate nohighlight">\(\langle S_1(x;\theta) \rangle=  \frac{\partial}{\partial \theta} \int f(x;\theta) dx = \frac{\partial}{\partial \theta} 1 = 0\)</span><span class="math notranslate nohighlight">\(
since \)</span>f(x;\theta)<span class="math notranslate nohighlight">\( is normalized for all values of \)</span>\theta<span class="math notranslate nohighlight">\(.\
\
Putting everything together:
\)</span><span class="math notranslate nohighlight">\(\mbox{cov}[S(\vec{x},\hat{\theta}),\hat{\theta}(\vec{x})] = 1 + \frac{\partial}{\partial \theta} b_n(\hat{\theta}).\)</span><span class="math notranslate nohighlight">\(
Their correlation coefficient is:
\)</span><span class="math notranslate nohighlight">\(\rho^2 = \frac{(\mbox{cov}[S,\hat{\theta}])^2}{V(S)V(\hat{\theta})} = \frac{\left( 1 + \frac{\partial}{\partial \theta} b_n(\hat{\theta})\right)^2}{I(\theta)V(\hat{\theta})}\)</span><span class="math notranslate nohighlight">\(
and since \)</span>\rho^2 \le 1<span class="math notranslate nohighlight">\(, we have
\)</span><span class="math notranslate nohighlight">\(V(\hat{\theta}) \ge \frac{\left( 1 + \frac{\partial}{\partial \theta} b_n(\hat{\theta})\right)^2}{I(\theta)}\)</span><span class="math notranslate nohighlight">\(
This is the so called &quot;**Rao-Cramér-Frechet inequality**&quot; (RCF) or
&quot;Information inequality&quot;.\
This inequality means that there is a lower bound on the variance of the
estimator; i.e. given a certain amount of information (a data set) we
can never find an estimator with lower variance than this bound. To
reduce the bound we need to get more information or get rid of the bias.
For an unbiased estimator the bound becomes
\)</span>V(\hat{\theta}) = 1/I(\theta)<span class="math notranslate nohighlight">\(.\
Now that we know what is the minimum variance of an estimator we can
also define the **efficiency** of the estimator as:
\)</span><span class="math notranslate nohighlight">\(\epsilon(\hat{\theta}) = \frac{V_{min}(\hat{\theta})}{V(\hat{\theta})} \le 1\)</span><span class="math notranslate nohighlight">\(
which for an unbiased estimator is
\)</span><span class="math notranslate nohighlight">\(\epsilon(\hat{\theta}) = \frac{1}{V(\hat{\theta})I(\theta)} \le 1\)</span><span class="math notranslate nohighlight">\( An
estimator with \)</span>\epsilon = 1$ is called <em>efficient</em>. It is not always
possible to find an <em>efficient</em> estimator, but it can be shown that:</p>
<ul class="simple">
<li><p>if an efficient estimator for a given problem exist, it will be
found using the ML method</p></li>
<li><p>ML estimators are efficient in the large sample limit.</p></li>
</ul>
<p>In simple words, the maximum likelihood estimator is the best you can
get…<br />
<br />
<em>Theorem</em>: An efficient estimator can be found if and only if it belongs
to the exponential family:
$<span class="math notranslate nohighlight">\(f(x;\theta) = \exp[A(\theta)\hat{\theta}(x) + B(\theta) + C(\theta) ]\)</span>$</p>
</div>
</div>
<div class="section" id="uncertainty-for-ml-estimators-sec-mlunc">
<h2>Uncertainty for ML estimators {#sec:MLunc}<a class="headerlink" href="#uncertainty-for-ml-estimators-sec-mlunc" title="Permalink to this headline">¶</a></h2>
<p>Let’s take the simplest case of a likelihood with only one parameter in
the large sample limit (i.e. the estimator is asymptotically unbiased,
efficient and the RCF is valid as an equality). Expand its NLL function
around <span class="math notranslate nohighlight">\(\theta = \hat{\theta}\)</span>:
$<span class="math notranslate nohighlight">\(F(\theta) = -\ln L(\theta)=F(\hat{\theta})+\frac{1}{2}\frac{d^2F}{d\theta^2}\lvert_{\theta=\hat{\theta}} (\theta-\hat{\theta})^2+\cdots\)</span><span class="math notranslate nohighlight">\(
(the first derivative vanishes by construction because of the ML
principle). Then let's approximate the likelihood with a Gaussian in the
neighborhood of its maximum: \)</span><span class="math notranslate nohighlight">\(L(\theta)
        \sim const\cdot \exp\left(-\frac{1}{2}\cdot\frac{d^2F}{d\theta^2}\lvert_{\theta=\hat{\theta}}(\theta-\hat{\theta})^2\right) 
        := const\cdot \exp\left(-\frac{(\theta-\hat{\theta})^2}{2\sigma^2}\right).\)</span><span class="math notranslate nohighlight">\(
By comparing the exponents we find:
\)</span><span class="math notranslate nohighlight">\(\sigma^2(\hat{\theta})=\frac{1}{\left.d^2F/d\theta^2\right|_{\theta=\hat{\theta}}} = \frac{1}{I(\hat{\theta})}.\)</span><span class="math notranslate nohighlight">\(
The variance is the inverse of the second derivative of the
log-likelihood at \)</span>\theta = \hat{\theta}<span class="math notranslate nohighlight">\(, i.e. the inverse of the
information.\
The difference \)</span>F(\theta) - F(\hat{\theta})<span class="math notranslate nohighlight">\( calculated at
\)</span>\theta = \hat{\theta} \pm n \cdot \sigma(\hat{\theta})<span class="math notranslate nohighlight">\(, using the
equations above is:
\)</span>$F(\hat{\theta} \pm n\sigma) - F(\hat{\theta}) = \frac{1}{2}\frac{d^2F}{d\theta^2}\lvert_{\theta=\hat{\theta}} (\hat{\theta} \pm n\sigma - \hat{\theta})^2 = \frac{1}{2}\frac{1}{\sigma^2} ( n\sigma )^2  = \frac{1}{2}n^2</p>
<p>This enables us to find the uncertainty of an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>
easily by looking at the graph for the log-likelihood function. When the
log-likelihood has decreased from the maximum by <span class="math notranslate nohighlight">\(0.5\)</span> you are at
<span class="math notranslate nohighlight">\(\pm 1 \sigma\)</span>, by <span class="math notranslate nohighlight">\(2\)</span> you are at <span class="math notranslate nohighlight">\(\pm2 \sigma\)</span>, by <span class="math notranslate nohighlight">\(4.5\)</span> you are at
<span class="math notranslate nohighlight">\(\pm 3\sigma\)</span> and so on.<br />
If the log-likelihood function is not parabolic at the maximum then you
can try with a non-linear transformation (<span class="math notranslate nohighlight">\(\theta\)</span> goes into
<span class="math notranslate nohighlight">\(z = z(\theta)\)</span>) such that <span class="math notranslate nohighlight">\(F(z)\)</span> shows the desired parabolic behavior:
the best estimator is then <span class="math notranslate nohighlight">\(\hat{z} = z(\hat{\theta})\)</span> and the standard
deviation <span class="math notranslate nohighlight">\(\sigma_{z}\)</span> of <span class="math notranslate nohighlight">\(z\)</span> can then be determined as above.<br />
If a transformation cannot be found (which is the typical case in any
realistic application), you can always proceed numerically and find the
values for which the likelihood crosses <span class="math notranslate nohighlight">\(1/2 n^2\)</span>.<br />
Monte Carlo techniques can also be used to estimate the standard
deviation or the variance of a parameter. One can simulate a large
amount pseudo-experiments (slang: toy data or toys) and for each of them
compute the ML estimator: the distribution of the ML estimators is then
used to compute the variance. To generate the toy data, one can choose
as “true” value of the parameter the one from the real experiment and as
the size of the sample the number of events of the real experiment.
Finally the value of the variance can be computed from
<span class="math notranslate nohighlight">\(s^2 = 1/(n-1) \sum (x_i - \bar{x})^2\)</span> (where <span class="math notranslate nohighlight">\(x_i\)</span> are the ML estimates
and <span class="math notranslate nohighlight">\(i\)</span> runs over the toy datasets) and give this as the statistical
error of the parameter estimated from the real measurement.<br />
In the case of several parameters
<span class="math notranslate nohighlight">\(\theta_{1},\theta_{2}, \ldots , \theta_{m}\)</span> the likelihood function is
generalized to
$<span class="math notranslate nohighlight">\(L(\theta_1,\theta_2,\ldots,\theta_m)=\prod_{i=1}^n f(x_i;\theta_1,\theta_2,\ldots,\theta_m).\)</span><span class="math notranslate nohighlight">\(
Expanding the NLL function around its minimum at \)</span>\hat{\theta}<span class="math notranslate nohighlight">\(, we
obtain (the first derivative vanishes - ML principle): \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
F(\theta_1,\ldots,\theta_m)&amp;=&amp;F(\hat{\theta}_1,\ldots,\hat{\theta}_m)+\frac{1}{2}\sum_{i,k}\frac{\partial^2F}{\partial \theta_i\partial \theta_k}(\theta_i-\hat{\theta}_i)(\theta_k-\hat{\theta}_k)+\cdots \\ &amp;=&amp;F(\hat{\theta}_1,\ldots,\hat{\theta}_m)+\frac{1}{2}\sum_{i,k}G_{ik}(\theta_i-\hat{\theta}_i)(\theta_k-\hat{\theta}_k)+\cdots\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where \)</span>G<span class="math notranslate nohighlight">\( is given by: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
V&amp;=&amp;G^{-1} \\
G_{ik}&amp;=&amp;\frac{\partial^2 F}{\partial \theta_i\partial \theta_k },\end{aligned}\)</span><span class="math notranslate nohighlight">\(
evaluated at the minimum \)</span>\hat{\theta}<span class="math notranslate nohighlight">\(. In the case of only two
parameters the contour lines are drawn as lines with the same likelihood
values \)</span>F({\bf \theta}) = F({\bf \hat{\theta}}) + 1/2 r^{2}$, which
correspond to ellipses (see
Sec. <a class="reference external" href="#sec:errorMatrix">[sec:errorMatrix]</a>{reference-type=”ref”
reference=”sec:errorMatrix”}).<br />
\</p>
</div>
<div class="section" id="binned-maximum-likelihood">
<h2>Binned Maximum Likelihood<a class="headerlink" href="#binned-maximum-likelihood" title="Permalink to this headline">¶</a></h2>
<p>The likelihood function as we described it in the previous chapter is
“unbinned”. This means that it is constructed out of all available data
points <span class="math notranslate nohighlight">\(x_{i}\)</span> and therefore no information is lost due to binning. For
large data sets, using each single point might very time consuming (in
the minimization of the NLL at each variation of the parameters you need
to loop over all data points) and it might be practical to bin the data
and represent it in histograms. We assume that the random variables
<span class="math notranslate nohighlight">\(x_{i}\)</span> are distributed according to a p.d.f. <span class="math notranslate nohighlight">\(f(x;\theta)\)</span> and that the
expectation value <span class="math notranslate nohighlight">\(\nu = (\nu_{1}, \ldots, \nu_{N})\)</span> for the number of
entries per bin <span class="math notranslate nohighlight">\(i\)</span> is given by $<span class="math notranslate nohighlight">\(\label{eq:nuBinned}
\nu_i=\int_{x_i^{min}}^{x_i^{max}}f(x;\theta) dx.\)</span><span class="math notranslate nohighlight">\( The boundaries of
bin \)</span>i<span class="math notranslate nohighlight">\( are denoted by \)</span>x_{i}^{min}<span class="math notranslate nohighlight">\( and \)</span>x_{i}^{max}<span class="math notranslate nohighlight">\(, respectively. We
can now think of the histogram as some sort of single measurement of a
\)</span>N<span class="math notranslate nohighlight">\(-dimensional random vector for which the combined probability density
is given by a multinomial distribution. This means we are asking for the
joint probability to observe \)</span>n_i<span class="math notranslate nohighlight">\( entries in bin \)</span>i<span class="math notranslate nohighlight">\( when the expected
is \)</span>\nu_i<span class="math notranslate nohighlight">\(. Normalizing by \)</span>n_{tot} = \sum n_{i}<span class="math notranslate nohighlight">\( we get:
\)</span><span class="math notranslate nohighlight">\(f_{comb}(\vec{n};\vec{\nu})=\frac{n_{tot}!}{n_1!\cdots n_N!}\left(\frac{\nu_1}{n_{tot}}\right)^{n_1}\cdots \left(\frac{\nu_N}{n_{tot}}\right)^{n_N}\)</span><span class="math notranslate nohighlight">\(
Remember that the dependence on the parameter \)</span>\theta<span class="math notranslate nohighlight">\( is embedded in
the \)</span>\nu_i<span class="math notranslate nohighlight">\( as in
Eq. [\[eq:nuBinned\]](#eq:nuBinned){reference-type=&quot;ref&quot;
reference=&quot;eq:nuBinned&quot;}. The negative logarithm of the joint
probability yields now the binned NLL function (all uninteresting terms
are dropped): \)</span><span class="math notranslate nohighlight">\(l(\theta) = \sum_{i=1}^N \ln\nu_i(\theta)^{n_i}\)</span><span class="math notranslate nohighlight">\( The
estimations for \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( are found as in the unbinned case by
minimizing the NLL.\
Taking the number of bins \)</span>N\to \infty<span class="math notranslate nohighlight">\( brings back the unbinned
likelihood case. Provided that the expected number of entries in a bin
is not zero (\)</span>\nu_i(\theta) &gt; 0$) the binned ML is usable even when some
bins have zero entries observed (in contrast with the least square
method that we will discussed in the next chapter).</p>
</div>
<div class="section" id="extended-maximum-likelihood-method-sec-emlf">
<h2>Extended Maximum Likelihood Method {#sec:EMLF}<a class="headerlink" href="#extended-maximum-likelihood-method-sec-emlf" title="Permalink to this headline">¶</a></h2>
<p>We applied up to now the ML to a fixed number of events <span class="math notranslate nohighlight">\(N\)</span>. We can
easily extend the ML to the case where the total number of events is
itself not known and it is treated as a parameter to be estimated. To do
this we can multiply the previous expression of the likelihood by a
Poisson p.d.f. which represents the probability to observe <span class="math notranslate nohighlight">\(n\)</span> events
when the expected number of events is <span class="math notranslate nohighlight">\(\nu\)</span>:
$<span class="math notranslate nohighlight">\(L(x;\vec{\theta}) = \prod_{i=1}^n f(x_i;{\bf \theta}) \to L_E(x;{\bf \theta}, \nu) = \frac{e^{-\nu}\nu^n}{n!} \prod_{i=1}^n f(x_i;{\bf \theta})\)</span><span class="math notranslate nohighlight">\(
and
\)</span><span class="math notranslate nohighlight">\(l(\vec{x};\vec{\theta}) = \sum_{i=1}^n \ln f(x_i;{\bf \theta}) \to l_E(\vec{x};{\bf \theta},\nu)  = \sum_{i=1}^n \ln \nu f(x_i;{\bf \theta}) - \nu + const\label{eq_eml}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\ln\nu f(x;{\bf \theta})<span class="math notranslate nohighlight">\( is now normalized to \)</span>\nu<span class="math notranslate nohighlight">\( instead of 1
and where we dropped the constant term \)</span>\ln n!<span class="math notranslate nohighlight">\( which is irrelevant in
the minimization. This new likelihood is called the
*extended-maximum-likelihood* or EML.\
We can now distinguish two cases:\
Case 1: the parameter \)</span>\nu<span class="math notranslate nohighlight">\( depends on \)</span>{\bf \theta}<span class="math notranslate nohighlight">\(. The EML
log-likelihood function can be written as \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\ln L({\bf \theta})&amp;=&amp;n\ln \nu({\bf \theta})-\nu({\bf \theta})+\sum_{i=1}^{n}\ln f(x_i;{\bf \theta})\\
&amp;=&amp;-\nu({\bf \theta})+\sum_{i=1}^{n}\ln(\nu({\bf \theta}) f(x_i;{\bf \theta}))\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where the additive terms not depending on \)</span>{\bf \theta}<span class="math notranslate nohighlight">\( are dropped. By
taking the Poisson term into consideration in the EML function, the
resulting variance is usually smaller, because when estimating
\)</span>{\bf \hat{\theta}}<span class="math notranslate nohighlight">\(, we use the extra information brought in by \)</span>n<span class="math notranslate nohighlight">\(.\
\
Case 2: \)</span>\nu<span class="math notranslate nohighlight">\( does not dependent on \)</span>{\bf \theta}<span class="math notranslate nohighlight">\(. Differentiating
Eq. [\[eq\_eml\]](#eq_eml){reference-type=&quot;ref&quot; reference=&quot;eq_eml&quot;}
w.r.t \)</span>\nu<span class="math notranslate nohighlight">\( and equating it to zero yields as estimator simply
\)</span>\hat{\nu} = n<span class="math notranslate nohighlight">\(, as expected. We also obtain as estimators the same
\)</span>\hat{\theta}<span class="math notranslate nohighlight">\( of the standard ML. Nevertheless the variance of the
\)</span>\hat{\theta}<span class="math notranslate nohighlight">\( would be bigger because now not only \)</span>\hat{\theta}<span class="math notranslate nohighlight">\( but
also \)</span>n$ is a source of statistical uncertainty.</p>
</div>
<div class="section" id="combination-of-measurements-with-the-ml-method">
<h2>Combination of Measurements with the ML Method<a class="headerlink" href="#combination-of-measurements-with-the-ml-method" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have different measurements of the same parameter <span class="math notranslate nohighlight">\(\theta\)</span> by
different experiments and you want to combine them using the ML method.
More precisely, suppose we have a set of <span class="math notranslate nohighlight">\(n\)</span> measured data points with
probability density <span class="math notranslate nohighlight">\(f(x;\theta)\)</span> from one experiment and a second set
with <span class="math notranslate nohighlight">\(m\)</span> measured data points <span class="math notranslate nohighlight">\(y_{i}\)</span>, which are distributed according
to a probability density <span class="math notranslate nohighlight">\(g(y;\theta)\)</span> from a second experiment. The two
probability densities <span class="math notranslate nohighlight">\(f(x;\theta)\)</span> and <span class="math notranslate nohighlight">\(g(y;\theta)\)</span> can have different
functional forms, because of the different experimental techniques used
to determine <span class="math notranslate nohighlight">\(\theta\)</span>. As an example you can think of <span class="math notranslate nohighlight">\(\theta\)</span> being the
mass of a particle and <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> the results of two experiments or the
results of the mass measurement in two decay modes.<br />
The two experiments together can be interpreted as one single experiment
and the resulting likelihood is just the product of:
$<span class="math notranslate nohighlight">\(L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)\cdot \prod_{i=1}^{m}g(y_i;\theta)=L_x(\theta)\cdot L_y(\theta)\)</span><span class="math notranslate nohighlight">\(
This expression becomes clear if you think back at the definition of
likelihood. The likelihood is based on the conditional probability that,
given a parameter \)</span>\theta$ we observe the data set we have. The product
above, just extends the conditional probability further to a larger data
set comprising two experiments.<br />
This way of combining different measurements is only valid in the case
where the two likelihood are totally uncorrelated, i.e. the two
experiments do not share any common source of uncertainty. If that is
not the case then the parameters correlation has to be included in the
likelihoods expressions. A real life example can be found in the
combination of the Higgs mass measurement performed by the ATLAS and CMS
collaborations [&#64;massHiggs].</p>
</div>
<div class="section" id="constraining-parameters">
<h2>Constraining parameters<a class="headerlink" href="#constraining-parameters" title="Permalink to this headline">¶</a></h2>
<p>It often happens that the parameters to be estimated are constrained,
for instance by a physical reason (e.g. mass <span class="math notranslate nohighlight">\(&gt; 0\)</span>) or by other
measurements. Imposing constraints always implies adding some
information, and therefore the errors of the parameters are in general
reduced.<br />
The most efficient method to deal with a constraint is to rewrite the
parameters such that the constraints are embedded in their definition.<br />
<br />
<strong>Example</strong>  Take <span class="math notranslate nohighlight">\(\theta_i\)</span> as fractions subjected to the constraint
that they should add to 1:
$<span class="math notranslate nohighlight">\(0 \le \theta \le 1 \qquad \; \qquad \sum_{i=1}^n \theta_i =1.\)</span><span class="math notranslate nohighlight">\( Then
we can redefine the parameters as: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\theta_1 &amp;=&amp; \psi_1\\
\theta_2 &amp;=&amp; (1-\psi_1)\psi_2\\
\theta_3 &amp;=&amp; (1-\psi_1)(1-\psi_2)\psi_3\\
&amp;\ldots&amp;\\
\theta_{k-1} &amp;=&amp; (1-\psi_1)(1-\psi_2)\ldots(1-\psi_{k-2})\psi_{k-1}\\
\theta_k &amp;=&amp; (1-\psi_1)(1-\psi_2)\ldots(1-\psi_{k-1})\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where the \)</span>\psi_i ; \forall i<span class="math notranslate nohighlight">\( are bounded to be between 0 and 1.\
\
The most general way to express a constraint is through an implicit
equation (or in general a set of equations) of the form:
\)</span>\vec{g}(\vec{\theta}) =0<span class="math notranslate nohighlight">\( and the general method to implement them is
to use the *Lagrange multipliers*. Given a likelihood
\)</span>L(\vec{x};\vec{\theta})<span class="math notranslate nohighlight">\( and the constraint \)</span>\vec{g}(\vec{\theta}) = 0<span class="math notranslate nohighlight">\(
we will find the maximum of:
\)</span><span class="math notranslate nohighlight">\(F(\vec{x};\vec{\theta}, \vec{\alpha}) = \ln L(\vec{x};\vec{\theta}) + \vec{\alpha} \vec{g}(\vec{\theta})\)</span><span class="math notranslate nohighlight">\(
with respect to \)</span>\vec{\theta}<span class="math notranslate nohighlight">\( and \)</span>\vec{\alpha}<span class="math notranslate nohighlight">\(. The estimators of
\)</span>\vec{\theta}<span class="math notranslate nohighlight">\( found in this way satisfy the constraints and also have
all the usual properties of maximum likelihood estimators.\
\
**Example**  Take the likelihood \)</span>L(x; \theta_1; \theta_2)<span class="math notranslate nohighlight">\( and say we
want to estimate \)</span>\theta_1<span class="math notranslate nohighlight">\( but we know from a different measurement
that \)</span>\theta_2<span class="math notranslate nohighlight">\( has a a value \)</span>\bar{\theta}<em>2 \pm \sigma</em>{\theta_2}<span class="math notranslate nohighlight">\(. We
can introduce the constraint on \)</span>\theta_2<span class="math notranslate nohighlight">\( by simply multiplying the
likelihood by a gaussian function centred at \)</span>\bar{\theta}<em>2<span class="math notranslate nohighlight">\( with width
\)</span>\sigma</em>{\theta_2}$ (or adding the equivalent parabolic term to the
log-likelihood).</p>
</div>
<div class="section" id="some-general-remarks-concerning-ml-estimators-sec-mlremarks">
<h2>Some general remarks concerning ML estimators {#sec:MLremarks}<a class="headerlink" href="#some-general-remarks-concerning-ml-estimators-sec-mlremarks" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>For large data sets (large <span class="math notranslate nohighlight">\(n\)</span>) the ML estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is
unbiased and normally distributed around the true value <span class="math notranslate nohighlight">\(\theta\)</span>.
The variance approaches the RCF-boundary, i.e. ML estimators are
efficient. They are furthermore consistent for large <span class="math notranslate nohighlight">\(n\)</span>. These
properties explain the popularity of the ML method.</p></li>
<li><p>A way to study the bias of an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is through
toy MC. Multiple data sets of the same size <span class="math notranslate nohighlight">\(n\)</span> of the original one,
all depending on the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, have to be generated and
analysed with the ML method. With the obtained results we can
produce the so called “pull-distribution”, i.e. the histogram filled
with the value <span class="math notranslate nohighlight">\((\theta - \hat{\theta}) / \sigma_{\hat{\theta}}\)</span>.
For an unbiased estimator, this distribution should be centred at
zero and have width 1.</p></li>
<li><p>It is important for the likelihood function and the ML estimator
that the probability density <span class="math notranslate nohighlight">\(f(x,\theta)\)</span> is normalized, i.e.
<span class="math notranslate nohighlight">\(\int f(x,\theta) dx\)</span> is independent from <span class="math notranslate nohighlight">\(\theta\)</span>. If the
maximization/minimization is done numerically, it is important to
ensure the normalization at each step (numerical programs like
<code class="docutils literal notranslate"><span class="pre">ROOFIT</span></code> do that automatically).</p></li>
<li><p>Errors on the ML estimator: For sufficiently large <span class="math notranslate nohighlight">\(n\)</span> the
likelihood function <span class="math notranslate nohighlight">\(L(\theta)\)</span>, the error <span class="math notranslate nohighlight">\(\sigma\)</span> of the estimator
can be determined from:</p>
<ul class="simple">
<li><p>the values for <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> for which <span class="math notranslate nohighlight">\(\ln L\)</span> decreases by 0.5</p></li>
<li><p><span class="math notranslate nohighlight">\(1/\sqrt{-\frac{\partial^2\ln L}{\partial a^2}}\)</span></p></li>
<li><p>the boundaries of the integral <span class="math notranslate nohighlight">\(\int_{a}^{b} L(\theta) d\theta\)</span>
that encloses 68% of the whole area.</p></li>
</ul>
<p>In the case of small <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\ln L\)</span> is not necessarily parabolic. The
function does not even have to be symmetric around its maximum.
Nevertheless <span class="math notranslate nohighlight">\(\Delta \ln L = \frac{1}{2}\)</span> can be used as a rule of
thumb to get an approximation for the 68% confidence interval. For a
better estimation of the uncertainty you can use Monte Carlo toys.</p>
</li>
</ul>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>W. Metzger [&#64;Metzger], “Statistical Methods in Data Analysis”: Ch.8</p></li>
<li><p>G. Cowan [&#64;CowanBook], “Statistical Data Analysis”,Ch. 6</p></li>
<li><p>R. Barlow [&#64;Barlow], ” A guide to the use of statistical methods in
the physical sciences”. Ch. 5</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="inference.html" title="previous page">Statistical inference</a>
    <a class='right-next' id="next-link" href="leastSquares.html" title="next page">Parameter Estimation - Least Squares {#sec:chi2}</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mauro Donega<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>