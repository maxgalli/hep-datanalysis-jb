
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Measurements uncertainties {#ch:errors} &#8212; Statistical Methods and Data Analysis Techniques</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Methods and Data Analysis Techniques</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notes.html">
   Preface
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probabilityDistributions.html">
   Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Jupyter Notebook files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../errors.html">
   Measurements uncertainties {#ch:errors}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../monteCarlo.html">
   Monte Carlo methods {#sec:MC}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../inference.html">
   Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../likelihood.html">
   Parameter Estimation - Likelihood  {#ChapterParameterEstimations}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../leastSquares.html">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hypothesisTesting.html">
   Hypotheses Testing {#ChapterHypothesisTesting}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../confidenceIntervals.html">
   Confidence Intervals {#ChapterConfidenceLimits}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mva.html">
   Multivariate Analysis Methods {#ChapterMVA}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unfolding.html">
   Unfolding {#ch:Unfolding}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/tmp/errors.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uncertainties-and-the-clt">
   Uncertainties and the CLT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#error-propagation-sectionerrorpropagation">
   Error propagation {#SectionErrorPropagation}
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#function-of-one-variable">
     Function of one variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#several-functions-of-several-variables">
     Several functions of several variables
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uncertainty-on-efficiencies">
   Uncertainty on efficiencies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uncertainty-on-the-mean">
   Uncertainty on the mean
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weighted-mean-sec-weigthedmean">
   Weighted mean {#sec:weigthedMean}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-closer-look-at-the-error-matrix-sec-errormatrix">
   A closer look at the error matrix {#sec:errorMatrix}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-vs-systematic-uncertainties-sectionstatvssysterrors">
   Statistical vs. Systematic Uncertainties {#SectionStatVSSystErrors}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#work-with-systematic-uncertainties">
   Work with systematic uncertainties
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-systematic-uncertainties">
   Evaluating Systematic Uncertainties
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-many-digits">
   How many digits?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="measurements-uncertainties-ch-errors">
<h1>Measurements uncertainties {#ch:errors}<a class="headerlink" href="#measurements-uncertainties-ch-errors" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we describe how to treat and combine the statistical and
systematic uncertainties associated to measurements.</p>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<p><strong>Precision:</strong> how reproducible is the measurement under identical
conditions.<br />
<br />
<strong>Accuracy:</strong> how close the measured value is to the nominal/reference
value.<br />
<br />
One can be very precise, but not accurate (always measuring exactly the
same, but wrong value). More measurements may increase the precision,
but not the accuracy.<br />
<br />
<strong>(Intrinsic) Resolution:</strong> the smallest change in a measured value that
the instrument can detect. NB: many particle detector systems are based
on internal statistical processes (like energy loss, shower development
in calorimeters etc..), and thus their resolution comes from the
addition of several sources which can are typically described by a
“Gaussian” distribution.<br />
We will use the names resolution for instruments and sensitivity
measurements.\</p>
<p><img alt="Precision and accuracy.[wiki]" src="../_images/precisionAccuracy.png" />{#fig:precisionAccuracy
width=”50%”}</p>
<p><br />
<strong>Measurement range:</strong> the difference between largest and smallest input
value that an instrument is capable of measuring/reading<br />
<br />
<strong>Dynamic range:</strong> the ratio between measurement range and the
resolution (quoted usually as log value “decibel=dB” in base-10)<br />
<br />
<strong>Bandwidth:</strong> the difference between the upper and lower frequencies
(in electronics) that an instrument is capable of measuring. Or the
maximal throughput for data transfer (computing)</p>
</div>
<div class="section" id="uncertainties-and-the-clt">
<h2>Uncertainties and the CLT<a class="headerlink" href="#uncertainties-and-the-clt" title="Permalink to this headline">¶</a></h2>
<p>Any measurement we perform is affected by several uncertainties
generated by several different sources. Let’s say you measure your
weight on a scale. The number indicated by the needle will be affected
by several sources of uncertainties: parallax, rounding, your movements
etc… That’s the reason why when presenting a measurement we don’t
just give the central value <span class="math notranslate nohighlight">\(x\)</span>, but we <span class="math notranslate nohighlight">\(\textit{must}\)</span> quote its
uncertainty <span class="math notranslate nohighlight">\(\pm \sigma_x\)</span>. This number is usually represented by a
Gaussian standard deviation. The reason why we can use the Gaussian
assumption as a way to present the uncertainty (i.e. why we implicitly
make the assumption that the measurements are Gaussian distributed)
comes from the central limit theorem (see
Sec. <a class="reference external" href="#sec:CLT">[sec:CLT]</a>{reference-type=”ref” reference=”sec:CLT”}).
A measurement affected by the effect of many independent additive
effects will be “approximately” Gaussian distributed. “Approximately”
means that the <em>core</em> of the distribution is well described by a
Gaussian distribution, while the <em>tails</em> typically will show deviations.</p>
</div>
<div class="section" id="error-propagation-sectionerrorpropagation">
<h2>Error propagation {#SectionErrorPropagation}<a class="headerlink" href="#error-propagation-sectionerrorpropagation" title="Permalink to this headline">¶</a></h2>
<p>Typically the measurement of an observable is extracted from the
combination of several different quantities measured directly. In this
section we will analyse how the uncertainty on those quantities can be
combined/propagated to the final measurement.</p>
<div class="section" id="function-of-one-variable">
<h3>Function of one variable<a class="headerlink" href="#function-of-one-variable" title="Permalink to this headline">¶</a></h3>
<p>We start the discussion about error propagation from the most simple
case. Let <span class="math notranslate nohighlight">\(f\)</span> be a function of only one variable <span class="math notranslate nohighlight">\(x\)</span>. The basic idea is
to see how much the function changes when the values of <span class="math notranslate nohighlight">\(x\)</span> moves within
its uncertainty. For this we make a Taylor expansion of <span class="math notranslate nohighlight">\(f\)</span> around
<span class="math notranslate nohighlight">\(x_{0}\)</span> to the first order:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}f(x)\approx f(x_0)+(x-x_0)\left(\frac{df}{dx}\right)_{x=x_0}$$ Using
$V(f)=&lt;f^2&gt;-&lt;f&gt;^2$ yields
$$V(f)=\sigma_f^2\approx \left(\frac{df}{dx}\right)^2\sigma_x^2.$$ This
approximation is only true if the uncertainties are small, i.e. the
first derivative must not vary too much within the neighborhood of a few
$\sigma$. The derivative should be estimated at the true value of $x$
and when that is unknown its measured value is used.\\### Function of several variables\\In the case of a function $f(x,y)$ of two variables $x$ and $y$, we
repeat the Taylor expansion to the first order:
$$f(x,y)\approx f(x_0,y_0)+\left(\frac{\partial f}{\partial x}\right)_{x_0,y_0}\cdot(x-x_0)+\left(\frac{\partial f}{\partial y}\right)_{x_0,y_0}\cdot(y-y_0)\end{aligned}\end{align} \]</div>
<p>Again we assume that the uncertainties are small, which allows us to
drop the higher-order terms of the Taylor expansion. We thus get the
result: $<span class="math notranslate nohighlight">\(\begin{aligned}
V(f) &amp;=&amp;\left(\frac{\partial f}{\partial x}\right)^2V(x)+\left(\frac{\partial f}{\partial y}\right)^2V(y)+2\frac{\partial f}{\partial x}\frac{\partial f}{\partial y}\cdot \mbox{cov}(x,y)\\
\sigma_{f}^2&amp;=&amp;\left(\frac{\partial f}{\partial x}\right)^2\sigma_x^2+\left(\frac{\partial f}{\partial y}\right)^2\sigma_y^2+2\frac{\partial f}{\partial x}\frac{\partial f}{\partial y}\cdot \mbox{cov}(x,y)\\\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\mbox{cov}(x,y) = \left&lt;(x-<x>)\cdot (y-<y>)\right&gt;$ is the
covariance, defined as:\</p>
<div class="math notranslate nohighlight">
\[\begin{split}V_{ij} = \mbox{cov}(x_{i},x_{j}) = 
\left(\begin{array}{cc}
    \sigma_x^2 &amp; \mbox{cov}(x,y) \\
    \mbox{cov}(x,y)   &amp; \sigma_y^2
  \end{array}\right) =
\left(\begin{array}{cc}
    \sigma_x^2 &amp; \rho\sigma_x \sigma_y \\
    \rho\sigma_x \sigma_y &amp; \sigma_y^2
  \end{array}\right)$$ where $\rho$ is the correlation coefficient
defined in Eq. [\[eq:rho\]](#eq:rho){reference-type=&quot;ref&quot;
reference=&quot;eq:rho&quot;}. The covariance matrix is a *symmetric* $n \times n$
matrix, which can be generalized for a function $f$ of $n$ variables
$x_1,x_2,\ldots x_n$ as:
$$\sigma_f^2=\sum_j\left(\frac{\partial f}{\partial x_j}\right)^2\cdot \sigma^2_{x_j}+\sum_{j}\sum_{k\ne j}\left(\frac{\partial f}{\partial x_j}\right)\left(\frac{\partial f}{\partial x_k}\right)\cdot \mbox{cov}(x_j,x_k)\end{split}\]</div>
<p>For continuous variables the diagonal elements <span class="math notranslate nohighlight">\(V_{ij}\)</span> are the
variances
$<span class="math notranslate nohighlight">\(\sigma_{x_i}^2=\int (x_i-&lt;x_i&gt;)^2 f(x_1,\ldots x_n)dx_1\ldots dx_n\)</span><span class="math notranslate nohighlight">\(
and they are always positive. The off-diagonal elements can be positive
or negative, and they represent the covariances:
\)</span><span class="math notranslate nohighlight">\(V_{ij}=\int (x_i-&lt;x_i&gt;)(x_j-&lt;x_j&gt;) f(x_1,\ldots x_n)dx_1\ldots dx_n.\)</span><span class="math notranslate nohighlight">\(\
**Example** Let \)</span>A = \frac{F-B}{F+B}<span class="math notranslate nohighlight">\( be the measured
forward/backward-asymmetry of an angular distribution, where \)</span>F<span class="math notranslate nohighlight">\( (\)</span>B<span class="math notranslate nohighlight">\()
is the forward (backward) hemisphere of a detector. Be \)</span>N = F+B<span class="math notranslate nohighlight">\( the
total number of measured events. If the uncertainties \)</span>\sigma_{F}<span class="math notranslate nohighlight">\( and
\)</span>\sigma_{B}<span class="math notranslate nohighlight">\( for \)</span>F<span class="math notranslate nohighlight">\( and \)</span>B<span class="math notranslate nohighlight">\( are uncorrelated (this is the case if \)</span>N<span class="math notranslate nohighlight">\(
is not fixed), we have
\)</span><span class="math notranslate nohighlight">\(\sigma_A=\frac{2FB}{N^2}\sqrt{\left(\frac{\sigma_F}{F}\right)^2
+\left(\frac{\sigma_B}{B}\right)^2}.\)</span><span class="math notranslate nohighlight">\( In the case of Poisson
distributed events (\)</span>\sigma_{F}^{2} = F<span class="math notranslate nohighlight">\( and \)</span>\sigma_{B}^{2}=B<span class="math notranslate nohighlight">\() we
have: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\sigma_A&amp;=&amp;\frac{2FB}{N^2}\sqrt{\frac{1}{F}
+\frac{1}{B}} \\
\sigma_A&amp;=&amp;\frac{1-A^2}{2}\sqrt{\left(\frac{1}{F}+\frac{1}{B}\right)}\end{aligned}\)</span>$
From that we can distinguish two limiting cases:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F\sim B\sim N/2\)</span> and the asymmetry <span class="math notranslate nohighlight">\(A\sim 0\)</span>:<br />
Thus the uncertainty is <span class="math notranslate nohighlight">\(\sigma_A\sim \frac{\delta N}{N}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(F\gg B\)</span> and hence <span class="math notranslate nohighlight">\(A\sim +1\)</span>:<br />
<span class="math notranslate nohighlight">\(\sigma_A\sim \frac{2\delta B}{N}\)</span>, i.e. the uncertainty is
dominated by the uncertainty of the smaller number of events.</p></li>
</ul>
<p>Alternatively we can also fix the total number of events <span class="math notranslate nohighlight">\(N\)</span> and
consider the events <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as binomially distributed: let <span class="math notranslate nohighlight">\(p\)</span> is
the probability that a particle is registered in the forward hemisphere
of the detector. From this it follows that:
<span class="math notranslate nohighlight">\(\sigma_F^2=\sigma_B^2=Np(1-p)\sim FB/N\)</span>. Because <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are fully
anti-correlated (which means that cov(F,B) = <span class="math notranslate nohighlight">\(- \sigma^2_{F}\)</span>), it
follows
$<span class="math notranslate nohighlight">\(\sigma_A^2=\left(\frac{\partial A}{\partial F}\, \, \frac{\partial A}{\partial B} \right)\left(\begin{array}{cc}\sigma_F^2 &amp; -\sigma_F^2 \\
-\sigma_F^2 &amp; \sigma_F^2
\end{array}\right)
{\frac{\partial A}{\partial F}\choose \frac{\partial A}{\partial B}}.\)</span><span class="math notranslate nohighlight">\(
Finally with \)</span>\partial A/ \partial F= - \partial A/ \partial B=1/N<span class="math notranslate nohighlight">\( we
get: \)</span><span class="math notranslate nohighlight">\(\sigma_A=\frac{2}{N}\sqrt{\frac{FB}{N}}\)</span>$\</p>
</div>
<div class="section" id="several-functions-of-several-variables">
<h3>Several functions of several variables<a class="headerlink" href="#several-functions-of-several-variables" title="Permalink to this headline">¶</a></h3>
<p>Finally we look at the most general case, in which we have a set of
random variables <span class="math notranslate nohighlight">\({\bf x} = (x_{1}, \ldots, x_{n})\)</span> with expectation
values <span class="math notranslate nohighlight">\({\bf \mu}= (\mu_{1}, \ldots, \mu_{n})\)</span> belonging to the set of
probability density functions <span class="math notranslate nohighlight">\({\bf F}({\bf x})={f_1,f_2,\ldots,f_n}\)</span>.
The covariance matrix <span class="math notranslate nohighlight">\(U_{kl}\)</span> is then given by:
$<span class="math notranslate nohighlight">\(U_{kl}=cov(f_k,f_l)= \sum_{i,j}\left(\frac{\partial f_k}{\partial x_i}\frac{\partial f_l}{\partial x_j}\right)_{\mbox{\bf x=\)</span>{\bf \mu}<span class="math notranslate nohighlight">\(}} cov(x_i,x_j).\)</span><span class="math notranslate nohighlight">\(
This can be written in a shortened way as \)</span>U = A, V, A^{T}<span class="math notranslate nohighlight">\( where the
matrix \)</span>A<span class="math notranslate nohighlight">\( of the derivatives is given by
\)</span><span class="math notranslate nohighlight">\(A_{ij}=\left(\frac{\partial f_i}{\partial x_j}\right)_{\mbox{\bf x=\)</span>\mu<span class="math notranslate nohighlight">\(}}\)</span><span class="math notranslate nohighlight">\(
and \)</span>A^{T}<span class="math notranslate nohighlight">\( is its transpose. The matrices \)</span>U=cov(f_{i},f_{j})<span class="math notranslate nohighlight">\( and
\)</span>V=cov(x_{i},x_{j})<span class="math notranslate nohighlight">\( contain the covariance for \)</span>f<span class="math notranslate nohighlight">\( and \)</span>x<span class="math notranslate nohighlight">\(. Both are
symmetric with dimension \)</span>n \times n<span class="math notranslate nohighlight">\(.\
\
**Example**  Transformation to polar coordinates in 2D. Assume we have
measured a point in cartesian coordinates \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\( with the
uncertainties \)</span>\sigma_{x}<span class="math notranslate nohighlight">\( and \)</span>\sigma_{y}<span class="math notranslate nohighlight">\(. The measurements in \)</span>x<span class="math notranslate nohighlight">\( and
\)</span>y<span class="math notranslate nohighlight">\( shall be independent such that we can write \)</span>V_{11}=\sigma_{x}^{2}<span class="math notranslate nohighlight">\(,
\)</span>V_{22}=\sigma_{y}^{2}<span class="math notranslate nohighlight">\( and \)</span>V_{i \ne j}=0<span class="math notranslate nohighlight">\(. Now we want to get the
covariance matrix in polar coordinates. The transformation equations are
\)</span>f_1:,r^2=x^2+y^2<span class="math notranslate nohighlight">\( and \)</span>f_2:, \theta=arctan (y/x)<span class="math notranslate nohighlight">\(. It follows for
\)</span>A=\partial f_i/\partial x_i$:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \left( \begin{array}{cc}
 \frac{\partial r}{\partial x} &amp; \frac{\partial r}{\partial y}  \\
 \frac{\partial \theta}{\partial x}&amp; \frac{\partial \theta}{\partial y} \end{array} \right)
 =\left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{y}{r}  \\
 \frac{-y}{r^2}&amp; \frac{x}{r^2} \end{array} \right)$$ And with this we
can compute $U=A\,V\,A^{T}$: $$\begin{aligned}
U &amp;=&amp; \left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{y}{r}  \\
 \frac{-y}{r^2}&amp; \frac{x}{r^2} \end{array} \right)
 \cdot \left( \begin{array}{cc}
 \sigma_x^2 &amp; 0  \\
 0&amp; \sigma_y^2 \end{array} \right) \cdot
\left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{-y}{r^2}  \\
 \frac{y}{r}&amp; \frac{x}{r^2} \end{array} \right)\\
U&amp;=&amp;\left( \begin{array}{cc}
 \frac{1}{r^2}(x^2\sigma^2_x+y^2\sigma^2_y) &amp; \frac{xy}{r^3}(-\sigma_x^2+\sigma_y^2)  \\
 \frac{xy}{r^3}(-\sigma_x^2+\sigma_y^2)&amp; \frac{1}{r^4}(y^2\sigma_x^2+x^2\sigma_y^2) \end{array} \right)
=\left( \begin{array}{cc}
\sigma_r^2 &amp; \sigma^2_{r\theta} \\
\sigma^2_{r\theta} &amp; \sigma_{\theta}^2
\end{array} \right)\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="uncertainty-on-efficiencies">
<h2>Uncertainty on efficiencies<a class="headerlink" href="#uncertainty-on-efficiencies" title="Permalink to this headline">¶</a></h2>
<p>Suppose you are performing a “counting experiment” and you decide to
accept events only if they pass a set of selection criteria (e.g. <span class="math notranslate nohighlight">\(p_T&gt;\)</span>
20 GeV; <span class="math notranslate nohighlight">\(|\eta|&lt;2.5\)</span>). Call <span class="math notranslate nohighlight">\(N_0\)</span> the total number of events and <span class="math notranslate nohighlight">\(N_p\)</span>
the subset passing the selection. The efficiency of the selection is:
$<span class="math notranslate nohighlight">\(\epsilon = \frac{N_p}{N_0}\)</span><span class="math notranslate nohighlight">\( You cannot apply a straightforward error
propagation on uncorrelated Poisson uncertainties because \)</span>N_p<span class="math notranslate nohighlight">\( and
\)</span>N_0<span class="math notranslate nohighlight">\( are correlated.\
The correct way to compute the uncertainty is to look at the equivalent
binomial problem with total number of events \)</span>N_0<span class="math notranslate nohighlight">\( and probability
\)</span>\epsilon<span class="math notranslate nohighlight">\( to pass:
\)</span><span class="math notranslate nohighlight">\(\left( \Delta \epsilon \right)^2 = \frac{\epsilon(1-\epsilon)}{N_0}\)</span><span class="math notranslate nohighlight">\(
Another way to get to the same result is to work with the uncorrelated
variables pass \)</span>N_p<span class="math notranslate nohighlight">\( and fail \)</span>N_f<span class="math notranslate nohighlight">\(, such that the total number \)</span>N_0<span class="math notranslate nohighlight">\( is
not a fixed number: \)</span><span class="math notranslate nohighlight">\(\epsilon = \frac{N_p}{N_p + N_f}\)</span><span class="math notranslate nohighlight">\( and then apply
error propagation: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\left( \Delta \epsilon \right)^2 &amp;=&amp; \left( \frac{\partial \epsilon}{\partial N_p}\right)^2(\Delta N_p)^2 + \left(  \frac{\partial \epsilon}{\partial N_f}\right)^2 (\Delta N_f)^2 \\
 &amp;=&amp; \left( \frac{N_f}{N_0^2}\right)^2(\Delta N_p)^2 + \left(-\frac{N_p}{N_0^2}\right)^2 (\Delta N_f)^2 \\
 &amp;=&amp; \frac{(1-\epsilon)^2N_p + \epsilon^2 N_f}{N_0}\\
 &amp;=&amp; \frac{\epsilon(1-\epsilon)}{N_0}\\\end{aligned}\)</span>$</p>
</div>
<div class="section" id="uncertainty-on-the-mean">
<h2>Uncertainty on the mean<a class="headerlink" href="#uncertainty-on-the-mean" title="Permalink to this headline">¶</a></h2>
<p>Suppose we measure <span class="math notranslate nohighlight">\(n\)</span> times the quantity <span class="math notranslate nohighlight">\(x\)</span>. The measured mean value
of <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\(\bar{x} = \sum_i x_i / n\)</span>. As all the single measurements,
also the mean will be affected by statistical fluctuations. The
difference between the measured mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span> (the
true-unknown value of the quantity) is described by a Gaussian
distribution (because of the CLT) with variance
<span class="math notranslate nohighlight">\(V(\bar{x}) = \sigma^2 / n\)</span>. For <span class="math notranslate nohighlight">\(n\to \infty\)</span> (if the measurement is
not biased, see properties of estimators in
Sec. <a class="reference external" href="#sec:propEstimator">[sec:propEstimator]</a>{reference-type=”ref”
reference=”sec:propEstimator”}) it will converge to the “true” value:
<span class="math notranslate nohighlight">\(\langle \bar{x} \rangle = \mu\)</span>. The variance of <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the
variance of <span class="math notranslate nohighlight">\(x\)</span> divided by <span class="math notranslate nohighlight">\(n\)</span>: <span class="math notranslate nohighlight">\(V(\bar{x})= \sigma^2 /n\)</span>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
  Var(\bar{x}) &amp; = &amp;  \langle (\bar{x} -\mu )^2 \rangle \\
               &amp; = &amp;  \langle \left( \frac{1}{n} \sum_i x_i - \mu  \right) ^2\rangle\\
               &amp; = &amp;  \frac{1}{n^2} n \langle x^2 \rangle + \frac{n(n-1)}{n^2} \langle x_i x_j\rangle_{i\neq j} - 2\mu\langle \bar{x} \rangle + \mu^2\\
               &amp; = &amp;  \frac{\langle x^2 \rangle}{n} + \frac{n-1}{n} \mu^2 -\mu^2\\
               &amp; = &amp;  \frac{\langle x^2 -\mu^2\rangle}{n} = \frac{\sigma^2}{n} \\\end{aligned}\)</span><span class="math notranslate nohighlight">\(\
The *standard deviation of the mean* falls like \)</span>1/\sqrt{n}<span class="math notranslate nohighlight">\(. To improve
the resolution of your measurement by a factor of 2 you need to get 4
times more measurements (*slang:* 4 times more statistics).\
\
**Example** Take a photo-detector with an energy resolution of 50 keV.
If a mono-energetic photon (coming e.g. from a certain nuclear decay) is
registered, its energy is only known to a precision of 50 keV. If 100
(mono-energetic) photons are measured (all coming from the same nuclear
decay), then the uncertainty of the mean energy is only
\)</span>50/\sqrt{100} = 5$ keV. For a resolution of 1 keV we need 2500 events.
So, to double the precision, you need four times more photons.\</p>
</div>
<div class="section" id="weighted-mean-sec-weigthedmean">
<h2>Weighted mean {#sec:weigthedMean}<a class="headerlink" href="#weighted-mean-sec-weigthedmean" title="Permalink to this headline">¶</a></h2>
<p>Suppose we want to compute the average of a set of measurements <span class="math notranslate nohighlight">\(x_{i}\)</span>
with different uncertainties <span class="math notranslate nohighlight">\(\sigma_{i}\)</span>. Intuitively the measurements
with large uncertainties will “matter” less than measurements with small
uncertainties.<br />
<br />
<strong>Example</strong> You have two measurements of <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(10\pm0.1\)</span> and <span class="math notranslate nohighlight">\(8\pm 5\)</span>.
In this case the second measurement will have basically no weight in
your knowledge about <span class="math notranslate nohighlight">\(x\)</span>.<br />
<br />
The correct way to obtain the mean in this case is to take into account
explicitly the uncertainty of the measurements: $<span class="math notranslate nohighlight">\(\begin{aligned}
\label{eq:wmean}
\bar{x}&amp;=&amp;\frac{\sum x_i/\sigma_i^2}{\sum 1/\sigma_i^2} \\
\sigma_{\bar{x}}^2&amp;=&amp;\frac{1}{\sum 1/\sigma_i^2}\end{aligned}\)</span>$ In this
case the individual results are weighted such that the values with small
uncertainties contribute more to the average.<br />
Some comments:</p>
<ul class="simple">
<li><p>The weighted mean collapses to the arithmetic mean when fixing all
the uncertainty’s to be equal</p></li>
<li><p>“Few measurements with small uncertainties are better than many
measurements with large uncertainties”. Let the uncertainty of a
first set of <span class="math notranslate nohighlight">\(n_1\)</span> measurements of the quantity <span class="math notranslate nohighlight">\(x\)</span> be <span class="math notranslate nohighlight">\(\sigma_{1}\)</span>.
The uncertainty on the mean is
<span class="math notranslate nohighlight">\(\sigma_{\bar{x}} = \sigma_1/\sqrt{n_1}\)</span>. If we have a second set of
<span class="math notranslate nohighlight">\(n_2\)</span> measurements with uncertainty <span class="math notranslate nohighlight">\(\sigma_{2}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{2} &gt; \sigma_{1}\)</span> then to get to the same precision you need
to collect more data as:
<span class="math notranslate nohighlight">\(n_{2} = n_{1} \left( \frac{\sigma_{2}}{\sigma_{1}} \right) ^{2}\)</span></p></li>
<li><p>Care must be taken if the individual results and their uncertainty’s
deviate too much from each other. Consider the following example: An
experiment measures in one hour <span class="math notranslate nohighlight">\(100 \pm 10\)</span> events, and another
experiment measures in one hour only <span class="math notranslate nohighlight">\(1 \pm 1\)</span> events. The
Eq. <a class="reference external" href="#eq:wmean">[eq:wmean]</a>{reference-type=”ref”
reference=”eq:wmean”} would then tell us that we have <span class="math notranslate nohighlight">\(2 \pm 1\)</span>
events. But the (unweighted) mean would give <span class="math notranslate nohighlight">\(50.5 \pm 5\)</span>. In this
case instead of blindly quote the mean or the weighted mean you
should go back and understand <em>why</em> you get such different outcomes
(it might be a problem of some parameters of the data taking, some
faulty equipment, some trivial mistake etc…). In case you can’t
find any reason for that, it would be wise to give the full
information at hand and preset both results</p></li>
</ul>
<p><strong>Example</strong> Compute the best estimate of the Higgs mass from the ATLAS (
<span class="math notranslate nohighlight">\(m_H= 125.36 \pm 0.41\)</span> GeV) [&#64;ATLASmassH] and CMS
(<span class="math notranslate nohighlight">\(m_H = 125.02 \pm 0.30\)</span> GeV) [&#64;CMSmassH]. Applying the formula for the
weighted average we get: <span class="math notranslate nohighlight">\(m_H = 125.14 \pm 0.24\)</span> GeV. Compare it with
the official LHC combination.\</p>
</div>
<div class="section" id="a-closer-look-at-the-error-matrix-sec-errormatrix">
<h2>A closer look at the error matrix {#sec:errorMatrix}<a class="headerlink" href="#a-closer-look-at-the-error-matrix-sec-errormatrix" title="Permalink to this headline">¶</a></h2>
<p>We have encountered in the previous sections the error matrix (also
called covariance matrix). Here we will take a closer look at it,
focusing on the importance of the off-diagonal terms describing the
correlations.<br />
Let’s start from the case of a 2D probability density function built
from two <em>uncorrelated</em> Gaussian distributions in <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The two
p.d.f.’s are: $<span class="math notranslate nohighlight">\(\begin{aligned}
P(x) &amp;=&amp; \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma_x} e^{-\frac{1}{2}\frac{x^2}{\sigma_x^2}}\\
P(y) &amp;=&amp; \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma_y} e^{-\frac{1}{2}\frac{y^2}{\sigma_y^2}}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
(for simplicity we take the two distributions to be centred at 0) and
the combined 2D uncorrelated distribution is just the product of the
two:
\)</span><span class="math notranslate nohighlight">\(P(x,y) = \frac{1}{2\pi} \frac{1}{\sigma_x\sigma_y} e^{-\frac{1}{2} \left( \frac{x^2}{\sigma_x^2} + \frac{y^2}{\sigma_y^2}\right)}\)</span><span class="math notranslate nohighlight">\(
In one dimension the Gaussian probability is reduced by \)</span>1/\sqrt{e}<span class="math notranslate nohighlight">\(
when moving away from the maximum by 1\)</span>\sigma<span class="math notranslate nohighlight">\(. In 2D this point becomes
a curve and in this particular example an ellipse with equation:
\)</span><span class="math notranslate nohighlight">\(\frac{x^2}{\sigma_x^2} +\frac{y^2}{\sigma_y^2} = 1\)</span><span class="math notranslate nohighlight">\( We can rewrite
the same equation in matrix form (in the case of no correlation is an
overkill but this notation will become useful in the following):
\)</span><span class="math notranslate nohighlight">\(\left(x,y \right)
\left( \begin{array}{cc}
    \frac{1}{\sigma_x^2} &amp; 0 \\
    0 &amp; \frac{1}{\sigma_y^2}  \\
  \end{array} \right)
\left( \begin{array}{c}
  x\\
  y\\
\end{array} \right) = 1\)</span><span class="math notranslate nohighlight">\( The matrix in the previous equation is called
the *inverse of the error matrix* and its inverse is called **error
matrix for \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\(**: \)</span><span class="math notranslate nohighlight">\(\left( \begin{array}{cc}
    \sigma_x^2 &amp; 0 \\
    0 &amp; \sigma_y^2  \\
  \end{array} \right)\)</span><span class="math notranslate nohighlight">\( The general element of the error(covariance)
matrix for \)</span>n<span class="math notranslate nohighlight">\( variables \)</span>x_1,\ldots,x_n<span class="math notranslate nohighlight">\( is given by:
\)</span><span class="math notranslate nohighlight">\(\langle (x_i-\bar{x_i})(x_j-\bar{x_j}) \rangle.\)</span><span class="math notranslate nohighlight">\(\
The notation above allows to treat in a simple way the case of
*correlated variables*.\
Take the previous uncorrelated case and rotate the \)</span>(x,y)<span class="math notranslate nohighlight">\( axes as :
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
x' &amp;=&amp; x\cos\theta - y\sin\theta\\
y' &amp;=&amp; x\sin\theta + y\cos\theta\end{aligned}\)</span><span class="math notranslate nohighlight">\( Let's use a numerical
example: be \)</span>\sigma_x = 1/4<span class="math notranslate nohighlight">\( and \)</span>\sigma_y = 1/2$ (see
Fig. <a class="reference external" href="#fig:errorEllipse">1.2</a>{reference-type=”ref”
reference=”fig:errorEllipse”})</p>
<p><img alt="Interpretation of the uncertainty for two variables: the errorellipse." src="../_images/errorEllipse.png" />{#fig:errorEllipse
width=”50%”}</p>
<p>Then the uncorrelated case reads: $<span class="math notranslate nohighlight">\(16 x^2 + 4 y^2 =1.\)</span><span class="math notranslate nohighlight">\( Applying the
rotation (i.e. correlating the two measurements see
Fig. [1.2](#fig:errorEllipse){reference-type=&quot;ref&quot;
reference=&quot;fig:errorEllipse&quot;}): \)</span><span class="math notranslate nohighlight">\(\left( x~y \right)'
\left( \begin{array}{cc}
    \cos\theta   &amp; \sin\theta \\
    -\sin \theta &amp; \cos \theta  \\
  \end{array} \right)
\left( \begin{array}{cc}
    1/\sigma_x^2 &amp; 0 \\
    0 &amp; 1/\sigma_y^2  \\
  \end{array} \right)
\left( \begin{array}{cc}
    \cos\theta   &amp; -\sin\theta \\
    \sin \theta &amp; \cos \theta  \\
  \end{array} \right)
\left( \begin{array}{c}
    x \\
    y \\
  \end{array} \right)'\)</span><span class="math notranslate nohighlight">\( we get \)</span><span class="math notranslate nohighlight">\(\label{eq:ellipse}
\left( x~y \right)'
\left( \begin{array}{cc}
    13 &amp;  3\sqrt{3} \\
    3\sqrt{3} &amp;  7 \\
  \end{array} \right)
\left( \begin{array}{c}
    x \\
    y \\
  \end{array} \right)'.\)</span><span class="math notranslate nohighlight">\( The matrix in the centre is the &quot;inverse error
matrix&quot; and its inverse is the &quot;error matrix&quot;: \)</span><span class="math notranslate nohighlight">\(\frac{1}{64}
\left( \begin{array}{cc}
    7 &amp;  -3\sqrt{3} \\
    -3\sqrt{3} &amp;  13 \\
  \end{array} \right)\)</span>$ Given the error matrix is trivial to extract
uncertainties on the variables and their correlation coefficients:</p>
<ul class="simple">
<li><p>the uncertainty on <span class="math notranslate nohighlight">\(x'\)</span> is given by intersection of the rectangle
inscribing the ellipse with the x-axis: <span class="math notranslate nohighlight">\(\sigma_{x'}^2 = 7/64\)</span> the
square root of the first diagonal element of the error matrix</p></li>
<li><p>the uncertainty on <span class="math notranslate nohighlight">\(y'\)</span> is given by intersection of the rectangle
inscribing the ellipse with the y-axis: <span class="math notranslate nohighlight">\(\sigma_{y'}^2 = 13/64\)</span> the
square root of the second diagonal element of the error matrix</p></li>
<li><p>the intersection of the ellipse with the x-axis is
<span class="math notranslate nohighlight">\(\sqrt{1/13} = 0.277\)</span> the inverse of the square root of the first
diagonal element of the inverse error matrix</p></li>
<li><p>the intersection of the ellipse with the y-axis is
<span class="math notranslate nohighlight">\(\sqrt{1/7} = 0.378\)</span> the inverse of the square root of the second
diagonal element of the inverse error matrix</p></li>
<li><p>the off-diagonal elements of the error-matrix are
<span class="math notranslate nohighlight">\(\rho \sigma_{x'} \sigma_{y'}\)</span>; knowing <span class="math notranslate nohighlight">\(\sigma_{x'}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{y'}\)</span> from the diagonal elements we obtain a correlation
coefficient <span class="math notranslate nohighlight">\(\rho = 0.54\)</span>.</p></li>
<li><p>the semi-axes of the ellipse are the square roots of the eigenvalues
of the error matrix (here we know the diagonalized matrix, i.e.
before rotation, and we can just read them off: 0.25, 0.5)</p></li>
</ul>
</div>
<div class="section" id="statistical-vs-systematic-uncertainties-sectionstatvssysterrors">
<h2>Statistical vs. Systematic Uncertainties {#SectionStatVSSystErrors}<a class="headerlink" href="#statistical-vs-systematic-uncertainties-sectionstatvssysterrors" title="Permalink to this headline">¶</a></h2>
<p>When repeating measurements (for example to reduce the uncertainty by
averaging over many results), the usual assumption is that the
experiments can be repeated under identical conditions, being
independent of each other and thus giving identical, independent
results. Unfortunately, this <em>ideal world</em> does not exist. Repeated
measurements will give slightly different results, due to diverse
sources such as changing experimental conditions (mostly unknown),
imprecise measurement (resolution), thermal or quantum fluctuations and
others. The differences in the results are “randomly” varying, giving
the so-called <em>statistical uncertainty</em>. For these kind of
uncertainties, as in previous section, repeating the measurement
increases the precision.<br />
A different kind of uncertainty is represented by the <em>systematic
uncertainty</em>[&#64;systBarlow].</p>
<ul class="simple">
<li><p><strong>Systematic effect</strong>= background, selection bias, efficiencies,
energy resolutions, angular resolution, theory
renormalizarion/factorization scales, etc…</p></li>
<li><p><strong>Systematic uncertainty</strong> = the uncertainty in estimating a
systematic effect</p></li>
<li><p><strong>Systematic mistake</strong> = result of negleting such effects</p></li>
</ul>
<p>Systematic uncertainties are usually independent from the statistical
uncertainties. It is therefore important to always quote both
uncertainties separately in the results:
$<span class="math notranslate nohighlight">\(x = 10.2 \pm 0.2 \text{ (stat)} \pm 0.3 \text{ (syst)} [units]%  \pm 0.3 \text{ (theory)} [units].\)</span>$
The systematic uncertainty is a statement made by the experimenters
about their understanding of their own equipment, and in general it will
not decrease with larger data samples (like the statistical
uncertainty)<a class="footnote-reference brackets" href="#id2" id="id1">1</a>. An interesting situation is reached when the
systematic uncertainty is larger than the statistical one. In this case
the precision of the result will not be improved by taking more data; it
will only improve by better understanding the experimental setup.\</p>
</div>
<div class="section" id="work-with-systematic-uncertainties">
<h2>Work with systematic uncertainties<a class="headerlink" href="#work-with-systematic-uncertainties" title="Permalink to this headline">¶</a></h2>
<p>Once the systematic uncertainties are singled out, they can be treated
with the same covariance matrices techniques developed above for the
statistical uncertainties.<br />
Suppose you have two measurements <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> with statistical
uncertainties <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> respectively and a common
systematic uncertainty <span class="math notranslate nohighlight">\(S\)</span>. Putting together the components in a matrix
we get: $<span class="math notranslate nohighlight">\(V_{i,j}^{tot} = \left( \begin{array}{cc}
 \sigma_1^2+S^2 &amp; S^2  \\
 S^2&amp; \sigma_2^2+S^2\end{array} \right)\)</span><span class="math notranslate nohighlight">\(\
If, instead of a constant systematic uncertainty, the uncertainty is
given as a percentage \)</span>T= \epsilon x_{i}<span class="math notranslate nohighlight">\( (e.g. \)</span>\epsilon = 0.01<span class="math notranslate nohighlight">\( for a
1%), then the covariance matrix is:
\)</span><span class="math notranslate nohighlight">\(V_{i,j}^{tot} = \left( \begin{array}{cc}
 \sigma_1^2+\epsilon^2 x_1^2 &amp; \epsilon^2x_1x_2  \\
 \epsilon^2x_1x_2 &amp; \sigma^2+\epsilon^2x_2^2 \end{array} \right)\)</span><span class="math notranslate nohighlight">\(\
\
**Example** Consider two variables \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\( with two sources of
uncertainties: a statistical (\)</span>s_{x}, s_{y}<span class="math notranslate nohighlight">\() with *no* correlation and
a systematic (\)</span>c_{x}, c_{y}<span class="math notranslate nohighlight">\() with *full* correlation (e.g. luminosity):
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
x &amp;=&amp; x_{0} \pm s_{x} \text{ (stat)} \pm c_{x} \text{ (syst)} \\
y &amp;=&amp; y_{0} \pm s_{y} \text{ (stat)} \pm c_{y} \text{ (syst)}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Because the uncertainties are already separated into a correlated and
uncorrelated category, they can be summed up in quadrature at the matrix
level, yielding: \)</span><span class="math notranslate nohighlight">\(V_{ij}^{tot} = \left( \begin{array}{cc}
s_{x}^{2} &amp; 0 \\
0 &amp; s_{y}^{2} \end{array} \right) + \left( \begin{array}{cc}
c_{x}^{2} &amp; c_{xy} \\
c_{yx} &amp; c_{y}^{2} \end{array} \right) = \left( \begin{array}{cc}
\sigma_{x}^{2} &amp; \rho \sigma_{x} \sigma_{y} \\
\rho \sigma_{x} \sigma_{y} &amp; \sigma_{y}^{2} \end{array} \right),\)</span><span class="math notranslate nohighlight">\( where
\)</span>\rho<span class="math notranslate nohighlight">\( is the correlation coefficient
\)</span>\rho = \frac{c_{xy}}{\sigma_{x} \sigma_{y}}<span class="math notranslate nohighlight">\( and
\)</span>\sigma_{i}^{2} = s_{i}^{2} + c_{i}^{2}<span class="math notranslate nohighlight">\( is the sum of the squared
individual uncertainties for \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\(, respectively.\
\
**Example** Take three variables \)</span>x_{1}, x_{2}, x_{3}<span class="math notranslate nohighlight">\( with statistical
uncertainties \)</span>\sigma_{1},\sigma_{2},\sigma_{3}<span class="math notranslate nohighlight">\(, a common systematic
uncertainty \)</span>S<span class="math notranslate nohighlight">\( and a second systematic uncertainty \)</span>T<span class="math notranslate nohighlight">\( shared by only
\)</span>x_1<span class="math notranslate nohighlight">\( and \)</span>x_2<span class="math notranslate nohighlight">\(. In this case the covariance matrix reads:
\)</span><span class="math notranslate nohighlight">\(V_{i,j}^{tot} = \left( \begin{array}{ccc}
 \sigma_1^2+S^2+T^2 &amp; S^2+T^2  &amp; S^2 \\
 S^2+T^2 &amp; \sigma_2^2+S^2+T^2 &amp; S^2 \\
S^2 &amp; S^2 &amp; \sigma_3^2+S^2
 \end{array} \right)\)</span>$</p>
</div>
<div class="section" id="evaluating-systematic-uncertainties">
<h2>Evaluating Systematic Uncertainties<a class="headerlink" href="#evaluating-systematic-uncertainties" title="Permalink to this headline">¶</a></h2>
<p>To deal with systematic uncertainties one has to distinguish between
<em>known</em> and <em>unknown</em> (or <em>unsuspected</em>) sources of uncertainties.<br />
<br />
“Known” sources can be:</p>
<ul class="simple">
<li><p>uncertainties on factors in the analysis: calibration, efficiencies,
corrections, etc…</p></li>
<li><p>theoretical uncertainties on branching ratios, masses, fragmentation
etc…</p></li>
</ul>
<p>To evaluate the impact of systematic uncertainties from known sources
<span class="math notranslate nohighlight">\(s_{i}\)</span> on a correction factor <span class="math notranslate nohighlight">\(F\)</span>, there are several possibilities.
Either one can take several (the more the better) typical assumptions
for <span class="math notranslate nohighlight">\(s_{i}\)</span> and repeat the calculation of <span class="math notranslate nohighlight">\(F\)</span> and then calculate the
standard deviation of <span class="math notranslate nohighlight">\(F\)</span>. Or an experimental parameter (for example the
energy resolution) can be varied up and down by one sigma and check the
change in the variable. Or again another possibility is to take two
extreme assumptions as values for the source <span class="math notranslate nohighlight">\(s_{i}\)</span> and argue that the
true value has to be in between them and use the difference divided by
<span class="math notranslate nohighlight">\(\sqrt{12}\)</span>. The factor <span class="math notranslate nohighlight">\(\sqrt{12}\)</span> is due to the standard deviation of
the uniform distribution, which can be used to model the total ignorance
about the parameter value.<br />
<br />
Uncertainties from “unsuspected” sources can be studied by repeating the
analysis in different ways such as:</p>
<ul class="simple">
<li><p>vary the range of data used for extraction of the result</p></li>
<li><p>use only a subset of the data (e.g. split the data into two
categories)</p></li>
<li><p>change cuts, binning, borders of the histogram.</p></li>
<li><p>change the parameterization or the fit technique</p></li>
</ul>
<p>Finding trends as a result of the above checks usually points to an
“unsuspected” systematic effect.<br />
<br />
However you <em>should not</em> go through the previous list blindly and sum up
in quadrature all resulting deviations. This will simply increase the
systematic uncertainty the more effects the experimenters will conceive!
It is wrong to state that “the more careful you are the bigger should
your systematic uncertainty be”. Remember that out of 20 checks one is
expected to be off more than <span class="math notranslate nohighlight">\(2\sigma\)</span> and every third is off by
<span class="math notranslate nohighlight">\(1\sigma\)</span> or more. If no systematic effect is expected a priori, and if
the deviation from the expected result is not significant, no additional
systematic uncertainty must be added. On the other hand, if you do see a
deviation, try to understand where it comes from and eventually fix it.
Only as a last resort include a discrepancy in systematic uncertainties.</p>
</div>
<div class="section" id="how-many-digits">
<h2>How many digits?<a class="headerlink" href="#how-many-digits" title="Permalink to this headline">¶</a></h2>
<p>We report here the recommendation from the PDG on how to round the
numbers representing your results. The basic rule states that if the
three highest order digits of the uncertainty lie between 100 and 354,
we round to two significant digits. If they lie between 355 and 949, we
round to one significant digit. Finally, if they lie between 950 and
999, we round up to 1000 and keep two significant digits. In all cases,
the central value is given with a precision that matches that of the
uncertainty. So, for example, the result (coming from an average) 0.827
<span class="math notranslate nohighlight">\(\pm\)</span> 0.119 would appear as 0.83 <span class="math notranslate nohighlight">\(\pm\)</span> 0.12, while 0.827 <span class="math notranslate nohighlight">\(\pm\)</span> 0.367
would turn into 0.8 <span class="math notranslate nohighlight">\(\pm\)</span> 0.4.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>L. Lyons [&#64;Lyons], “Statistics for nuclear and particle physics”. Ch. 1
and Ch. 3<br />
R. Barlow [&#64;Barlow], ” A guide to the use of statistical methods in the
physical sciences”. Ch. 4<br />
J.R. Taylor [&#64;Taylor], “Introduction to error analysis”<br />
R. Barlow [&#64;systBarlow], “Systematic Errors: facts and fictions”,
<a class="reference external" href="https://arxiv.org/abs/hep-ex/0207026">https://arxiv.org/abs/hep-ex/0207026</a><br />
PDG [&#64;PDG], On rounding conventions: Introduction\</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Some systematic uncertainties do get reduced with larger data
samples. Take the case of a systematics uncertainty associated to a
calibration. If the calibration is performed on a sample of
available data, the larger the calibration sample the smaller will
be the uncertainty.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tmp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mauro Donega<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>