
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probability &#8212; Statistical Methods and Data Analysis Techniques</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Probability Distributions" href="probabilityDistributions.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Methods and Data Analysis Techniques</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probabilityDistributions.html">
   Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="errors.html">
   Measurements uncertainties {#ch:errors}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="monteCarlo.html">
   Monte Carlo methods {#sec:MC}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood.html">
   Parameter Estimation - Likelihood  {#ChapterParameterEstimations}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="leastSquares.html">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hypothesisTesting.html">
   Hypotheses Testing {#ChapterHypothesisTesting}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="confidenceIntervals.html">
   Confidence Intervals {#ChapterConfidenceLimits}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mva.html">
   Multivariate Analysis Methods {#ChapterMVA}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unfolding.html">
   Unfolding {#ch:Unfolding}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/probability.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/probability.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#randomness">
   Randomness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#axiomatic-definition">
   Axiomatic definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-as-frequency-limit">
   Probability as frequency limit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-probability">
   Conditional probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-theorem">
   Bayes’ Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subjective-probability-or-the-bayesian-interpretation">
   Subjective probability or the Bayesian interpretation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-density-function">
   Probability density function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cumulative-distribution-function">
   Cumulative distribution function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mean-median-and-mode">
   Mean, Median and Mode
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantiles">
   Quantiles
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-value">
   Expectation value
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-and-standard-deviation">
   Variance and Standard Deviation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#full-width-at-half-maximum">
   Full Width at Half Maximum
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#higher-moments">
   Higher Moments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#useful-inequalities">
   Useful Inequalities
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-than-one-dimension">
   More than one dimension
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformation-of-variables">
   Transformation of variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#covariance-and-correlation">
   Covariance and Correlation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problems">
   Problems
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="probability">
<h1>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h1>
<p>I was promised a book on data analysis and the first chapter is about probability, why?</p>
<p>The concept of probability, and how to work with it, is a pre-requisite to perform data analysis. Probabilities are the mathematical tool that allows us to make quantitative statements about data.</p>
<!-- Xcheck [test label](testLabel) and {eq}`my_label` and link to another [`.md`](./unfolding.md) and to {figure} --><div class="section" id="randomness">
<h2>Randomness<a class="headerlink" href="#randomness" title="Permalink to this headline">¶</a></h2>
<p>Probability and randomness come as very closed ideas.
Let’s begin by trying to understand what we mean by randomness.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>A good outline concerning randomness in classical systems can be found in J. Fords article “How random is a coin toss?”<a class="reference external" href="#refs">Ref. 1</a>. The author makes the analogy between quantum mechanics stemming from the finiteness of the Planck’s constant, special relativity stemming from the finiteness of the speed of light and the complexity theory stemming from dropping the assumption of infinite precision.</p>
</div>
<p>The classical example for randomness comes from tossing a coin where the outcome is head or tail. Because it is a classical system, its outcome can in principle be predicted by evaluating the equations of motion. So how can the aspect of randomness arise from a, at least in principle, deterministic system? The issue is that in order to predict precisely the evolution of a physical system it would be necessary to “prepare” it in a configuration with infinitely precise initial conditions, which is in practice not possible. On this case, probability comes in as a convenient tool to predict the outcome of the experiment (head/tail).</p>
<p>A different situation is encountered in quantum mechanics when we study the physics at the atomic and sub-atomic scale. Here the governing laws are “intrinsically” probabilistic.</p>
</div>
<div class="section" id="id1">
<h2>Probability<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>There are several definitions of probability. We will start from the
axiomatic definition and then introduce the frequency interpretation.
After having defined the conditional probability we will consider the
subjective interpretation arising from the Bayes theorem. Later in the
course we will address the effect on statistical inference coming from
the frequentists/Bayesian interpretation of probability.</p>
</div>
<div class="section" id="axiomatic-definition">
<h2>Axiomatic definition<a class="headerlink" href="#axiomatic-definition" title="Permalink to this headline">¶</a></h2>
<p>We beging from the axiomatic definition of probability (also known as
the Kolmogorov axioms). Let <span class="math notranslate nohighlight">\(S\)</span> be the set of possible outcomes of an
experiment. For every possible outcome <span class="math notranslate nohighlight">\(E\)</span> the probability <span class="math notranslate nohighlight">\(p(E)\)</span>
fulfills the following axioms:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(S) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E) \ge 0\)</span>, E <span class="math notranslate nohighlight">\(\in\)</span> S</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\cup E_i) = \sum P(E_i)\)</span> for any set of disjoint <span class="math notranslate nohighlight">\(E_i\)</span> and <span class="math notranslate nohighlight">\(E_j\)</span>
(i.e. when <span class="math notranslate nohighlight">\(E_i\)</span> and <span class="math notranslate nohighlight">\(E_j\)</span> are mutually exclusive)</p></li>
</ul>
<p>It follows immediately that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(E) = 1-P(E^*)\)</span> where <span class="math notranslate nohighlight">\(S = E \cup E^*\)</span> and <span class="math notranslate nohighlight">\(E\)</span> and <span class="math notranslate nohighlight">\(E^*\)</span> are
disjoint</p></li>
<li><p><span class="math notranslate nohighlight">\(P(E) \le 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\emptyset) = 0\)</span> where <span class="math notranslate nohighlight">\(\emptyset\)</span> is the null set</p></li>
</ul>
<p>From these axioms we implement working interpretations of probability:
the frequency limit and the Bayesian.</p>
</div>
<div class="section" id="probability-as-frequency-limit">
<h2>Probability as frequency limit<a class="headerlink" href="#probability-as-frequency-limit" title="Permalink to this headline">¶</a></h2>
<p>The most popular definition of probability is based on the limit of
relative frequencies. Assume we conduct an experiment which has a
certain number of outcomes (events). Suppose we prepare N identical
experiments and find that the outcome <span class="math notranslate nohighlight">\(E_i\)</span> occurs <span class="math notranslate nohighlight">\(N_i\)</span> times. We
assign the probability P<span class="math notranslate nohighlight">\((E_i)\)</span> to the outcome <span class="math notranslate nohighlight">\(E_i\)</span>, defined by its
relative frequency of occurrence:<br />
<span class="math notranslate nohighlight">\(P(E_i)=\lim_{N\rightarrow \infty} \frac{N_i}{N}.\)</span></p>
<p>It’s easy to verify
that this definition satisfies the axioms given above. This definition
is also called the objective posterior probability, because the
probability is defined a posteriori, i.e. <em>after the outcomes of the
experiment are known</em>.
The “frequency limit” approach is very useful in practice, however:</p>
<ul class="simple">
<li><p>the limit does not exist in a strict mathematical sense. This is
because there is no deterministic rule linking the outcome of
experiment <span class="math notranslate nohighlight">\(j\)</span> with the outcome of experiment <span class="math notranslate nohighlight">\(j+1\)</span>.</p></li>
<li><p>how do we prepare <span class="math notranslate nohighlight">\(N\)</span> identical experiments? Is it sufficient if
they are very similar? (e.g. back to the coin toss: at each toss the
coin has some abrasion and the <span class="math notranslate nohighlight">\((j+1)^{th}\)</span> toss is not identical to
the <span class="math notranslate nohighlight">\(j^{th}\)</span>).</p></li>
<li><p>nobody can conduct infinitely many experiments. When does the series
converge to the limit?</p></li>
</ul>
</div>
<div class="section" id="conditional-probability">
<h2>Conditional probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h2>
<p>Let A and B be two different events. The probability for A to happen is
<span class="math notranslate nohighlight">\(P(A)\)</span> and correspondingly <span class="math notranslate nohighlight">\(P(B)\)</span> is the probability for B to happen.
The probability that either A or B happens is given by:</p>
<div class="math notranslate nohighlight" id="equation-my-label">
<span class="eqno">(1)<a class="headerlink" href="#equation-my-label" title="Permalink to this equation">¶</a></span>\[
P(A\,\cup\, B)=P(A)+P(B)-P(A\,\cap\,B)
\]</div>
<p>where <span class="math notranslate nohighlight">\(P(A\, \cap \, B)\)</span> denotes the probability that A and B occur together.
If A and B are
mutually exclusive, then <span class="math notranslate nohighlight">\(P(A\, \cap \, B)=0\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(P(A|B)\)</span> be the <strong>conditional probability</strong> that event A occurs,
given that event B has already occured. Then
<span class="math notranslate nohighlight">\(P(A\, \cap \, B)=  P(A|B) \cdot P(B)\)</span>: the probability that A and B
happen it’s the probability that A happens given B, multiplied by the
probability that B happens. If the two events are independent, then
<span class="math notranslate nohighlight">\(P(A|B)=P(A)\)</span>, i.e. the occurrence of A does not depend on B, and so
<span class="math notranslate nohighlight">\(P(A\, \cap\, B)=P(A)\cdot P(B)\)</span>. The conditional probability <span class="math notranslate nohighlight">\(P(A|B)\)</span>
can be written as:</p>
<p><span class="math notranslate nohighlight">\(P(A|B)=\frac{P(A\, \cap\, B)}{P(B)}.\)</span></p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Suppose you draw a card from a deck of 52 cards and it’s red.
What is the probability that it is a diamond ?</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(diamond|red)=}\frac{\mbox{P(diamond}\, \cap\, \mbox{red)}}{\mbox{P(red)}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\mbox{P(diamond}\, \cap\, \mbox{red)}\)</span> = number of red diamonds divided by the total number of cards = 13/52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(red)}\)</span> = number of red cards divided by the total number of cards = 26/52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(diamond|red)} = \frac{13/52}{26/52} = 13/26  = 0.5\)</span></p>
<p>which makes sense: 50% of the red cards are diamonds</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Suppose you draw a card from a deck of 52 cards and it’s red.</p>
<p>What is the probability that it is a queen ?</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen|red)}=\frac{\mbox{P(queen}\, \cap\, \mbox{red)}}{\mbox{P(red)}} \)</span></p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen}\, \cap\, \mbox{red)}\)</span> = number of red queens divided by the total number of cards = 2 / 52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(red)}\)</span> = number of red cards divided by the total number of cards = 26/52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen|red)}= \frac{2/52}{26/52} = 1/13\)</span></p>
<p>which makes sense: only 2 cards are queen out of the 26 red cards.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Suppose you draw a card from a deck of 52 cards and it’s red.</p>
<p>What is the probability that it is a queen of diamonds ?</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen of diamonds|red)=}\frac{\mbox{P(queen of diamonds}\, \cap\, \mbox{red)}}{\mbox{P(red)}} \)</span></p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen of diamonds}\, \cap\, \mbox{red)}\)</span> = number of queens of diamonds divided by the total number of cards = 1 / 52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(red)}\)</span> = number of red cards divided by the total number of cards = 26/52</p>
<p><span class="math notranslate nohighlight">\(\mbox{P(queen|red)}= \frac{1/52}{26/52} = 1/26\)</span></p>
<p>which makes sense: only 1 cards are queen of diamonds out of the 26 red cards.</p>
</div>
</div>
<div class="section" id="bayes-theorem">
<h2>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h2>
<p>The Bayes’ theorem formalize the relation between the conditional probability P(A|B) and P(B|A):</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Thomas Bayes, British clergyman [1702 - 1761]. The so-called “Bayes’ Theorem” is named after him, but it has been independently re-discovered by Pierre-Simon Laplace [1749 – 1827].</p>
</div>
<div class="math notranslate nohighlight">
\[
P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}
\]</div>
<p>It can be proven in one line by writing <span class="math notranslate nohighlight">\(P(A \cap B)\)</span> in two different ways:</p>
<p><span class="math notranslate nohighlight">\(P(A\, \cap\, B)= P(A|B) \cdot P(B) =  P(B|A) \cdot P(A) = P(B\, \cap\, A)\)</span></p>
<p>and hence</p>
<p><span class="math notranslate nohighlight">\(P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}.\)</span></p>
<p>In the general case of <span class="math notranslate nohighlight">\(n\)</span>-classes of events with the properties <span class="math notranslate nohighlight">\(A_{i}\)</span>, the theorem
generalizes to:</p>
<div class="math notranslate nohighlight">
\[
P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_i P(B|A_i)\cdot P(A_i)}.
\]</div>
<p>The theorem states that the probability that A happens given B is equal to
the probability that B happens given A (note that A and B are inverted)
times your <strong>prior knowledge</strong> about A and divided by a <strong>normalization
factor</strong> P(B). The normalization P(B), the probability for B to happen
given that A or not-A happens, in all practical cases is expresses as:
<span class="math notranslate nohighlight">\(P(B) = P(B|A)P(A)+P(B|\mbox{not-A})P(\mbox{not-A})\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Consider a topic coming from virology. We assume that 0.1%
of all swans of a certain colony are infected by an influenza virus. A
specially developed test for influenza shall, in case of an infected
bird, have a detection efficiency of 98%. Unfortunately, the probability
of error (false positive) is 3%, which means that the test indicates an
infection in 3% of all cases where the bird is not infected.</p>
<p>We ask now for the probability <span class="math notranslate nohighlight">\(P\)</span> that, after having had a positive test, a swan
is actually infected by the influenza virus.</p>
<p>According to the given information, we have:</p>
<p><span class="math notranslate nohighlight">\(P({\rm influenza})=0.001\)</span> (0.1% of all swans are infected)</p>
<p><span class="math notranslate nohighlight">\(P({\rm non-influenza})=1-0.001=0.999\)</span>.</p>
<p>The probabilities for the test response are:</p>
<p><span class="math notranslate nohighlight">\(P(+|{\rm influenza})=0.98\)</span> and</p>
<p><span class="math notranslate nohighlight">\(P(-|{\rm influenza})=1-0.98=0.02\)</span>.</p>
<p>Where <span class="math notranslate nohighlight">\(P(+|{\rm influenza})\)</span>  (<span class="math notranslate nohighlight">\(P(-|{\rm influenza})\)</span>) denote the positive (negative) response
under the condition that the swan is actually infected.</p>
<p>Furthermore, the probability of a wrong result of the test is given by</p>
<p><span class="math notranslate nohighlight">\(P(+|{\rm non-influenza})=0.03\)</span> and</p>
<p><span class="math notranslate nohighlight">\(P(-|{\rm non-influenza})=1-0.03=0.97\)</span>.</p>
<p>Therefore, the probability <span class="math notranslate nohighlight">\(P({\rm influenza}|+)\)</span> that a swan is infected, given the test was positive, is</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
P({\rm influenza}|+)
&amp;=&amp;\frac{P(+|{\rm influenza})\cdot P({\rm influenza})}{P(+|{\rm influenza})\cdot P({\rm influenza})+P(+|{\rm non-influenza})\cdot P({\rm non-influenza})}\\
&amp;=&amp;\frac{0.98\times 0.001}{0.98\times 0.001+0.03\times 0.999}\\
&amp;=&amp;3\%.
\end{aligned}\)</span></p>
<p>This means that an infection is present in only 3% of all cases in which
the test was positive!</p>
</div>
</div>
<div class="section" id="subjective-probability-or-the-bayesian-interpretation">
<h2>Subjective probability or the Bayesian interpretation<a class="headerlink" href="#subjective-probability-or-the-bayesian-interpretation" title="Permalink to this headline">¶</a></h2>
<p>In Bayesian inference the probability is interpreted as a subjective
“<strong>degree of belief</strong>” which can be modified by observations (more
data). This is the strength of the Bayesian theorem: it provides a
quantitative way to update the initial knowledge (prior belief) about a
proposition when new data becomes available. Try to compute as an
exercise how the <span class="math notranslate nohighlight">\(P({\rm influenza}|+)\)</span> of the previous example changes
if you get two consecutive positive tests.</p>
<p>A typical application of Bayes theorem in physics is to take the Bayes’ theorem and rewrite
it interpreting P(A) as the probability (“belief”) that a theory is
correct before doing the experiment; P(B<span class="math notranslate nohighlight">\(|\)</span>A) = P(result <span class="math notranslate nohighlight">\(|\)</span> theory) is
the probability of getting the result if the theory is true; P(B) = P
(result) is the probability of getting the result irrespective of
whether the theory is true or not, and P(A<span class="math notranslate nohighlight">\(|\)</span>B) = P (theory <span class="math notranslate nohighlight">\(|\)</span> result)
is our belief in the theory after having obtained the result.</p>
<div class="math notranslate nohighlight">
\[
P(\mbox{theory}|\mbox{result})=\frac{P(\mbox{result}|\mbox{theory}) \cdot P(\mbox{theory})}{P(\mbox{result})}.
\]</div>
<p>It’s important to notice the <em>inversion of the logic</em>: for
<span class="math notranslate nohighlight">\(P(\mbox{theory}|\mbox{result})\)</span> you have collected the data and you are
evaluating the probability of the theory to be right; for
<span class="math notranslate nohighlight">\(P(\mbox{result}|\mbox{theory})\)</span> (called the <strong>likelihood</strong>, we will
come back to the reason of this name later in these notes when talking
about fits) you are estimating the probability to obtain such a data
distribution given a certain theory.<br />
<br />
The fundamental difference between the frequentist approach and the
Bayesian approach relies in the interpretation. The frequentist’s
probability is interpreted as a <em>State of Nature</em>, whereas the Bayesian
probability is a <em>State of Knowledge</em> inducing inevitably some
subjectivity. Thus the probability of an event <span class="math notranslate nohighlight">\(P(E)\)</span> depends on the
information which is accessible to the observer. The function <span class="math notranslate nohighlight">\(P(E)\)</span> is
therefore not purely an intrinsic function of the event, it rather
depends on the knowledge and information possessed by the observer. A
question like “what is the probability that SuperSymmetry is a true
symmetry of nature?” has no meaning in frequentist inference: it is or
it is not. There is no probability associated to it. We can instead
associate a probability in Bayesian interference, interpreting the
probability as degree of belief.<br />
<br />
There is no a priori way to assign the “prior” assumption <span class="math notranslate nohighlight">\(P(A)\)</span> (in the
previous example <span class="math notranslate nohighlight">\(P(\mbox{theory})\)</span>): the assignment of the prior probability is
subjective. The usual prescription is to assume complete ignorance about
the prior P(A) and take all values of A as equiprobable.<br />
There are objections to this postulate:</p>
<ul class="simple">
<li><p>if we are completely ignorant about P(A), how do we know it is a
constant?</p></li>
<li><p>a different choice of P(A) would give a different P(A<span class="math notranslate nohighlight">\(|\)</span>B)</p></li>
<li><p>if we are ignorant about P(A), we are also ignorant about P(<span class="math notranslate nohighlight">\(A^2\)</span>)
or P(1/A), etc… taking any of these (P(<span class="math notranslate nohighlight">\(A^2\)</span>) or P(1/A), etc…)
as constant would imply a different P(A), giving a different
posterior probability.</p></li>
</ul>
<p>These objections are usually answered by the assertion (supported by
experience) that P(A<span class="math notranslate nohighlight">\(|\)</span>B) usually converges to about the same value
after several experiments irrespective of the initial choice of prior
P(A).</p>
<p>The distinction between the frequentist and Bayesian approaches to
statistical inference reaches its climax when addressing the problem of
setting confidence intervals in Sec. <a class="reference internal" href="confidenceIntervals.html"><span class="doc std std-doc"><code class="docutils literal notranslate"><span class="pre">Confidence</span> <span class="pre">Intervals</span></code></span></a>. The following is an example to give an idea of the problem.</p>
<div class="tip admonition">
<p class="admonition-title">Example: FIXME</p>
<p>Let’s take the measurement of the mass of the electron as
an example to understand the difference between the frequentist and
Bayesian interpretation of confidence intervals. You measure the mass of
the electron to be <span class="math notranslate nohighlight">\(520 \pm 10~keV/c^2\)</span>, i.e. you measured <span class="math notranslate nohighlight">\(520~keV/c^2\)</span>
with an apparatus with a resolution of <span class="math notranslate nohighlight">\(10~keV/c^2\)</span>.
It is tempting to conclude that “the mass of the electron is between 510 and 530 <span class="math notranslate nohighlight">\(keV/c^2\)</span> with 68%
probability”. This is not the frequentist’s meaning of probability. In
the frequentist interpretation, the statement that the electron has a
certain mass with a certain probability is nonsense. The electron has a
definite mass, the problem is that we do not know what the value is. It
sounds much more like a Bayesian statement: with a resolution, or
“error”, of <span class="math notranslate nohighlight">\(\sigma = 10~keV/c^2\)</span>, the probability that we will measure
a mass m when the true value is <span class="math notranslate nohighlight">\(m_e\)</span> is</p>
<p><span class="math notranslate nohighlight">\(P(m | m_e) \propto e^{-(m - m_e)^2/2\sigma^2}\)</span></p>
<p>this is the likelihood
term. Then by Bayes’ theorem, the probability that the true mass has the
value <span class="math notranslate nohighlight">\(m_e\)</span> after we have measured a value m is</p>
<p><span class="math notranslate nohighlight">\(P(m_e | m) = \frac{P(m | m_e)P_{prior}(m_e)}{P(m)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\propto P (m | m_e) \;\; \mbox{assuming} \;\;\;P_{prior}(m_e) = const\)</span></p>
<p><span class="math notranslate nohighlight">\(\propto e^{-(m-m_e)^2/2\sigma^2}\)</span></p>
<p>What we typically state is that
“the mass of the electron is between 510 and <span class="math notranslate nohighlight">\(530~keV/c^2\)</span> at 68%
confidence level”. Note the use of “confidence level” instead of “
probability”.</p>
<p>This and other subtleties will be discussed further when discussing
confidence intervals.</p>
</div>
</div>
<div class="section" id="probability-density-function">
<h2>Probability density function<a class="headerlink" href="#probability-density-function" title="Permalink to this headline">¶</a></h2>
<p>We define <strong>random variable</strong> any function of the data. The <strong>event
space</strong> is the set of all possible values of a random variable. A random
variable which can take any value between two arbitrarily given points
in the event space is called a <strong>continuous</strong> variable; conversely, if
the variable can only take certain values it is called a <strong>discrete</strong>
variable. In the same manner, data described by discrete or continuous
variables are called discrete data or continuous data respectively. The
distribution <span class="math notranslate nohighlight">\(f(x)\)</span> of a random variable <span class="math notranslate nohighlight">\(x\)</span> is called <strong>probability
density function (p.d.f.)</strong>. <span class="math notranslate nohighlight">\(f(x')dx'\)</span> is the probability to find <span class="math notranslate nohighlight">\(x\)</span>
in the interval between <span class="math notranslate nohighlight">\(x'\)</span> and <span class="math notranslate nohighlight">\(x'+dx'\)</span> and it is normalized
<span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty}f(x')dx'=1\)</span> (the probability to find <span class="math notranslate nohighlight">\(x\)</span>
anywhere in its event space is 1). Note that <span class="math notranslate nohighlight">\(f(x)\)</span> is <em>not</em> a
probability but <span class="math notranslate nohighlight">\(f(x)dx\)</span> is.</p>
</div>
<div class="section" id="cumulative-distribution-function">
<h2>Cumulative distribution function<a class="headerlink" href="#cumulative-distribution-function" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(x\)</span> be a one-dimensional continuous random variable distributed
according to <span class="math notranslate nohighlight">\(f(x)\)</span>. The <strong>cumulative distribution function (cdf)</strong>
<span class="math notranslate nohighlight">\(F(x')\)</span> gives the probability that the random variable <span class="math notranslate nohighlight">\(x\)</span> will be found
to have a value less than or equal to <span class="math notranslate nohighlight">\(x'\)</span>:</p>
<p><span class="math notranslate nohighlight">\(F(x') = \int_{-\infty}^{x'} f(x)dx\)</span></p>
<p>It follows trivially (with some abuse of notation) that
<span class="math notranslate nohighlight">\(F(-\infty)=0\)</span> and <span class="math notranslate nohighlight">\(F(+\infty)=1\)</span>. The function <span class="math notranslate nohighlight">\(F\)</span> is a monotonously
(but not necessarily strictly monotonously) rising function of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>The probability density function <span class="math notranslate nohighlight">\(f(x)\)</span> is then simply <span class="math notranslate nohighlight">\(f(x)=dF(x)/dx\)</span>.</p>
<p>The function <span class="math notranslate nohighlight">\(F\)</span> is dimensionless whereas the function <span class="math notranslate nohighlight">\(f\)</span> has dimension
<span class="math notranslate nohighlight">\(1/x\)</span>. The probability to observe the random variable <span class="math notranslate nohighlight">\(x\)</span> between two
values <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> can be written in terms of the cdf as:</p>
<p><span class="math notranslate nohighlight">\(P(x_1 \le x \le x_2)=\int_{x_1}^{x_2} f(x')dx'=F(x_2)-F(x_1)\)</span></p>
<!-- 

The relationship between $f$ and $F$ is depicted in

```{figure} ./tmp/Section1Bilder/FWHM.png
---
name: directive-fig
class: bg-primary mb-1
height: 150px
width: 200px
align: center
---
A density function $f(x)$ as well as its cumulative function $F(x)$.
```
-->
<p>The following is an example to show what is the cumulative of a pdf (chosen to be a gaussian - see <a class="reference external" href="./probabilityDistributions.html#gaussian-or-normal-distribution">next chapter</a> for more details on the gaussian pdf).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import relevant libraries</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># range of the x axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pdf(x)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> <span class="c1"># plot the gaussian pdf (norm.pdf) as a function of x</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/probability_10_0.png" src="_images/probability_10_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pdf(x)&#39;</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># cumulative plotted from -5 to 1</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/probability_11_0.png" src="_images/probability_11_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;cdf(x)&#39;</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># range of the x axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/probability_12_0.png" src="_images/probability_12_0.png" />
</div>
</div>
</div>
<div class="section" id="mean-median-and-mode">
<h2>Mean, Median and Mode<a class="headerlink" href="#mean-median-and-mode" title="Permalink to this headline">¶</a></h2>
<p>The <strong>arithmetic mean</strong> <span class="math notranslate nohighlight">\(\bar{x}\)</span> (or simply mean value) of a set of <span class="math notranslate nohighlight">\(N\)</span>
numbers <span class="math notranslate nohighlight">\(X_{i}\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\bar{x}=\frac{1}{N}\sum_{i=1}^N X_i\)</span></p>
<p>The mean of a function of <span class="math notranslate nohighlight">\(x\)</span>, (<span class="math notranslate nohighlight">\(\bar{f}\)</span>) can be calculated analogously:</p>
<p><span class="math notranslate nohighlight">\(\bar{f}=\frac{1}{N}\sum_{i=1}^N f(X_i).\)</span></p>
<p>If the <span class="math notranslate nohighlight">\(N\)</span> data points are
classified in <span class="math notranslate nohighlight">\(m\)</span> intervals (i.e. bins of a histogram), and if <span class="math notranslate nohighlight">\(n_{i}\)</span> stands for the number of entries in the
interval (bin) <span class="math notranslate nohighlight">\(i\)</span>, then:</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>See also the weighted mean in <a class="reference external" href="./errors.html#weighted-mean-sec-weigthedmean">Measurements uncertainties</a></p>
</div>
<p><span class="math notranslate nohighlight">\(\bar{x}=\frac{1}{N}\sum_{i=1}^{m}n_iX_i.\)</span></p>
<p>The <strong>median</strong> of a random variable <span class="math notranslate nohighlight">\(x\)</span> divides a frequency distribution
into two equally sized halves:</p>
<p><span class="math notranslate nohighlight">\(\int_{-\infty}^{x_{median}}f(x')dx'=\int_{x_{median}}^{+\infty}f(x')dx'=0.5.\)</span></p>
<p>The <strong>mode</strong> corresponds to the value of <span class="math notranslate nohighlight">\(x\)</span> where the probability
density <span class="math notranslate nohighlight">\(f(x)\)</span> has a maximum. The mode is not necessarily unique: if a
distribution has two maxima, we call it <em>bimodal</em>, if it has several
maxima, we call it <em>multimodal</em>. When only one maximum is present the
mode is also called <strong>most probable value</strong>.</p>
<!--
A sketch to illustrate the meaning of the mean, median, mode is shown in
Fig. [1.2](#BarlowComic){reference-type="ref" reference="BarlowComic"}.\

![[\[BarlowComic\]]{#BarlowComic label="BarlowComic"} The distribution
of the monthly income of Americans around the year 1950. This pictorial
representation explains well the differences between mean, mode and
median. Tricky question: which of the three describes the most important
property of the distribution? Ask yourself what is for you the most
important property, and what is the message you want to convey
highlighting that. (Taken
from [@Barlow])](Section1Bilder/BarlowComic){#BarlowComic
width="0.5\\linewidth"}

-->
<p>A rough relation between mode, median and mean (true for unimodal and
“not very skewed” distributions) is given by</p>
<p><span class="math notranslate nohighlight">\(\mbox{ Mean - Mode}=3\times\mbox{ (Mean - Median)}.\)</span></p>
<p>So by knowing
two out of the three, the third one can be estimated easily by this
formula.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 

<span class="n">mu</span><span class="p">,</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;array = &quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;size of the array = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array =  [10.02121056  7.98621946  4.03976427 -7.98567496 -4.12666818  4.48032298
  2.305154   -8.22200181  3.39185546  7.82969407 -3.22509666]
size of the array =  11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#compute the mean</span>
<span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
    <span class="nb">sum</span> <span class="o">+=</span> <span class="n">x</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;mean = &quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># or using the numpy mean function</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;mean = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean =  1.4995253805232884
mean =  1.4995253805232884
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute the median</span>
<span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sorted array = &quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sorted array =  [10.02121056  7.98621946  4.03976427 -7.98567496 -4.12666818  4.48032298
  2.305154   -8.22200181  3.39185546  7.82969407 -3.22509666]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.3918554582113787
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute the mode of this array:</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mode</span>
<span class="c1"># this will give you back the mode and the number of times it appears</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mode ModeResult(mode=array([5]), count=array([3]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the distribution to see visually the mode</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/probability_18_0.png" src="_images/probability_18_0.png" />
</div>
</div>
<p>The <strong>geometric mean</strong> <span class="math notranslate nohighlight">\(\mu_g\)</span> is defined as:</p>
<p><span class="math notranslate nohighlight">\(\mu_g = \sqrt[N]{x1\cdot x2\cdot \dots \cdot x_N}.\)</span></p>
<p>It is used to
characterize the mean of a geometric sequence
<span class="math notranslate nohighlight">\((a, ar, ar^2, ar^3, ...)\)</span>. The geometric interpretation of the
geometric mean of two numbers, a and b, is the length of one side of a
square whose area is equal to the area of a rectangle with sides of
lengths a and b.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>A population of bacteria grows from 2000 to 9000 in 3 days.
What is the daily grow (assuming a constant rate r) ?<br />
<span class="math notranslate nohighlight">\(1^{st}\)</span> day: <span class="math notranslate nohighlight">\(n_1\)</span> = 2000 + 2000 r<br />
<span class="math notranslate nohighlight">\(2^{nd}\)</span> day: <span class="math notranslate nohighlight">\(n_2\)</span> = n1 + n1 r = 2000 <span class="math notranslate nohighlight">\((1+r)^2\)</span><br />
<span class="math notranslate nohighlight">\(3^{rd}\)</span> day: <span class="math notranslate nohighlight">\(n_3\)</span> = n2 + n2 r = 2000 <span class="math notranslate nohighlight">\((1+r)^3\)</span> = 9000<br />
<span class="math notranslate nohighlight">\(\Rightarrow  1+r = \sqrt[3]{4.5} \Rightarrow r = 65.1\%\)</span>\</p>
</div>
<p>The function</p>
<p><span class="math notranslate nohighlight">\(f(x) = a \cdot (1+f)^x\)</span></p>
<p>is called geometrical or exponential growth when <span class="math notranslate nohighlight">\(x\)</span> is discrete or continuous respectively.<br />
It covers a particularly important role in finance where it describes
the compound interest.<br />
<br />
The <strong>harmonic mean</strong> <span class="math notranslate nohighlight">\(H\)</span> is defined as:</p>
<p><span class="math notranslate nohighlight">\(\frac{1}{H}=\frac{1}{N}\sum_{i} \frac{1}{X_{i}}.\)</span></p>
<p>It is characterize
the mean value of a harmonic sequence</p>
<p><span class="math notranslate nohighlight">\(\frac{1}{a}\;, \;\frac{1}{a+d}\;, \;\frac{1}{a+2d}\;,...\;,\; \frac{1}{a+kd}\;,...\)</span></p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>A car travels at 80 km/h for the half of the trip and 100
km/h for the second half. What’s his average speed? 2/(1/80+1/100) = 89
km/h (averaging over periods of time not distances)</p>
</div>
</div>
<div class="section" id="quantiles">
<h2>Quantiles<a class="headerlink" href="#quantiles" title="Permalink to this headline">¶</a></h2>
<p>Quantiles are values taken from the inverse of the
cdf of a random variable. When they are taken at regular intervals
they get special names. If the set of data is split into:</p>
<ul class="simple">
<li><p>two equally sized parts, then the value in the middle is the median.</p></li>
<li><p>four equally sized parts, then the four values are called <em>quartiles</em> Q1, Q2, Q3 and Q4. The value of Q2 coincides to the median.</p></li>
<li><p>ten parts we call them <em>deciles</em></p></li>
<li><p>hundred parts we call them <em>percentile</em>.</p></li>
</ul>
<p>The quantiles allow to describe any data distribution without knowing what is its underlying pdf. For example, when a baby is born the pediatrician will compare her/his weight or height with respect to a reference population of babies of the same age, in terms of percentiles. When a baby is in the 20-th weight percentile, it means that 20% of the babies in the reference sample will be lighter.</p>
<p>The following is an example of deciles on a gaussian distribution.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">15</span><span class="p">])</span>

<span class="c1"># cumulative of the gaussian pdf</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;x [units]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;cdf(x)&#39;</span><span class="p">)</span>

<span class="c1"># draw vertical lines</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="mf">0.1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="n">step</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="s1">&#39;0.2&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">),</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">)],</span> <span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="s1">&#39;0.2&#39;</span><span class="p">)</span>

<span class="c1"># gaussian pdf </span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;x [units]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;pdf(x)&#39;</span><span class="p">)</span>

<span class="c1"># write out the corresponding numerical values</span>
<span class="n">s</span> <span class="o">=</span><span class="p">[]</span>
<span class="n">p</span> <span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="mf">0.1</span>
    <span class="n">s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">))</span>
    <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">),</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">)],</span> <span class="p">[</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">step</span><span class="p">)),</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="s1">&#39;0.2&#39;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;0.0&#39;, &#39;0.1&#39;, &#39;0.2&#39;, &#39;0.3&#39;, &#39;0.4&#39;, &#39;0.5&#39;, &#39;0.6&#39;, &#39;0.7&#39;, &#39;0.8&#39;, &#39;0.9&#39;, &#39;1.0&#39;]
[&#39;-inf&#39;, &#39;-1.282&#39;, &#39;-0.842&#39;, &#39;-0.524&#39;, &#39;-0.253&#39;, &#39;0.000&#39;, &#39;0.253&#39;, &#39;0.524&#39;, &#39;0.842&#39;, &#39;1.282&#39;, &#39;inf&#39;]
</pre></div>
</div>
<img alt="_images/probability_21_1.png" src="_images/probability_21_1.png" />
</div>
</div>
</div>
<div class="section" id="expectation-value">
<h2>Expectation value<a class="headerlink" href="#expectation-value" title="Permalink to this headline">¶</a></h2>
<p>The <strong>expectation value</strong> of a random variable <span class="math notranslate nohighlight">\(x\)</span> (or first moment) is defined as:</p>
<p><span class="math notranslate nohighlight">\(&lt;x&gt;=\int_{-\infty}^{\infty} x' f(x') dx'\)</span></p>
<p>and for discrete variables
<span class="math notranslate nohighlight">\(r\)</span> as:</p>
<p><span class="math notranslate nohighlight">\(&lt;r&gt;=\sum r_i P(r_i).\)</span></p>
<p>where <span class="math notranslate nohighlight">\(f(x)dx\)</span> is the probability that
the value x is found to be in the interval <span class="math notranslate nohighlight">\((x, x+dx)\)</span> and in the
discrete case <span class="math notranslate nohighlight">\(P(r_i)\)</span> is the probability that the value <span class="math notranslate nohighlight">\(r_i\)</span> occurs.<br />
<br />
The expectation value reduces to the arithmetic mean when the
probability for any value to occur (<span class="math notranslate nohighlight">\(f(x)\)</span> in the continuous case or
<span class="math notranslate nohighlight">\(P(r)\)</span> in the discrete case) is constant. Take e.g. the discrete case
and set the probability for any event <span class="math notranslate nohighlight">\(r_i\)</span> <span class="math notranslate nohighlight">\((i = 1,...,N )\)</span> to occurr
to be the same. Because <span class="math notranslate nohighlight">\(P(r_i)\)</span> is normalized <span class="math notranslate nohighlight">\(\sum_{i=1}^N P(r_i) = 1\)</span>
then for each <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(P(r_i) = 1/N\)</span>:</p>
<p><span class="math notranslate nohighlight">\(&lt; r &gt; = \sum r_i P(r_i) = \frac{1}{N}\sum r_i\)</span></p>
<p>More generally, the expectation value for any <em>symmetric</em> probability
distribution will coincide with the arithmetic mean. In case of
<em>asymmetric</em> distributions, the expectation value will be pulled towards
the side of the tail (more details on this when we will see some
examples of probability distibution functions).<br />
<br />
The expectation value of a function <span class="math notranslate nohighlight">\(h(x)\)</span> is defined by
<span class="math notranslate nohighlight">\(&lt;h&gt;=\int h(x')f(x')dx'\)</span>.<br />
<br />
The expectation value is a linear operator, i.e.</p>
<p><span class="math notranslate nohighlight">\(&lt;a\cdot g(x)+b\cdot h(x)&gt; = a  &lt;g(x)&gt;+\,b  &lt;h(x)&gt;,\)</span></p>
<p>but in general <span class="math notranslate nohighlight">\(&lt;fg&gt;\ne&lt;f&gt;&lt;g&gt;\)</span>. The equality is true only if <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are
independent.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>In quantum mechanics, to compute the expectation value of A,
the eigenvalues (outcomes of a measurement) are weighted on their
probability to occur:
<span class="math notranslate nohighlight">\(\langle A \rangle_\psi = \sum_j a_j |\langle\psi|\phi_j\rangle|^2\)</span></p>
</div>
</div>
<div class="section" id="variance-and-standard-deviation">
<h2>Variance and Standard Deviation<a class="headerlink" href="#variance-and-standard-deviation" title="Permalink to this headline">¶</a></h2>
<p>The expectation values of <span class="math notranslate nohighlight">\(x^{n}\)</span> and of <span class="math notranslate nohighlight">\((x-&lt;x&gt;)^{n}\)</span> are called the
<span class="math notranslate nohighlight">\(n^{th}\)</span> <em>algebraic</em> moment <span class="math notranslate nohighlight">\(\mu_{n}\)</span> and the <span class="math notranslate nohighlight">\(n^{th}\)</span> <em>central</em> moment
<span class="math notranslate nohighlight">\(\mu'_{n}\)</span>, respectively. Central refers to the fact that the
expectation value of x is subtracted from each value of x (which in case
of a symmetric distribution effectively centers it at zero).<br />
<br />
The first algebraic moment <span class="math notranslate nohighlight">\(\mu_{1}\)</span> is equal to the expectation value
<span class="math notranslate nohighlight">\(&lt;x&gt;\)</span> and it is usually just called <span class="math notranslate nohighlight">\(\mu\)</span>. The first central moment is
zero by definition.</p>
<p>The second central moment is a measure of the width
of a probability distribution and is called <strong>variance</strong> <span class="math notranslate nohighlight">\(V(x)\)</span>. Its
square root is called the <strong>standard deviation</strong> <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
V(x) &amp;=&amp; &lt;(x-&lt;x&gt;)^2&gt;              = &lt;x^2 + &lt;x&gt;^2 -2x&lt;x&gt; &gt;= \\ 
     &amp;=&amp;  &lt;x^2&gt; + &lt;x&gt;^2 - 2&lt;x&gt;&lt;x&gt; = &lt;x^2&gt;-&lt;x&gt;^2          =\sigma^2\end{aligned}\)</span></p>
<p>where we just used the linearity of the expectation value.</p>
<p>We will see when discussing the characteristic function that any a pdf
is uniquely described by its moments (see
<a class="reference external" href="./probabilityDistributions.html#characteristic-function-sec-characteristic">Characteristic function</a>)</p>
<p>It is important to notice that quantities like the variance or the
standard deviation are defined using expectation values, and they can
only be determined if the “true” underlying probability density of the
sampling distribution is known. When analysing data we need to distinguish the distribution of the
collected data, the (known) <strong>sampling distribution</strong>, from the (unknown) <strong>parent
distribution</strong> that we are effectively sampling with our measurements.
One of the goals of data analysis (called parameter estimation) is to
estimate the parameters of the parent distribution from the properties
of the collected data sample.</p>
<p>When the the mean <span class="math notranslate nohighlight">\(&lt;x&gt;\)</span> of the parent distribution is not known, we define the
<em>sample variance</em>, commonly called <span class="math notranslate nohighlight">\(s^{2}\)</span>, as:</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
\label{samplevariance}
  s^2&amp;=&amp; \frac{1}{N-1}\sum_i(x_i-\bar{x})^2 \\\end{aligned}\)</span>.</p>
<p>To compute this expression, one has to loop on the data a first time to compute the sample mean, and then a second time to compute the sample variance. A little algebra, transforms the previous expression into</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
     s^2 &amp;=&amp; \frac{1}{N-1}\left(\sum_i x_i^2-\frac{1}{N}\left(\sum_i x_i\right)^2\right)                                                                   \end{aligned}\)</span></p>
<p>that can be computed by looping only once on the events. This is particularly useful when the number of data points is huge.</p>
<p>The value of <span class="math notranslate nohighlight">\(s^{2}\)</span> can be understood as the best estimate of the
“true” variance of the parent distribution. The origin of the factor
<span class="math notranslate nohighlight">\(\frac{1}{N-1}\)</span> instead of the usual <span class="math notranslate nohighlight">\(\frac{1}{N}\)</span> will become clear
when we will discuss the <a class="reference external" href="./likelihood.html#properties-of-the-estimators-sec-propestimator">theory of estimators</a></p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>The following table collects some of the quantities defined above,
for the Maxwell distribution. The probability density for the magnitude
of the velocity <span class="math notranslate nohighlight">\(v\)</span> of molecules in an ideal gas at temperature <span class="math notranslate nohighlight">\(T\)</span> is
given by:</p>
<div class="math notranslate nohighlight">
\[
f(v)=N\cdot (m/2\pi k_{B}T)^{\frac{3}{2}}\exp(-mv^2/2k_{B}T)\cdot 4\pi v^2.
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(m\)</span> is the mass of the molecule and <span class="math notranslate nohighlight">\(k_{B}\)</span> is the Boltzmann
constant.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mode (most probable value) <span class="math notranslate nohighlight">\(v_m\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((2kT/m)^{1/2}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Mean <span class="math notranslate nohighlight">\(&lt;v&gt;\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((8kT/\pi m)^{1/2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Median</p></td>
<td><p><span class="math notranslate nohighlight">\(v_{median}= 1.098\cdot v_m\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>RMS-velocity <span class="math notranslate nohighlight">\(v_{rms}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((3kT/m)^{1/2}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="full-width-at-half-maximum">
<h2>Full Width at Half Maximum<a class="headerlink" href="#full-width-at-half-maximum" title="Permalink to this headline">¶</a></h2>
<p>Another way to characterize the spread of the data is to compute the
Full Width at Half Maximum (FWHM). Given a
distribution P(x), it can be computed as the difference between the two
values of x at which P(x) is equal to half of its maximum. For a
gaussian distribution the relation between FWHM and the standard
deviation is:</p>
<p><span class="math notranslate nohighlight">\(\mbox{FWHM} = 2\sqrt{2\ln2}\sigma \sim 2.355\sigma\)</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Gaussian mu = 0, sigma = 1</span>
<span class="nb">max</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">halfMax</span> <span class="o">=</span> <span class="nb">max</span><span class="o">/</span><span class="mf">2.</span>

<span class="c1"># Constant array at helfMax</span>
<span class="n">f</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">halfMax</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.001</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pdf(x)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s1">&#39;k&#39;</span><span class="p">)</span> <span class="c1"># Gaussian</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span><span class="s1">&#39;k&#39;</span><span class="p">)</span> <span class="c1"># Constant</span>

<span class="c1"># Find the intersection between the gaussian and the constant </span>
<span class="c1"># by finding where the sign of the difference of the two functions changes</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">)))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">f</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Half Maximum = &quot;</span><span class="p">,</span> <span class="n">halfMax</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Intersections: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;FWHM = &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/probability_25_0.png" src="_images/probability_25_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Half Maximum =  0.19947114020071635
Intersections:  [-1.178  1.177]
FWHM =  2.3550000000007865
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="higher-moments">
<h2>Higher Moments<a class="headerlink" href="#higher-moments" title="Permalink to this headline">¶</a></h2>
<p>The third moment is called <strong>skewness</strong> <span class="math notranslate nohighlight">\(\gamma_1\)</span> and it is often
defined as</p>
<p><span class="math notranslate nohighlight">\(\gamma_1=\mu'_3/\sigma^3=\frac{1}{\sigma^3}&lt;(x-&lt;x&gt;)^3&gt;=
\frac{1}{\sigma^3}(&lt;x^3&gt;-3&lt;x&gt;&lt;x^2&gt;+2&lt;x&gt;^3).\)</span></p>
<p>The quantity <span class="math notranslate nohighlight">\(\gamma_1\)</span> is
dimensionless and characterizes the skew. It is zero for symmetric
distributions, and positive(negative) for asymmetric distributions with
a tail to the right(left).</p>
<p>Another definition of skewness often used is the <em>Pearson’s skew</em> given
by <span class="math notranslate nohighlight">\((mean-mode)/\sigma\)</span>.</p>
<p>FIXME</p>
<a class="reference internal image-reference" href="_images/skewness.png"><img alt="_images/skewness.png" class="align-center" src="_images/skewness.png" style="width: 400px;" /></a>
<p>The <strong>kurtosis</strong> <span class="math notranslate nohighlight">\(\gamma_2\)</span> is defined as
<span class="math notranslate nohighlight">\(\gamma_{2}=\mu'_4/\sigma^4-3\)</span>, which is a (dimensionless) measure of
how the distribution behaves in the tails compared to its maximum. The
factor <span class="math notranslate nohighlight">\(-3\)</span> is subtracted to obtain a kurtosis of zero for a Gaussian
distribution. A positive <span class="math notranslate nohighlight">\(\gamma_{2}\)</span> means that the distribution has a
larger maximum and larger tails than a Gaussian distribution with the
same values for mean and variance</p>
<p>FIXME</p>
<a class="reference internal image-reference" href="_images/kurtosis.png"><img alt="_images/kurtosis.png" class="align-center" src="_images/kurtosis.png" style="width: 400px;" /></a>
<p>Here are some useful scipy functions to compute moments:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">mu</span><span class="p">,</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="c1">#print (X)</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">moment</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;-moment&quot;</span><span class="p">,</span> <span class="n">moment</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">moment</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 -moment 0.0
2 -moment 26.06396929926389
3 -moment 17.07878635180023
4 -moment 1982.3218870809642
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">skew</span>
<span class="n">skew</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># left tail</span>
<span class="n">Xleft</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">skew</span><span class="p">(</span><span class="n">Xleft</span><span class="p">))</span>

<span class="c1"># right tail</span>
<span class="n">Xright</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="n">skew</span><span class="p">(</span><span class="n">Xright</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>left -0.3818440590736698
right 0.337575590140799
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">kurtosis</span>
<span class="n">kurtosis</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.08194774145399863
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="useful-inequalities">
<h2>Useful Inequalities<a class="headerlink" href="#useful-inequalities" title="Permalink to this headline">¶</a></h2>
<p>Two useful inequalities can be used to estimate the upper limits
probabilities if the underlying distribution is not known. Both
inequalities are given without proof.<br />
<br />
Let <span class="math notranslate nohighlight">\(x\)</span> be a positive random variable. Then it holds:</p>
<p><span class="math notranslate nohighlight">\(P(x\geq a) \leq \frac{&lt;x&gt;}{a}.\)</span></p>
<p>This inequality provides an upper limit for the probability
of random events located in the tails of the distribution.</p>
<p>Let <span class="math notranslate nohighlight">\(x\)</span> be a positive random variable. Then it holds:</p>
<p><span class="math notranslate nohighlight">\(P(\,|~x~-&lt;x&gt;|\geq k)\leq \frac{\sigma^2}{k^2}\)</span></p>
<p>So for example the probability for a result to deviate for more than three standard
deviations from the expectation value, is always smaller than 1/9,
independent of the underlying probability distribution. The inequality
is true in general, but it gives quite a weak limit (for a Gaussian
distribution, the probability to lie outside three standard deviations
is about 0.0027, see next chapter). It is nevertheless useful for
qualitative considerations, if nothing at all is known about the
underlying distribution.</p>
</div>
<div class="section" id="more-than-one-dimension">
<h2>More than one dimension<a class="headerlink" href="#more-than-one-dimension" title="Permalink to this headline">¶</a></h2>
<p>When doing measurements we are often performing a sampling of a
multi-dimensional dimensional pdf. For example in the case of a 2D
pdf, the probability to observe <span class="math notranslate nohighlight">\(X_1\)</span> in <span class="math notranslate nohighlight">\([x_1, x_1+dx_1]\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> in
<span class="math notranslate nohighlight">\([x_2, x_2+dx_2]\)</span> is:</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
P(x_1&lt; X_1 &lt; x_1+dx_1, x_2 &lt; X_2 &lt; x_2+dx_2) &amp;=&amp; f(x_1,x_2)dx_1dx_2\\
P(a&lt; X_1 &lt; b, c &lt; X_2 &lt; d) &amp;=&amp; \int_a^b dx_1 \int_c^d dx_2 f(x_1,x_2)\\\end{aligned}\)</span></p>
<p>For multi-dimensional pdfs we can define the concept of marginal and
conditional pdf. To get the idea let’s take a 2D example:</p>
<ul>
<li><p><strong>Marginal</strong> pdf: it’s the pdf describing <span class="math notranslate nohighlight">\(x_1\)</span> independently of the
value of <span class="math notranslate nohighlight">\(x_2\)</span>; the 2D probability can be “marginalised / projected”
by integrating over one variable</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
f_1(x_1) &amp;=&amp; \int_{-\infty}^{+\infty}f(x_1,x_2)dx_2 \\
f_2(x_2) &amp;=&amp; \int_{-\infty}^{+\infty}f(x_1,x_2)dx_1 \\
\end{aligned}\)</span></p>
</li>
<li><p><strong>Conditional</strong> pdf: it’s the pdf of <span class="math notranslate nohighlight">\(x_2\)</span> for a fixed value of
<span class="math notranslate nohighlight">\(x_1\)</span>: <span class="math notranslate nohighlight">\(f(x_2|x_1)\)</span></p>
<p><span class="math notranslate nohighlight">\(\begin{equation}
P(a&lt;X_2&lt;b|X_1 = x1) = \int_a^bf(x_2|x_1)dx_2
\end{equation}\)</span></p>
</li>
</ul>
<p>In plain english: the marginal pdf ignores the values of the other
variable(s), the conditional pdf fix the value of the other variable(s).</p>
<p>FIXME Normalization Coditional probability</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">axes3d</span>

<span class="c1"># multivariate gaussian mu = 0; sigma = 1</span>
<span class="k">def</span> <span class="nf">gg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span><span class="n">y</span><span class="o">*</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># gaussian mu = 1; sigma </span>
<span class="c1"># def g(x, sigma):</span>
<span class="c1">#     return 1./math.sqrt(2*math.pi)/sigma * np.exp(-0.5*(x*x))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">gg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Z&#39;</span><span class="p">)</span>

<span class="c1"># Conditional distribution</span>
<span class="c1"># p(X=x, y=1)</span>
<span class="n">xline</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">yline</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span><span class="o">+</span><span class="mf">0.</span><span class="o">*</span><span class="n">y</span>
<span class="n">zline</span> <span class="o">=</span> <span class="n">gg</span><span class="p">(</span><span class="n">xline</span><span class="p">,</span><span class="n">yline</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">xline</span><span class="p">,</span> <span class="n">yline</span><span class="p">,</span> <span class="n">zline</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p(X=x,Y)&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1">#ax1 = plt.axes(projection=&#39;3d&#39;)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">)</span>
<span class="n">cset</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">)</span>
<span class="n">cset</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Z&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-14-6a35344075b7&gt;:36: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().
  ax2 = fig.gca(projection=&#39;3d&#39;)
</pre></div>
</div>
<img alt="_images/probability_32_1.png" src="_images/probability_32_1.png" />
<img alt="_images/probability_32_2.png" src="_images/probability_32_2.png" />
</div>
</div>
</div>
<div class="section" id="transformation-of-variables">
<h2>Transformation of variables<a class="headerlink" href="#transformation-of-variables" title="Permalink to this headline">¶</a></h2>
<p>Let’s consider a 2-dimensional event space as an example (easily
generalizable to the N-dimensional case) and call the two random
variables <span class="math notranslate nohighlight">\((x,y)\)</span>. How does the pdf <span class="math notranslate nohighlight">\(f(x,y)\)</span> transforms under a change
of variable to <span class="math notranslate nohighlight">\((X,Y)\)</span>? (see
Fig.<a class="reference external" href="#fig:transformation">1.10</a>{reference-type=”ref”
reference=”fig:transformation”})</p>
<p><img alt="[[fig:transformation]]{#fig:transformationlabel=&quot;fig:transformation&quot;} Transformation ofvariables." src="Section1Bilder/transformation" />{#fig:transformation
width=”60%”}</p>
<p>Consider a small interval <span class="math notranslate nohighlight">\(A\)</span> around a point <span class="math notranslate nohighlight">\((x,y)\)</span> that we will
transform to a small interval <span class="math notranslate nohighlight">\(B\)</span> around the point <span class="math notranslate nohighlight">\((X,Y)\)</span> such that
<span class="math notranslate nohighlight">\(P(A)=P(B)\)</span>:</p>
<p><span class="math notranslate nohighlight">\(P\left[(X,Y) \in B\right] = P\left[(x,y) \in A \right] = \int_A f(x,y) dx dy.\)</span></p>
<p>Assume the transformation</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
X &amp;=&amp; u_x(x,y)\\
Y &amp;=&amp; u_y(x,y)\end{aligned}\)</span></p>
<p>to be bijective so that the inverse exist
(together with the first derivative):</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
x &amp;=&amp; w_x(X,Y)\\
y &amp;=&amp; w_y(X,Y).\end{aligned}\)</span></p>
<p>Then</p>
<p><span class="math notranslate nohighlight">\(\begin{aligned}
P(A) &amp;=&amp; P(B)\\
\int_A f(x,y) dx dy &amp;=&amp; \int_B f(w_x(X,Y), w_y(X,Y)) |J| dX dY\end{aligned}\)</span></p>
<p>where J is the Jacobian determinant:</p>
<p><span class="math notranslate nohighlight">\(J = 
\left| \begin{array}{cc}
\frac{\partial w_x}{\partial x} \frac{\partial w_x}{\partial y}\\
\frac{\partial w_y}{\partial x} \frac{\partial w_y}{\partial y}
\end{array}\right|\)</span></p>
<p>So the p.d.f. in (X,Y) is the p.d.f. in (x,y) times the Jacobian:</p>
<p><span class="math notranslate nohighlight">\(g(X,Y) = f(x,y) |J| = f(w_x(X,Y), w_y(X,Y)) |J|\)</span></p>
</div>
<div class="section" id="covariance-and-correlation">
<h2>Covariance and Correlation<a class="headerlink" href="#covariance-and-correlation" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f(x_1,x_2) dx_1 dx_2\)</span> be the joint probability to observe
<span class="math notranslate nohighlight">\(x_1\in [x_1,x_1+dx_1]\)</span> and <span class="math notranslate nohighlight">\(x_2\in [x_2, x_2+dx_2]\)</span>. The two variables
<span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> are <strong>independent</strong> if and only if they fulfill the
following relation:</p>
<p><span class="math notranslate nohighlight">\(f(x_1,x_2)=f(x_1)\cdot f(x_2).\)</span></p>
<p>If this is the
case, the two variables are said to be <strong>uncorrelated</strong>. If the above
condition is not fulfilled, then the variables are dependent, i.e.
<strong>correlated</strong> (see the definition of correlation below).<br />
<br />
The <strong>covariance</strong> <span class="math notranslate nohighlight">\(cov(x_{1},x_{2})\)</span> between two variables is defined
as</p>
<p><span class="math notranslate nohighlight">\(cov(x_1,x_2)=&lt;(x_1-&lt;x_1&gt;)\cdot (x_2-&lt;x_2&gt;)&gt;=&lt;x_1x_2&gt;-&lt;x_1&gt;&lt;x_2&gt;.\)</span></p>
<p>If two variables are independent then <span class="math notranslate nohighlight">\(&lt;x_1x_2&gt;=&lt;x_1&gt;&lt;x_2&gt;\)</span> and so the
covariance is zero.<br />
Knowing the covariance between two variables we can write the general
expression of the variance of their sum:</p>
<p><span class="math notranslate nohighlight">\(V(x_1+x_2)=V(x_1)+V(x_2)+2\cdot cov(x_1,x_2).\)</span></p>
<p>Some more properties
of the covariance are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(cov(x,a) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(cov(x,x) = V(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(cov(x,y) = cov(y,x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(cov(ax,by) = ab~cov(x,y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(cov(x+a,y+b) = cov(x,y)\)</span> is translation invariant (shift origin)</p></li>
<li><p><span class="math notranslate nohighlight">\(cov(x,y)\)</span> has units !</p></li>
</ul>
<p>The <strong>correlation</strong> coefficient between <span class="math notranslate nohighlight">\(x_{1}\)</span> and <span class="math notranslate nohighlight">\(x_{2}\)</span> is defined
as:</p>
<p><span class="math notranslate nohighlight">\(\label{eq:rho}
 \rho_{x_1x_2} = \frac{cov(x_1,x_2)}{\sqrt{V(x_1)V(x_2)}}\)</span></p>
<p>The correlation coefficient is the covariance normalized by the square root
of the product of the variances, to get a value between +1 and -1.
Scatter plots for different correlation coefficients <span class="math notranslate nohighlight">\(\rho\)</span> are shown in
Fig. <a class="reference external" href="#figWikiCorr">1.11</a>{reference-type=”ref” reference=”figWikiCorr”}.
If two variables are independent, given that their covariance is zero,
also <span class="math notranslate nohighlight">\(\rho_{x_1x_2}=0\)</span>. The inverse is not necessarily true. This means
that <span class="math notranslate nohighlight">\(\rho_{x_1x_2}=0\)</span> can hold but <span class="math notranslate nohighlight">\(x_{1}\)</span> and <span class="math notranslate nohighlight">\(x_{2}\)</span> can nevertheless
be dependent, as illustrated by the examples in
Fig.<a class="reference external" href="#figWikiCorr">1.11</a>{reference-type=”ref” reference=”figWikiCorr”}:\</p>
<p><img alt="[[figWikiCorr]]{#figWikiCorr label=&quot;figWikiCorr&quot;} Some examples forcorrelation coefficients  [wiki]. Note in particular the secondraw where the correlation coefficient is not defined for the horizontalline because one of the variables has null variance; and the third raw,where the correlation coefficient is alwayszero." src="Section1Bilder/wikiCorr" />{#figWikiCorr width=”80%”}</p>
<p>Given a sample of size <span class="math notranslate nohighlight">\(n\)</span>
(<span class="math notranslate nohighlight">\((x_{1},y_{1}),(x_{2},y_{2}),\ldots, (x_{n},y_{n})\)</span>), the <em>sample
covariance</em> or empirical covariance <span class="math notranslate nohighlight">\(s_{xy}\)</span>, which is the best estimate
for the (true) covariance <span class="math notranslate nohighlight">\(s_{xy}\)</span> is given by:</p>
<p><span class="math notranslate nohighlight">\(s_{xy}=\frac{1}{n-1}\sum_i(x_i-\bar{x})(y_i-\bar{y}).\)</span></p>
<p>and the
empirical correlation <span class="math notranslate nohighlight">\(r_{xy}\)</span> also called
Pearson-correlation-coefficient gives the best estimate for the (true)
correlation coefficient <span class="math notranslate nohighlight">\(\rho_{xy}\)</span>:</p>
<p><span class="math notranslate nohighlight">\(r_{xy}=\frac{s_{xy}}{\sqrt{s_x} \sqrt{s_y}}.\)</span></p>
<p>Here, the standard
deviations (already familiar from
Eq. <a class="reference external" href="#samplevariance">[samplevariance]</a>{reference-type=”ref”
reference=”samplevariance”}) are labeled with <span class="math notranslate nohighlight">\(s_{x}\)</span> and <span class="math notranslate nohighlight">\(s_{y}\)</span>,
respectively.<br />
<br />
A word of caution: “correlation is not causation”. The fact that one can
find a correlation between two observables does not necessarily means
that there is a causality relation between the two; see two examples in
Figs. <a class="reference external" href="#fig:corrCausation1">1.12</a>{reference-type=”ref”
reference=”fig:corrCausation1”} and
<a class="reference external" href="#fig:corrCausation2">1.13</a>{reference-type=”ref”
reference=”fig:corrCausation2”}.</p>
<p><img alt="[[fig:corrCausation1]]{#fig:corrCausation1label=&quot;fig:corrCausation1&quot;}Correlation does not imply causation!&quot;Chocolate consumption, cognitive function, and Nobel laureates.&quot;N.Engl. J. Med. 2012 Oct 18; 367(16):1562-4." src="Section1Bilder/chocoNobel" />{#fig:corrCausation1 width=”50%”}</p>
<p><img alt="[[fig:corrCausation2]]{#fig:corrCausation2label=&quot;fig:corrCausation2&quot;}Correlation does not imply causation! Dataform the U.S. Department ofAgriculture." src="Section1Bilder/Lemongraph" />{#fig:corrCausation2
width=”50%”}</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Most of the material of this section is beautifully exposed in:</p>
<ul class="simple">
<li><p>R. Feynman [&#64;Feynman], “Feynamn lectures on physics”: Ch.6</p></li>
<li><p>W. Metzger [&#64;Metzger], “Statistical Methods in Data Analysis”: Ch.2</p></li>
<li><p>L. Lyons [&#64;Lyons], “Statistics for Nuclear and Particle Physicist”:
Ch. 2</p></li>
</ul>
<span class="target" id="id2"></span></div>
<div class="section" id="problems">
<h2>Problems<a class="headerlink" href="#problems" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Take as starting point the example in the Bayes’ theorem section and suppose that a swan is tested positive twice in two subsequent tests. What would then be the probability that the swan is actually infected by the influenza virus.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="probabilityDistributions.html" title="next page">Probability Distributions</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mauro Donega<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>