
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parameter Estimation - Least Squares {#sec:chi2} &#8212; Statistical Methods and Data Analysis Techniques</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hypotheses Testing {#ChapterHypothesisTesting}" href="hypothesisTesting.html" />
    <link rel="prev" title="Parameter Estimation - Likelihood {#ChapterParameterEstimations}" href="likelihood.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Methods and Data Analysis Techniques</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probabilityDistributions.html">
   Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="errors.html">
   Measurements uncertainties {#ch:errors}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="monteCarlo.html">
   Monte Carlo methods {#sec:MC}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood.html">
   Parameter Estimation - Likelihood  {#ChapterParameterEstimations}
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hypothesisTesting.html">
   Hypotheses Testing {#ChapterHypothesisTesting}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="confidenceIntervals.html">
   Confidence Intervals {#ChapterConfidenceLimits}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mva.html">
   Multivariate Analysis Methods {#ChapterMVA}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unfolding.html">
   Unfolding {#ch:Unfolding}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/leastSquares.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-least-squares-method">
     The Least Squares Method
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connection-to-the-likelihood-function">
       Connection to the Likelihood Function
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-a-straight-line">
     Fitting a Straight Line
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#considering-the-systematic-errors">
       Considering the systematic Errors
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#straight-line-fit-with-errors-on-both-variables">
       Straight Line Fit with Errors on both Variables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-notation-and-the-uncertainty-on-the-fitted-parameters-sec-matrixnotation">
     Matrix Notation and the uncertainty on the fitted parameters {#sec:matrixNotation}
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-let-s-go-back-to-the-fit-of-a-straight-line-of-the-form-f-x-mx-b-to-n-data-points-which-have-independent-and-common-errors-such-that-bf-v-sigma-2-i-i-e-v-ij-sigma-2-delta-ij-in-matrix-notation-we-then-get-begin-array-cc-f-1-b-mx-1-f-2-b-mx-2-vdots-vdots-f-n-b-mx-n-end-array-qquad-bf-c-left-begin-array-cc-1-x-1-1-x-2-vdots-vdots-1-x-n-end-array-right-bf-hat-a-sigma-2-bf-c-tc-1-frac-1-sigma-2-bf-c-ty-which-can-be-explicitly-written-as-label-matrix-ls-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right">
   <strong>
    Example
   </strong>
   Let’s go back to the fit of a straight line of the form
   <span class="math notranslate nohighlight">
    \(f(x) = mx + b\)
   </span>
   to
   <span class="math notranslate nohighlight">
    \(N\)
   </span>
   data points, which have independent and common
errors, such that
   <span class="math notranslate nohighlight">
    \({\bf V} = \sigma^{2} I\)
   </span>
   , i.e.
   <span class="math notranslate nohighlight">
    \(V_{ij} = \sigma^{2} \delta_{ij}\)
   </span>
   . In matrix notation we then get:
$
   <span class="math notranslate nohighlight">
    \(\begin{array}{cc}
f_1 &amp; = b+ mx_1 \\
f_2 &amp; = b+ mx_2 \\
\vdots &amp; \vdots \\
f_N &amp; = b+ mx_N  \\
\end{array} 
\qquad
{\bf C} = \left( \begin{array}{cc}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_N \\
\end{array} \right)\)
   </span>
   <span class="math notranslate nohighlight">
    \(
\)
   </span>
   <span class="math notranslate nohighlight">
    \({\bf \hat{a}} = \sigma^2({\bf C^TC})^{-1}\frac{1}{\sigma^2}{\bf C^Ty}\)
   </span>
   <span class="math notranslate nohighlight">
    \(
which can be explicitly written as: \)
   </span>
   $\label{matrix_ls}
{\bf \hat{a}}=\left(\begin{array}{c}
\hat{b} \
\hat{m}
\end{array} \right)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#left-begin-array-cc-sum-i-1-sum-i-x-i-sum-i-x-i-sum-i-x-i-2-end-array-right-1-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-the-inversion-of-the-2-times-2-matrix-gives-frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-which-finally-leads-to-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right">
   \left(\begin{array}{cc}
\sum_i 1 &amp; \sum_i x_i \
\sum_i x_i &amp; \sum_i x_i^2 \
\end{array} \right)^{-1}
\left(\begin{array}{c}
\sum_i y_i \
\sum_i x_iy_i
\end{array} \right)$
   <span class="math notranslate nohighlight">
    \( The inversion of the \)
   </span>
   2 \times 2
   <span class="math notranslate nohighlight">
    \(-matrix gives:
\)
   </span>
   <span class="math notranslate nohighlight">
    \(\frac{1}{N(\overline{x^2}-\bar{x}^2)}
\left(\begin{array}{cc}
\overline{x^2}&amp; -\bar{x} \\
-\bar{x} &amp; 1 \\
\end{array} \right)\)
   </span>
   <span class="math notranslate nohighlight">
    \( which finally leads to:
\)
   </span>
   ${\bf \hat{a}}=\left(\begin{array}{c}
\hat{b} \
\hat{m}
\end{array} \right)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-which-corresponds-to-the-expressions-for-hat-m-and-hat-b-which-we-extracted-in-ls-slope-ls-slope-reference-type-ref-reference-ls-slope-and-ls-intercept-ls-intercept-reference-type-ref-reference-ls-intercept-the-variance-for-the-estimator-hat-bf-a-is-bf-v-bf-hat-a-left-begin-array-cc-v-b-cov-b-m-cov-b-m-v-m-end-array-right">
   \frac{1}{N(\overline{x^2}-\bar{x}^2)}
\left(\begin{array}{cc}
\overline{x^2}&amp; -\bar{x} \
-\bar{x} &amp; 1 \
\end{array} \right)
\left(\begin{array}{c}
\sum_i y_i \
\sum_i x_iy_i
\end{array} \right).$
   <span class="math notranslate nohighlight">
    \( Which corresponds to the expressions for
\)
   </span>
   \hat{m}
   <span class="math notranslate nohighlight">
    \( and \)
   </span>
   \hat{b}
   <span class="math notranslate nohighlight">
    \( which we extracted
in [\[ls\_slope\]](#ls_slope){reference-type="ref" reference="ls_slope"}
and [\[ls\_intercept\]](#ls_intercept){reference-type="ref"
reference="ls_intercept"}. The variance for the estimator
\)
   </span>
   \hat{ {\bf a}}
   <span class="math notranslate nohighlight">
    \( is: \)
   </span>
   ${\bf V}({\bf\hat{a}})=\left(\begin{array}{cc}
V(b)&amp; cov(b,m) \
cov(b,m) &amp; V(m) \
\end{array} \right)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binned-chi-2-fit-sec-binnedchi2">
     Binned
     <span class="math notranslate nohighlight">
      \(\chi^{2}\)
     </span>
     fit {#sec:binnedchi2}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-of-the-chi-2-to-test-the-goodness-of-fit-sec-chi2goodnessoffit">
     Use of the
     <span class="math notranslate nohighlight">
      \(\chi^2\)
     </span>
     to test the goodness of fit {#sec:chi2GoodnessOfFit}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="parameter-estimation-least-squares-sec-chi2">
<h1>Parameter Estimation - Least Squares {#sec:chi2}<a class="headerlink" href="#parameter-estimation-least-squares-sec-chi2" title="Permalink to this headline">¶</a></h1>
<p>Together with the maximum likelihood method, the method of least squares
(LS or LSQ) is very often used in parameters estimation. In this chapter
we will give a description of the method together with a few examples
and show its relation with the ML method.\</p>
<div class="section" id="the-least-squares-method">
<h2>The Least Squares Method<a class="headerlink" href="#the-least-squares-method" title="Permalink to this headline">¶</a></h2>
<p>Let’s take a set of independent Gaussian random variables <span class="math notranslate nohighlight">\(y_i\)</span>, with
<span class="math notranslate nohighlight">\(i=1,\ldots,N\)</span> and let’s assume that each <span class="math notranslate nohighlight">\(y_i\)</span> is distributed around an
unknown mean <span class="math notranslate nohighlight">\(\mu_i\)</span> with variance <span class="math notranslate nohighlight">\(\sigma^2_i\)</span>, where the mean is
predicted by a function <span class="math notranslate nohighlight">\(f(x_i;a)\)</span> (see
Fig. <a class="reference external" href="#fig:chi2Sketch">1.1</a>{reference-type=”ref”
reference=”fig:chi2Sketch”}). In the typical application the <span class="math notranslate nohighlight">\(y_i\)</span> are
the (independent) measurements with uncertainty <span class="math notranslate nohighlight">\(\sigma_i\)</span> and
<span class="math notranslate nohighlight">\(f(x_i;a)\)</span> is the functional form of the “model” for which you are
interested in estimating the value of some parameters (in this case
<span class="math notranslate nohighlight">\(a\)</span>).</p>
<p><img alt="Sketch to illustrate the notation.[[fig:chi2Sketch]]{#fig:chi2Sketchlabel=&quot;fig:chi2Sketch&quot;}" src="Section7Bilder/chi2Sketch.pdf" />{#fig:chi2Sketch
width=”60%”}\</p>
<p>If the data <span class="math notranslate nohighlight">\(\{y_i\}\)</span> are Gaussian distributed around the mean
<span class="math notranslate nohighlight">\(f(x_i;a)\)</span> then the sum: $<span class="math notranslate nohighlight">\(\label{eq:ChisquareEquation}
\chi^{2} = \sum_{i=1}^{N}\left(\frac{y_{i} -f(x_{i};a)}{\sigma_{i}} \right)^{2}\)</span><span class="math notranslate nohighlight">\(
obeys a \)</span>\chi^{2}<span class="math notranslate nohighlight">\(-distribution with \)</span>(N-1)<span class="math notranslate nohighlight">\( degrees of freedom (the
number of measurements, minus the number of fitted parameters). In the
general case of \)</span>p<span class="math notranslate nohighlight">\(-parameters to be fitted
(\)</span>f(x_i;a)\to f(x_i;\vec{a})<span class="math notranslate nohighlight">\( ) the number of degrees of freedom will be
\)</span>N-p<span class="math notranslate nohighlight">\(.\
To find the best estimate for the parameter \)</span>a<span class="math notranslate nohighlight">\( we proceed in a way
similar to what we have done in the ML-method: we look for the value of
the parameter \)</span>a<span class="math notranslate nohighlight">\( which minimize the \)</span>\chi^2<span class="math notranslate nohighlight">\(. If you interpret the
\)</span>\chi^2<span class="math notranslate nohighlight">\( as a distance, the LS method corresponds to minimize the
distance between the measured data and the considered model. This boils
down to calculating the minimum of the \)</span>\chi^2<span class="math notranslate nohighlight">\( as:
\)</span><span class="math notranslate nohighlight">\(\frac{d\chi^2}{da} = 2 \sum_{i=1}^N \frac{d}{da}f(x_i;a) \cdot \frac{y_i - f(x_i ; a)}{\sigma_i^2} = 0.\)</span><span class="math notranslate nohighlight">\(
In case of \)</span>p<span class="math notranslate nohighlight">\( parameters \)</span>a_1,\ldots,a_p<span class="math notranslate nohighlight">\( and \)</span>f(x;\vec{a})<span class="math notranslate nohighlight">\(, the idea
is the same, just the minimization will have to be performed
simultaneously in \)</span>p$ dimensions.</p>
<div class="section" id="connection-to-the-likelihood-function">
<h3>Connection to the Likelihood Function<a class="headerlink" href="#connection-to-the-likelihood-function" title="Permalink to this headline">¶</a></h3>
<p>The simplest way to see the relation between the LS and the ML methods
is to take a set of data <span class="math notranslate nohighlight">\((x_{i},y_{i})\)</span> for which the <span class="math notranslate nohighlight">\(x_{i}\)</span> are known
precisely and the <span class="math notranslate nohighlight">\(y_{i}\)</span> are known with uncertainties <span class="math notranslate nohighlight">\(\sigma_{i}\)</span>.
Under the assumption that the <span class="math notranslate nohighlight">\(y_{i}\)</span> are Gaussian distributed (for
instance when coming from the CLT), the probability to observe <span class="math notranslate nohighlight">\(y_i\)</span>
given the prediction <span class="math notranslate nohighlight">\(f(x_i;a)\)</span> is:
$<span class="math notranslate nohighlight">\(p(y_i|a)=\frac{1}{\sigma_i\sqrt{2\pi}}e^{-(y_i-f(x_i|a))^2/2\sigma_i^2}\)</span><span class="math notranslate nohighlight">\(
From this we can build the likelihood function for the complete data set
as: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
L(a,\vec{y})&amp;=&amp;\prod_ip(y_i|a) \\
\ln L(a,\vec{y})&amp;=&amp;-\frac{1}{2}\sum_{i} \left(\frac{y_i-f(x_i|a)}{\sigma_i}\right)^2
-\sum_{i} \ln\sigma_i\sqrt{2\pi},\end{aligned}\)</span><span class="math notranslate nohighlight">\( where only the first
term depends on \)</span>a<span class="math notranslate nohighlight">\(. To minimize the negative log-likelihood as a
function of the parameter \)</span>a<span class="math notranslate nohighlight">\( we will have to minimize:
\)</span><span class="math notranslate nohighlight">\(\chi^2=\sum_{i} \left(\frac{y_i-f(x_i|a)}{\sigma_i}\right)^2\)</span><span class="math notranslate nohighlight">\( which
corresponds to the \)</span>\chi^2$ procedure shown in the previous section.
(Eq. <a class="reference external" href="#eq:ChisquareEquation">[eq:ChisquareEquation]</a>{reference-type=”ref”
reference=”eq:ChisquareEquation”}).</p>
</div>
</div>
<div class="section" id="fitting-a-straight-line">
<h2>Fitting a Straight Line<a class="headerlink" href="#fitting-a-straight-line" title="Permalink to this headline">¶</a></h2>
<p>As a first example of the application of the LS method, we take a set of
<span class="math notranslate nohighlight">\(N\)</span> independent measurements <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> where we assume that the model
is linear, and in particular that <span class="math notranslate nohighlight">\(f(x) = mx\)</span> (i.e. a straight line
passing through the origin). The quantity which has to be minimized is
then: $<span class="math notranslate nohighlight">\(\label{}
\chi^2=\sum_i\left(\frac{y_i-mx_i}{\sigma_i}\right)^2\)</span><span class="math notranslate nohighlight">\( We furthermore
assume that all the uncertainties are the same:
\)</span>\sigma_{i} = \sigma , , \forall i<span class="math notranslate nohighlight">\(. Differentiating with respect to
\)</span>m<span class="math notranslate nohighlight">\( and equating to zero to obtain the best estimator \)</span>\hat{m}<span class="math notranslate nohighlight">\(, we
have: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\frac{\partial \chi^2}{\partial m}&amp;=&amp;-\frac{2}{\sigma^2}\sum_i(x_iy_i-mx_i^2)\\
\sum_i(x_iy_i-mx_i^2) &amp;=&amp;0 \\
\sum_i x_iy_i&amp;=&amp;m\sum_i x_i^2 \\
\hat{m}&amp;=&amp;\sum_i\frac{x_i y_i}{N\overline{x^2}}=\frac{\overline{xy}}{\overline{x^2}} \qquad \left(\sum x_i^2 = N \overline{x^2}\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The variance of \)</span>\hat{m}<span class="math notranslate nohighlight">\( can be determined by error propagation to be:
\)</span><span class="math notranslate nohighlight">\(V(\hat{m})=\sum_i\left(\frac{x_i}{N\overline{x^2}}\right)^2\sigma^2=\frac{\sigma^2}{N\overline{x^2}}.\)</span><span class="math notranslate nohighlight">\(\
When the straight line does not pass through the origin,
\)</span>f(x_{i};m,b) = mx_{i} + b<span class="math notranslate nohighlight">\( the solution of the LS method is:
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\hat{m} &amp;=&amp; \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\bar{x}^2} \label{ls_slope}\\
\hat{b} &amp;=&amp; \bar{y}-\hat{m}\bar{x} \label{ls_intercept}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The variances are given by: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\sigma_m^2 &amp;=&amp; V(\hat{m})=\frac{\sigma^2}{N(\overline{x^2}-\bar{x}^2)} \\
\sigma_b^2 &amp;=&amp; V(\hat{b})=\frac{\sigma^2\overline{x^2}}{N(\overline{x^2}-\bar{x}^2)} \\
cov(\hat{m},\hat{b}) &amp;=&amp; -\frac{\sigma^2\bar{x}}{N(\overline{x^2}-\bar{x}^2)} \\\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The \)</span>\chi^{2}<span class="math notranslate nohighlight">\( for the best fit is:
\)</span><span class="math notranslate nohighlight">\(\chi^2 =\frac{V(y)}{\sigma^2}(1-\rho^2(x,y))\)</span><span class="math notranslate nohighlight">\( Note that \)</span>V(y)<span class="math notranslate nohighlight">\( is not
the same as \)</span>\sigma^{2}<span class="math notranslate nohighlight">\(! \)</span>V(y) = \bar{y^{2}} - \bar{y}^{2}<span class="math notranslate nohighlight">\( is the
variance of the whole data sample, whereas \)</span>\sigma<span class="math notranslate nohighlight">\( describes the
standard deviation of a single measurement around its true value (here
we assumed \)</span>\sigma_i = \sigma ; \forall i<span class="math notranslate nohighlight">\().\
If the errors \)</span>\sigma_{i}<span class="math notranslate nohighlight">\( are not all the same, we have to minimize the
following expression: \)</span><span class="math notranslate nohighlight">\(\sum_i\frac{(y_i-mx_i-b)^2}{\sigma_i^2}.\)</span><span class="math notranslate nohighlight">\( The
solution of this minimization can again be obtained by the equations
given above, if the two means \)</span>\bar{x}<span class="math notranslate nohighlight">\( and \)</span>\bar{y}<span class="math notranslate nohighlight">\( are replaced by
the weighted means. Furthermore the normalization is no longer given
simply given by \)</span>N<span class="math notranslate nohighlight">\( but it is given by \)</span>\sum_{i} 1/\sigma_{i}^{2}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\frac{\sum_i y_i}{N}\to \frac{\sum_iy_i/\sigma_i^2}{\sum_i 1/\sigma_i^2}\)</span><span class="math notranslate nohighlight">\(
And the quantity \)</span>\sigma^{2}<span class="math notranslate nohighlight">\( has to be replaced in the expressions for
the variance by \)</span><span class="math notranslate nohighlight">\(\sigma^2\to \frac{N}{\sum_i 1/\sigma_i^2}.\)</span>$</p>
<p>For the sake of completeness we give here the full solution in a form
which is particularly useful when plugged into a program. We define:
$<span class="math notranslate nohighlight">\(\begin{aligned}
S_1&amp;=&amp;\sum_i w_i=\sum_i 1/\sigma_i^2 \\
S_x &amp;=&amp;\sum w_i x_i \quad S_y=\sum_i w_iy_i \\
S_{xx}&amp;=&amp;\sum_iw_ix_i^2 \quad S_{xy}=\sum_iw_ix_iy_i \quad S_{yy}=\sum_iw_iy_i^2\end{aligned}\)</span><span class="math notranslate nohighlight">\(
With this notation we can rewrite the linear system of equation as
\)</span><span class="math notranslate nohighlight">\(\left(\begin{array}{cc}
 S_1  &amp; S_x \\
S_x &amp; S_{xx} \\
\end{array}\right)\cdot {b \choose m} ={S_y \choose S_{xy}}.\)</span><span class="math notranslate nohighlight">\( With the
expression of the determinant \)</span>D<span class="math notranslate nohighlight">\(: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
D&amp;=&amp;S_1S_{xx}-S_x^2 \\\end{aligned}\)</span><span class="math notranslate nohighlight">\( we can compute: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\hat{b}&amp;=&amp;(S_{xx}S_y-S_xS_{xy})/D \\
\hat{m}&amp;=&amp;(-S_{x}S_y+S_1S_{xy})/D\end{aligned}\)</span><span class="math notranslate nohighlight">\(\
Once the slope \)</span>\hat{m}<span class="math notranslate nohighlight">\( and the intercept \)</span>\hat{b}<span class="math notranslate nohighlight">\( are obtained, we
can compute the uncertainty for an arbitrary interpolated (or
extrapolated) point \)</span>y<span class="math notranslate nohighlight">\( for a given \)</span>x<span class="math notranslate nohighlight">\(. For such a given \)</span>x<span class="math notranslate nohighlight">\( the
predicted value for \)</span>y<span class="math notranslate nohighlight">\( is just \)</span>y = \hat{m} x + \hat{b}<span class="math notranslate nohighlight">\( and the error
for the interpolated value \)</span>y<span class="math notranslate nohighlight">\( is then: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
V(y)&amp;=&amp;V(\hat{m}x+\hat{b})=V(\hat{m}x)+V(\hat{b})+2\cdot cov(\hat{m}x,\hat{b})\\
V(y)&amp;=&amp;x^2V(\hat{m})+V(\hat{b})+2x\cdot cov(\hat{m},\hat{b}) \\
V(y)&amp;=&amp;\frac{\sigma^2(x-\bar{x})^2}{N(\overline{x^2}-\bar{x}^2)}+\sigma^2/N\end{aligned}\)</span>$
We will see in Ch. <a class="reference external" href="#ChapterMVA">[ChapterMVA]</a>{reference-type=”ref”
reference=”ChapterMVA”} how this will be used as a simple learning
algorithm.</p>
<div class="section" id="considering-the-systematic-errors">
<h3>Considering the systematic Errors<a class="headerlink" href="#considering-the-systematic-errors" title="Permalink to this headline">¶</a></h3>
<p>As an example we consider a straight line fit where all the measurements
<span class="math notranslate nohighlight">\(y_{i}\)</span> have a common statistic error <span class="math notranslate nohighlight">\(\sigma\)</span> and a common systematic
error <span class="math notranslate nohighlight">\(S\)</span>. We know from the discussion about systematic errors in
Ch. <a class="reference external" href="#ch:errors">[ch:errors]</a>{reference-type=”ref”
reference=”ch:errors”} that the covariance matrix <span class="math notranslate nohighlight">\(cov(y_{i},y_{j})\)</span> can
be written as <span class="math notranslate nohighlight">\(cov(y_{i},y_{j}) = \delta_{ij}\sigma^{2} +S^{2}\)</span>. The
estimators for the slope and the intercept, <span class="math notranslate nohighlight">\(\hat{m}\)</span> and <span class="math notranslate nohighlight">\(\hat{b}\)</span>,
respectively, are again given by
Eq. <a class="reference external" href="#ls_slope">[ls_slope]</a>{reference-type=”ref”
reference=”ls_slope”}
and <a class="reference external" href="#ls_intercept">[ls_intercept]</a>{reference-type=”ref”
reference=”ls_intercept”}. The complete formula for the variances reads
therefore as follows: $<span class="math notranslate nohighlight">\(\begin{aligned}
V(\hat{m})&amp;=&amp;\frac{1}{N^2(\overline{x^2}-\bar{x}^2)^2}\sum_{i,j}(x_i-\bar{x})(x_j-\bar{x})\cdot cov(y_i,y_j) \\
\label{summand}&amp;=&amp;\frac{1}{N^2(\overline{x^2}-\bar{x}^2)^2}\left(\sum_i(x_i-\bar{x})^2\sigma^2+
\sum_{i,j}(x_i-\bar{x})(x_j-\bar{x})S^2\right)\\
&amp;=&amp;\frac{1}{N^2(\overline{x^2}-\bar{x}^2)^2}\left(\sum_i(x_i-\bar{x})^2\sigma^2\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The second summand in Eq. [\[summand\]](#summand){reference-type=&quot;ref&quot;
reference=&quot;summand&quot;} vanishes, because \)</span>1/N \sum x_{i} = \bar{x}<span class="math notranslate nohighlight">\(.\
The variance for \)</span>\hat{b}<span class="math notranslate nohighlight">\( is:
\)</span><span class="math notranslate nohighlight">\(V(\hat{b})=\frac{1}{N^2(\overline{x^2}-\bar{x}^2)^2}\sum_{i,j}(\overline{x^2}-\bar{x}x_i)
(\overline{x^2}-\bar{x}x_j)\cdot cov(y_i,y_j)\)</span><span class="math notranslate nohighlight">\( The sum
\)</span>\sum_i (\overline{x^2}-\bar{x}x_i)=N(\overline{x^2}-\bar{x}^2)<span class="math notranslate nohighlight">\( does
not vanish in this equation, thus an additional term appears which is
just \)</span>S^{2}$; a common systematic error only influences the variance of
the intercept, but it does not change the variance of the slope, as we
would have naively expected.</p>
</div>
<div class="section" id="straight-line-fit-with-errors-on-both-variables">
<h3>Straight Line Fit with Errors on both Variables<a class="headerlink" href="#straight-line-fit-with-errors-on-both-variables" title="Permalink to this headline">¶</a></h3>
<p>Now we allow for both variables <span class="math notranslate nohighlight">\(x_{i}\)</span> and <span class="math notranslate nohighlight">\(y_{i}\)</span> to have errors
<span class="math notranslate nohighlight">\(\sigma_{x_{i}}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y_{i}}\)</span> (see
Fig. <a class="reference external" href="#fig:LSXY">1.3</a>{reference-type=”ref” reference=”fig:LSXY”}). This
means that the sum of the squared residuals of the error ellipsis of the
straight line has to be minimized, i.e.:
$<span class="math notranslate nohighlight">\(S(m,b)=\sum_i\frac{(y_i-mx_i-b)^2}{\sigma^2_{y_i}+m^2\sigma^2_{x_i}}\)</span>$\</p>
<p>[[fig:LSXY]]{#fig:LSXY label=”fig:LSXY”}</p>
<p><img alt="LS fit with uncertainty only on &quot;y&quot; (left) or both on &quot;x&quot; and &quot;y&quot;(right)." src="Section7Bilder/chi2UncertaintyY.pdf" />{#fig:LSXY
width=”45%”} <img alt="LS fit with uncertainty only on &quot;y&quot; (left) or both on &quot;x&quot;and &quot;y&quot; (right)." src="Section7Bilder/chi2UncertaintyXY.pdf" />{#fig:LSXY
width=”45%”}</p>
<p>As usual, the two equations <span class="math notranslate nohighlight">\(\partial S / \partial m = 0\)</span> and
<span class="math notranslate nohighlight">\(\partial S / \partial b = 0\)</span> have to be solved.<br />
The condition <span class="math notranslate nohighlight">\(\partial S / \partial b = 0\)</span> leads to
$<span class="math notranslate nohighlight">\(\hat{b}=\frac{\sum y_i/\kappa_i-\hat{m}\sum x_i/\kappa_i}{\sum 1/\kappa_i}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\kappa_i=\sigma^2_{y_i}+m^2\sigma^2_{x_i}<span class="math notranslate nohighlight">\(. For \)</span>\hat{m}<span class="math notranslate nohighlight">\(, if the
errors are all the same on \)</span>x<span class="math notranslate nohighlight">\( (i.e. \)</span>\sigma_{x_{i}} = \sigma_{x}<span class="math notranslate nohighlight">\() and
on \)</span>y<span class="math notranslate nohighlight">\( (i.e. \)</span>\sigma_{y_{i}} = \sigma_{y}<span class="math notranslate nohighlight">\(), then the solution for the
straight line fit is given by: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\hat{m}&amp;=&amp;\frac{\sigma_x}{\sigma_y}(A\pm\sqrt{A^2+1}) \\
A&amp;=&amp;\frac{\sigma_x^2V(y)-\sigma_y^2V(x)}{2\sigma_x\sigma_y\cdot cov(x,y)} \\
\bar{y}&amp;=&amp;\hat{m}\bar{x}+\hat{b}\end{aligned}\)</span>$ Other particular cases
can be found making assumption on the uncertainties, but to solve the
generic problem numerical techniques are typically used.</p>
</div>
</div>
<div class="section" id="matrix-notation-and-the-uncertainty-on-the-fitted-parameters-sec-matrixnotation">
<h2>Matrix Notation and the uncertainty on the fitted parameters {#sec:matrixNotation}<a class="headerlink" href="#matrix-notation-and-the-uncertainty-on-the-fitted-parameters-sec-matrixnotation" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\({\bf a}\)</span> be a vector of <span class="math notranslate nohighlight">\(n\)</span> parameters <span class="math notranslate nohighlight">\(\{a_{i}\}, i=1,\ldots,n\)</span>.
The <span class="math notranslate nohighlight">\(N\)</span> measured data points can be represented by a vector
<span class="math notranslate nohighlight">\({\bf y} = \{y_{i}\},~ i=1,\ldots,N\)</span> and the function to be fitted as
<span class="math notranslate nohighlight">\(f(x_{i};{\bf a})\)</span>. The <span class="math notranslate nohighlight">\(\chi^2\)</span> expression in matrix form becomes:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\chi^2&amp;=&amp;\sum_i\sum_j[y_i-f(x_i;{\bf a})]V_{ij}^{-1}[y_j-f(x_j;{\bf a})]\\
      &amp;=&amp;({\bf y}-{\bf f})^T{\bf V}^{-1}({\bf y}-{\bf f})={\bf r^T}{\bf V}^{-1}{\bf r}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Here, \)</span>{\bf r} = {\bf y} - {\bf f}<span class="math notranslate nohighlight">\( is the vector of residuals and
\)</span>\bf{V}<span class="math notranslate nohighlight">\( is the covariance matrix. By differentiating \)</span>\chi^{2}<span class="math notranslate nohighlight">\( w.r.t
each \)</span>a_{i}<span class="math notranslate nohighlight">\( and equating each of them to zero yields \)</span>n<span class="math notranslate nohighlight">\( equations,
which have to be solved simultaneously in order to get the estimator
\)</span>{\bf \hat{a}}<span class="math notranslate nohighlight">\(. Many mathematical packages (e.g. MatLab, Octave,
etc\...) offer a way to solve matrix problems; in slang moving from the
single components equations to the matrix form is called
&quot;vectorization&quot;. Because they use highly optimized algorithms, it is
always preferable to work with the vectorized version of the problem.\
Let \)</span>f(x;{\bf a})<span class="math notranslate nohighlight">\( be a linear function of the parameters \)</span>{\bf a}<span class="math notranslate nohighlight">\(:
\)</span>f(x;{\bf a})=\sum_r c_r(x) a_r<span class="math notranslate nohighlight">\(. Where the linearity is on the
\)</span>{\bf a}<span class="math notranslate nohighlight">\(, the \)</span>c_r(x)<span class="math notranslate nohighlight">\( can be any function of \)</span>x<span class="math notranslate nohighlight">\(. Written in matrix
form, it reads:
\)</span><span class="math notranslate nohighlight">\(\chi^2=({\bf y}-{\bf Ca})^T{\bf V}^{-1}({\bf y}-{\bf Ca}).\)</span><span class="math notranslate nohighlight">\( If we
have \)</span>N<span class="math notranslate nohighlight">\( data points and \)</span>n<span class="math notranslate nohighlight">\( coefficients \)</span>(n \le N)<span class="math notranslate nohighlight">\(, then \)</span>{\bf y}<span class="math notranslate nohighlight">\(
and \)</span>{\bf a}<span class="math notranslate nohighlight">\( are column vectors with dimension \)</span>N<span class="math notranslate nohighlight">\( and \)</span>n<span class="math notranslate nohighlight">\(,
respectively. The covariance matrix \)</span>{\bf V}<span class="math notranslate nohighlight">\( has dimension \)</span>N \times N<span class="math notranslate nohighlight">\(
and the matrix \)</span>{\bf C}<span class="math notranslate nohighlight">\( has dimensions \)</span>N \times n<span class="math notranslate nohighlight">\(.\
Minimizing the \)</span>\chi^2<span class="math notranslate nohighlight">\(, the equations
\)</span>\partial \chi^{2} / \partial {\bf a} = 0<span class="math notranslate nohighlight">\( are:
\)</span>$\partial \chi^{2} / \partial {\bf a} = -2 ( \bf{C}^T \bf{V}^{-1} {\bf \bf{y}} - \bf{C}^T \bf{V}^{-1}\bf{C} {\bf a}) = 0</p>
<p>The solution for the estimator <span class="math notranslate nohighlight">\(\hat{ {\bf a}}\)</span> is then:
$<span class="math notranslate nohighlight">\({\bf \hat{a}}=({\bf C^T}{\bf V}^{-1}{\bf C})^{-1}{\bf C^T}{\bf V}^{-1}{\bf y} := \bf{B}{\bf y}\)</span><span class="math notranslate nohighlight">\(
which means that the solution \)</span>{\bf a}<span class="math notranslate nohighlight">\( are linear functions of the
measurements \)</span>{\bf y}<span class="math notranslate nohighlight">\(.\
Using error propagation we can find the covariance matrix for the
\)</span>{\bf \hat{a}}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\bf{U} = \bf{B} \bf{V} \bf{B}^T = [{\bf C^TV}^{-1}{\bf C}]^{-1}\)</span>$</p>
</div>
</div>
<div class="section" id="example-let-s-go-back-to-the-fit-of-a-straight-line-of-the-form-f-x-mx-b-to-n-data-points-which-have-independent-and-common-errors-such-that-bf-v-sigma-2-i-i-e-v-ij-sigma-2-delta-ij-in-matrix-notation-we-then-get-begin-array-cc-f-1-b-mx-1-f-2-b-mx-2-vdots-vdots-f-n-b-mx-n-end-array-qquad-bf-c-left-begin-array-cc-1-x-1-1-x-2-vdots-vdots-1-x-n-end-array-right-bf-hat-a-sigma-2-bf-c-tc-1-frac-1-sigma-2-bf-c-ty-which-can-be-explicitly-written-as-label-matrix-ls-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right">
<h1><strong>Example</strong> Let’s go back to the fit of a straight line of the form
<span class="math notranslate nohighlight">\(f(x) = mx + b\)</span> to <span class="math notranslate nohighlight">\(N\)</span> data points, which have independent and common
errors, such that <span class="math notranslate nohighlight">\({\bf V} = \sigma^{2} I\)</span>, i.e.
<span class="math notranslate nohighlight">\(V_{ij} = \sigma^{2} \delta_{ij}\)</span>. In matrix notation we then get:
$<span class="math notranslate nohighlight">\(\begin{array}{cc}
f_1 &amp; = b+ mx_1 \\
f_2 &amp; = b+ mx_2 \\
\vdots &amp; \vdots \\
f_N &amp; = b+ mx_N  \\
\end{array} 
\qquad
{\bf C} = \left( \begin{array}{cc}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_N \\
\end{array} \right)\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\({\bf \hat{a}} = \sigma^2({\bf C^TC})^{-1}\frac{1}{\sigma^2}{\bf C^Ty}\)</span><span class="math notranslate nohighlight">\(
which can be explicitly written as: \)</span>$\label{matrix_ls}
{\bf \hat{a}}=\left(\begin{array}{c}
\hat{b} \
\hat{m}
\end{array} \right)<a class="headerlink" href="#example-let-s-go-back-to-the-fit-of-a-straight-line-of-the-form-f-x-mx-b-to-n-data-points-which-have-independent-and-common-errors-such-that-bf-v-sigma-2-i-i-e-v-ij-sigma-2-delta-ij-in-matrix-notation-we-then-get-begin-array-cc-f-1-b-mx-1-f-2-b-mx-2-vdots-vdots-f-n-b-mx-n-end-array-qquad-bf-c-left-begin-array-cc-1-x-1-1-x-2-vdots-vdots-1-x-n-end-array-right-bf-hat-a-sigma-2-bf-c-tc-1-frac-1-sigma-2-bf-c-ty-which-can-be-explicitly-written-as-label-matrix-ls-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="left-begin-array-cc-sum-i-1-sum-i-x-i-sum-i-x-i-sum-i-x-i-2-end-array-right-1-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-the-inversion-of-the-2-times-2-matrix-gives-frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-which-finally-leads-to-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right">
<h1>\left(\begin{array}{cc}
\sum_i 1 &amp; \sum_i x_i \
\sum_i x_i &amp; \sum_i x_i^2 \
\end{array} \right)^{-1}
\left(\begin{array}{c}
\sum_i y_i \
\sum_i x_iy_i
\end{array} \right)$<span class="math notranslate nohighlight">\( The inversion of the \)</span>2 \times 2<span class="math notranslate nohighlight">\(-matrix gives:
\)</span><span class="math notranslate nohighlight">\(\frac{1}{N(\overline{x^2}-\bar{x}^2)}
\left(\begin{array}{cc}
\overline{x^2}&amp; -\bar{x} \\
-\bar{x} &amp; 1 \\
\end{array} \right)\)</span><span class="math notranslate nohighlight">\( which finally leads to:
\)</span>${\bf \hat{a}}=\left(\begin{array}{c}
\hat{b} \
\hat{m}
\end{array} \right)<a class="headerlink" href="#left-begin-array-cc-sum-i-1-sum-i-x-i-sum-i-x-i-sum-i-x-i-2-end-array-right-1-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-the-inversion-of-the-2-times-2-matrix-gives-frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-which-finally-leads-to-bf-hat-a-left-begin-array-c-hat-b-hat-m-end-array-right" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-which-corresponds-to-the-expressions-for-hat-m-and-hat-b-which-we-extracted-in-ls-slope-ls-slope-reference-type-ref-reference-ls-slope-and-ls-intercept-ls-intercept-reference-type-ref-reference-ls-intercept-the-variance-for-the-estimator-hat-bf-a-is-bf-v-bf-hat-a-left-begin-array-cc-v-b-cov-b-m-cov-b-m-v-m-end-array-right">
<h1>\frac{1}{N(\overline{x^2}-\bar{x}^2)}
\left(\begin{array}{cc}
\overline{x^2}&amp; -\bar{x} \
-\bar{x} &amp; 1 \
\end{array} \right)
\left(\begin{array}{c}
\sum_i y_i \
\sum_i x_iy_i
\end{array} \right).$<span class="math notranslate nohighlight">\( Which corresponds to the expressions for
\)</span>\hat{m}<span class="math notranslate nohighlight">\( and \)</span>\hat{b}<span class="math notranslate nohighlight">\( which we extracted
in [\[ls\_slope\]](#ls_slope){reference-type=&quot;ref&quot; reference=&quot;ls_slope&quot;}
and [\[ls\_intercept\]](#ls_intercept){reference-type=&quot;ref&quot;
reference=&quot;ls_intercept&quot;}. The variance for the estimator
\)</span>\hat{ {\bf a}}<span class="math notranslate nohighlight">\( is: \)</span>${\bf V}({\bf\hat{a}})=\left(\begin{array}{cc}
V(b)&amp; cov(b,m) \
cov(b,m) &amp; V(m) \
\end{array} \right)<a class="headerlink" href="#frac-1-n-overline-x-2-bar-x-2-left-begin-array-cc-overline-x-2-bar-x-bar-x-1-end-array-right-left-begin-array-c-sum-i-y-i-sum-i-x-iy-i-end-array-right-which-corresponds-to-the-expressions-for-hat-m-and-hat-b-which-we-extracted-in-ls-slope-ls-slope-reference-type-ref-reference-ls-slope-and-ls-intercept-ls-intercept-reference-type-ref-reference-ls-intercept-the-variance-for-the-estimator-hat-bf-a-is-bf-v-bf-hat-a-left-begin-array-cc-v-b-cov-b-m-cov-b-m-v-m-end-array-right" title="Permalink to this headline">¶</a></h1>
<p>\frac{\sigma^2}{N(\overline{x^2}-\bar{x}^2)}\left(\begin{array}{cc}
\overline{x^2}&amp; -\bar{x} \
-\bar{x} &amp; 1 \
\end{array} \right)$<span class="math notranslate nohighlight">\(\
**Example** In a second example we fit the parabola
\)</span>f(x) = a_{0} + a_{1} x + a_{2} x^{2}<span class="math notranslate nohighlight">\( to \)</span>N<span class="math notranslate nohighlight">\( data points. Again we
assume the errors being independent and equal for all data points. The
matrix \)</span>{\bf C}<span class="math notranslate nohighlight">\( is now given by: \)</span><span class="math notranslate nohighlight">\({\bf C}=\left( \begin{array}{ccc}
1 &amp; x_1 &amp; x_1^2\\
1 &amp; x_2 &amp; x_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_N &amp; x_N^2 \\
\end{array} \right)\)</span><span class="math notranslate nohighlight">\( Going through the same steps as for the linear
case we obtain: \)</span><span class="math notranslate nohighlight">\({\bf \hat{a}}=
\left( \begin{array}{c}
\hat{a_0} \\
\hat{a_1} \\
\hat{a_2}\\
\end{array} \right)
=\left( \begin{array}{ccc}
\sum_i 1 &amp; \sum_i x_i &amp; \sum_i x_i^2 \\
\sum_i x_i &amp; \sum x_i^2 &amp; \sum_i x_i^3 \\
\sum_i x_i^2 &amp; \sum x_i^3 &amp; \sum_i x_i^4 \\
\end{array} \right)^{-1}
\left( \begin{array}{c}
\sum_i y_i  \\
\sum_i x_iy_i \\
\sum x_i^2y_i\\
\end{array} \right)\)</span><span class="math notranslate nohighlight">\( The generalization of this method to higher-order
polynomials follows the same pattern.\
Another way to write the (inverse) of the covariance matrix is:
\)</span><span class="math notranslate nohighlight">\((U^{-1})_{ij} = \frac{1}{2}\left[ \frac{\partial^2 \chi^2}{\partial a_i \partial a_j }  \right]_{\bf a = \hat{a}}\)</span><span class="math notranslate nohighlight">\(
which is the expression of the RCF bound in the case that the
measurements \)</span>{\bf y}<span class="math notranslate nohighlight">\( are Gaussian distributed, and where we use the
relation with the log likelihood \)</span>- \ln L = \chi^2/2<span class="math notranslate nohighlight">\( (a part from an
overall constant).\
Again in case of a function \)</span>f<span class="math notranslate nohighlight">\( linear in \)</span>{\bf a}<span class="math notranslate nohighlight">\(, the \)</span>\chi^{2}<span class="math notranslate nohighlight">\(
becomes quadratic in \)</span>{\bf a}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\chi^2({\bf a})=\chi^2({\bf \hat{a}})+\frac{1}{2}\sum_{i,j=1}\left[\frac{\partial^2\chi^2}{\partial a_i\partial a_j}\right]_{a=\hat{a}}(a_i-\hat{a}_i)(a_j-\hat{a}_j)\)</span><span class="math notranslate nohighlight">\(
Using the expression of the variance we just found, the equation above
corresponds to contours in the parameter space defined by
\)</span>\hat{a}<em>i\pm \hat{\sigma_i}<span class="math notranslate nohighlight">\( and therefore giving the \)</span>\pm 1 \sigma<span class="math notranslate nohighlight">\(
deviations from the estimators:
\)</span><span class="math notranslate nohighlight">\(\chi^2({\bf a}\pm \hat{\sigma})=\chi^2({\bf \hat{a}})+1=\chi^2_{min}+1.\)</span><span class="math notranslate nohighlight">\(
Hence the \)</span>\chi^{2}<span class="math notranslate nohighlight">\(-function is a parabola with a minimum at \)</span>\hat{a}<span class="math notranslate nohighlight">\(
and the errors \)</span>\sigma<span class="math notranslate nohighlight">\( for the estimators are determined by
\)</span>\chi^{2}</em>{min}+1<span class="math notranslate nohighlight">\(. In general, if the function \)</span>f$ is not linear in the
parameters, the contour will not be an ellipse, but it will still define
a confidence region reflecting the statistical uncertainty on the fitted
parameters. The precise construction of the confidence region will be
developed in the
Sec. <a class="reference external" href="#ChapterConfidenceLimits">[ChapterConfidenceLimits]</a>{reference-type=”ref”
reference=”ChapterConfidenceLimits”}. It is important to notice that the
confidence level of the region defined by the contour, depends on the
number of parameters fitted: 6.83% for one parameter, 39.4% for two,
19.9% for three, etc… (see again
Sec. <a class="reference external" href="#ChapterConfidenceLimits">[ChapterConfidenceLimits]</a>{reference-type=”ref”
reference=”ChapterConfidenceLimits”}).\</p>
<div class="section" id="binned-chi-2-fit-sec-binnedchi2">
<h2>Binned <span class="math notranslate nohighlight">\(\chi^{2}\)</span> fit {#sec:binnedchi2}<a class="headerlink" href="#binned-chi-2-fit-sec-binnedchi2" title="Permalink to this headline">¶</a></h2>
<p>In this section we will apply the LS method to binned data. In the case
of <span class="math notranslate nohighlight">\(f\)</span> being a probability distribution function (a p.d.f. instead of
any generic function), we can interpret the value of <span class="math notranslate nohighlight">\(f\)</span> integrated over
a given range (“bin”), as the expected number of events in that bin
<span class="math notranslate nohighlight">\(f_i = E[y_i]\)</span>: $<span class="math notranslate nohighlight">\(\label{eq:LS}
f_i({\bf a}) = n\int_{x_i^{min}}^{x_i^{max}} f(x;{\bf a}) dx = n p_i({\bf a})\)</span><span class="math notranslate nohighlight">\(
where \)</span>x_i^{min}<span class="math notranslate nohighlight">\( and \)</span>x_i^{max}<span class="math notranslate nohighlight">\( are the bin boundaries, the
\)</span>p_i({\bf a})<span class="math notranslate nohighlight">\( is the probability to have an event in the bin \)</span>i<span class="math notranslate nohighlight">\(, given
the parameters \)</span>{\bf a}<span class="math notranslate nohighlight">\( and n is the overall normalization.\
The fit proceeds as before minimizing the \)</span>\chi^2<span class="math notranslate nohighlight">\( w.r.t. the parameters
\)</span>{\bf a}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\chi^2(y_i|{\bf a})=\sum_i \frac{(y_i - f_i({\bf a}))^2}{\sigma_i^2}\)</span><span class="math notranslate nohighlight">\(
where here \)</span>\sigma_i<span class="math notranslate nohighlight">\( is the variance of the *expected* number of
entries in bin \)</span>i<span class="math notranslate nohighlight">\(. If the number of entries in bin \)</span>i<span class="math notranslate nohighlight">\( is small
compared to the total number of entries in the histogram then we can
assume that they are Poisson distributed and the variance is equal to
the mean \)</span>\sigma_i^2 = f_i({\bf a}) = n p_i({\bf a})<span class="math notranslate nohighlight">\(.\
Often, instead of using the variance of the expected number of entries
in bin \)</span>i<span class="math notranslate nohighlight">\(, the variance of the *observed* number of entries in bin \)</span>i<span class="math notranslate nohighlight">\(
is used, leading to:
\)</span><span class="math notranslate nohighlight">\(\chi^2(y_i|{\bf a})=\sum_i \frac{(y_i - f_i({\bf a}))^2}{y_i} .\)</span><span class="math notranslate nohighlight">\( This
new expression is called the **modified LS** method. Even if
computationally easier to implement (contraty to \)</span>\sigma_i^2<span class="math notranslate nohighlight">\( the
observed \)</span>y_i<span class="math notranslate nohighlight">\(, obviously, does not depend on \)</span>\bf{a}$), it brings in
the problem of the variance estimation for bins which are poorly
populated or have no entries at all. As a rule of thumb, try to have at
least 5 entries per bin. Two situations are rather typical: small
statistics in the tails of a distribution, or the whole histogram is
sparsely populated. In the first case, try to rebin, for the latter just
move to an unbinned ML fit to use the full information you have in your
data.\</p>
</div>
<div class="section" id="use-of-the-chi-2-to-test-the-goodness-of-fit-sec-chi2goodnessoffit">
<h2>Use of the <span class="math notranslate nohighlight">\(\chi^2\)</span> to test the goodness of fit {#sec:chi2GoodnessOfFit}<a class="headerlink" href="#use-of-the-chi-2-to-test-the-goodness-of-fit-sec-chi2goodnessoffit" title="Permalink to this headline">¶</a></h2>
<p>If the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> is large after minimization, then the function is
probably badly chosen (i.e. not correctly describing the data);
intuitively, the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> should be small if the function describes
the data. On the other hand, the <span class="math notranslate nohighlight">\(\chi^{2}\)</span> can be small if you have too
many degrees of freedom (fit n-points with a polynomial of degree n) or,
even with a bad model, when the uncertainties are overestimated
(<span class="math notranslate nohighlight">\(\sigma_i^2\)</span> sits at the denominator). You can always get very small
<span class="math notranslate nohighlight">\(\chi^{2}\)</span> if you assume large enough uncertainties! If the errors are
too large the whole method loses its predictive power.<br />
We have already encountered
in <a class="reference external" href="#SubSectionChi2">[SubSectionChi2]</a>{reference-type=”ref”
reference=”SubSectionChi2”} the <span class="math notranslate nohighlight">\(\chi^{2}\)</span>-distribution:
$<span class="math notranslate nohighlight">\(f(\chi^2;n)=\frac{2^{-n/2}}{\Gamma(n/2)}\chi^{n-2}e^{-\chi^2/2}.\)</span><span class="math notranslate nohighlight">\( The
distribution depends on \)</span>n<span class="math notranslate nohighlight">\(, the number of degrees of freedom, which is
the number of data points minus the number of parameters of the model.
Because the \)</span>\chi^{2}<span class="math notranslate nohighlight">\( distribution has expectation value \)</span>n<span class="math notranslate nohighlight">\( and
variance \)</span>2n<span class="math notranslate nohighlight">\(, we expect the \)</span>\chi^{2}<span class="math notranslate nohighlight">\( divided by the number of degrees
of freedom to be approximately one: \)</span>\chi^{2} / n \approx 1<span class="math notranslate nohighlight">\(. If the
\)</span>\chi^{2} / n<span class="math notranslate nohighlight">\( is much larger than one, the data are not properly
described by the model. Let's introduce the definition of \)</span>p-<span class="math notranslate nohighlight">\(value to
quantify the level of agreement of the model with data:
\)</span><span class="math notranslate nohighlight">\(p=\int_{\chi^2}^\infty f(x';n) dx' \label{eq:pvalue}\)</span><span class="math notranslate nohighlight">\( where \)</span>f(x’;n)<span class="math notranslate nohighlight">\(
is the \)</span>\chi^2<span class="math notranslate nohighlight">\( distribution for \)</span>n<span class="math notranslate nohighlight">\( degrees of freedom. Values can be
computed in `ROOT` using
`TMath::ChisquareQuantile(Double_t p, Double_t ndf)`.\
The \)</span>p-<span class="math notranslate nohighlight">\(value for a given \)</span>(\chi^2; n)<span class="math notranslate nohighlight">\( is a measurement of the
&quot;goodness of fit&quot;; when repeating the experiment several times it gives
the probability, under the hypothesis \)</span>f<span class="math notranslate nohighlight">\(, of obtaining a result as
incompatible with \)</span>f<span class="math notranslate nohighlight">\( or worse (i.e. \)</span>\chi^2<span class="math notranslate nohighlight">\( equal or larger) than the
one actually observed. The threshold on the \)</span>p-<span class="math notranslate nohighlight">\(value used to reject the
model is subjective; typical values used are of a few percent.
Fig. [1.4](#fig:chi2PDG){reference-type=&quot;ref&quot; reference=&quot;fig:chi2PDG&quot;}
maps the relation between the \)</span>\chi^2<span class="math notranslate nohighlight">\(, the number of degrees of freedom
and the \)</span>p-<span class="math notranslate nohighlight">\(value. In particular is shown the example where, for \)</span>n=4<span class="math notranslate nohighlight">\(,
a \)</span>\chi^2&gt;6$ will be observed in 20% of the cases.</p>
<p><img alt="[[fig:chi2PDG]]{#fig:chi2PDG label=&quot;fig:chi2PDG&quot;} One minus the cumulative distribution, , for n degrees offreedom. This gives the value for the  goodness-of-fittest." src="Section7Bilder/chi2pdg.pdf" />{#fig:chi2PDG width=”70%”}</p>
<p>We will come back in
Ch. <a class="reference external" href="#ChapterHypothesisTesting">[ChapterHypothesisTesting]</a>{reference-type=”ref”
reference=”ChapterHypothesisTesting”} to different quantitative ways to
evaluate the compatibility of the data with a model</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Most of the material of this section comes from:</p>
<ul class="simple">
<li><p>G. Cowan [&#64;CowanBook], “Statistical Data Analysis”: Ch. 7</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="likelihood.html" title="previous page">Parameter Estimation - Likelihood  {#ChapterParameterEstimations}</a>
    <a class='right-next' id="next-link" href="hypothesisTesting.html" title="next page">Hypotheses Testing {#ChapterHypothesisTesting}</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mauro Donega<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>