
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probability Distributions &#8212; Statistical Methods and Data Analysis Techniques</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Jupyter Notebook files" href="notebooks.html" />
    <link rel="prev" title="Probability" href="probability.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Methods and Data Analysis Techniques</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notes.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   Jupyter Notebook files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="errors.html">
   Measurements uncertainties {#ch:errors}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="monteCarlo.html">
   Monte Carlo methods {#sec:MC}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood.html">
   Parameter Estimation - Likelihood  {#ChapterParameterEstimations}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="leastSquares.html">
   Parameter Estimation - Least Squares {#sec:chi2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hypothesisTesting.html">
   Hypotheses Testing {#ChapterHypothesisTesting}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="confidenceIntervals.html">
   Confidence Intervals {#ChapterConfidenceLimits}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mva.html">
   Multivariate Analysis Methods {#ChapterMVA}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unfolding.html">
   Unfolding {#ch:Unfolding}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/probabilityDistributions2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/probabilityDistributions2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-combinatorics">
   Basic combinatorics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutations-with-repetitions">
     Permutations with repetitions:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutations-without-repetitions">
     Permutations without repetitions:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combinations-without-repetitions">
     Combinations without repetitions:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combinations-with-repetitions">
     Combinations with repetitions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discrete-distributions">
   Discrete Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bernoulli-trials">
     Bernoulli trials
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binomial">
     Binomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multinomial-distribution">
     Multinomial Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson-distribution">
     Poisson Distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-distributions">
   Continuous Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#uniform-distribution">
     Uniform Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-or-normal-distribution">
     Gaussian or Normal Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chi-2-distribution-subsectionchi2">
     <span class="math notranslate nohighlight">
      \(\chi^2\)
     </span>
     Distribution {#SubSectionChi2}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-normal-distribution">
     Log-Normal Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exponential-distribution">
     Exponential Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gamma-distribution">
     Gamma Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#student-s-t-distribution-sectionstudentt">
     Student’s
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -distribution {#SectionStudentT}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-distribution-sectionfdistributionchapter3">
     <span class="math notranslate nohighlight">
      \(F\)
     </span>
     Distribution {#SectionFDistributionChapter3}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weibull-distribution">
     Weibull Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cauchy-breit-wigner-distribution-subsectioncauchydistribution">
     Cauchy (Breit-Wigner) Distribution {#SubsectionCauchyDistribution}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#landau-distribution">
     Landau Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#crystal-ball">
     Crystal Ball
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#characteristic-function-sec-characteristic">
   Characteristic Function {#sec:characteristic}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-central-limit-theorem-sec-clt">
   The Central Limit Theorem {#sec:CLT}
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="probability-distributions">
<h1>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we will describe the most common discrete and continuous
probability distribution functions encountered in physics.</p>
<div class="section" id="basic-combinatorics">
<h2>Basic combinatorics<a class="headerlink" href="#basic-combinatorics" title="Permalink to this headline">¶</a></h2>
<p>Given the importance of combinatorics for what follows, we summarize in
this section a few of the main results.</p>
<p>Typically we will be talking about sequences of objects: a distinction
has to be made on whether we care or not about the order of the elements
in a sequence. If we care about the order we talk about
<strong>permutations</strong>, otherwise we talk about <strong>combinations</strong> (permutations
are ordered combinations).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>take the set of letters <span class="math notranslate nohighlight">\(\{abc\}\)</span>. The sequences <span class="math notranslate nohighlight">\(\{cab\}\)</span>
and <span class="math notranslate nohighlight">\(\{bac\}\)</span> are considered equivalent combinations but distinct
permutations.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;HALLO&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HALLO
</pre></div>
</div>
</div>
</div>
<div class="section" id="permutations-with-repetitions">
<h3>Permutations with repetitions:<a class="headerlink" href="#permutations-with-repetitions" title="Permalink to this headline">¶</a></h3>
<p>pick r objects from a set of n and put them back each time. The number
of permutations (ordered sequences) is:</p>
<p><span class="math notranslate nohighlight">\(n^r\)</span></p>
<p><strong>Example</strong>: A byte is a
sequence of 8 bits (0/1): the number of permutations with repetitions is
<span class="math notranslate nohighlight">\(2^8 = 256\)</span>. A lock with three digits (form 0 to 9) has <span class="math notranslate nohighlight">\(10^3\)</span>
permutations.</p>
</div>
<div class="section" id="permutations-without-repetitions">
<h3>Permutations without repetitions:<a class="headerlink" href="#permutations-without-repetitions" title="Permalink to this headline">¶</a></h3>
<p>pick r objects from a set of n and don’t put them back. At each pick you
will have one less object to choose from, so the number number of
permutations is reduced with respect to <span class="math notranslate nohighlight">\(n^r\)</span> (permutations with
repetitions). The number of permutations (ordered sequences) is:</p>
<p><span class="math notranslate nohighlight">\(\frac{n!}{(n-r)!}\)</span></p>
<p><strong>Example</strong>: take all permutations without
repetitions of the 52 cards in the deck: 52! (first pick you choose
among 52 cards, second pick among 51 etc…). Take all permutations of
the first 4 picks in a deck of 52 cards:
<span class="math notranslate nohighlight">\(52\cdot51\cdot50\cdot49 = 52!/ (52-4)!\)</span> (first you pick from 52, then
from 51, then from 50, then form 49).</p>
</div>
<div class="section" id="combinations-without-repetitions">
<h3>Combinations without repetitions:<a class="headerlink" href="#combinations-without-repetitions" title="Permalink to this headline">¶</a></h3>
<p>pick r objects from a set of n and don’t put them back. At each pick you
will have less objects to choose from as for permutations, but this time
all sequences that differ only by their order are considered to be the
same. The number of combinations (non-ordered sequences) is the number
of permutations corrected by the factor that describes the number of
ordered sequences (i.e. r!):</p>
<p><span class="math notranslate nohighlight">\(\frac{n!}{(n-r)!}\frac{1}{r!} = \binom{n}{r}\)</span></p>
<p>These numbers are the
so-called binomial coefficients, which appear in the binomial theorem:</p>
<p><span class="math notranslate nohighlight">\((p+q)^n=\sum_{r=0}^{n}{n\choose r}\, p^r\cdot q^{n-r}\)</span></p>
<p>An interesting
propertiy is that the number of combinations extracting r objects from n
or (n-r) from n is the same.</p>
<p><strong>Example</strong>: “lotto” (six-numbers lottery game): 6 numbers are extracted
(without putting them back) from a set of 90. The order of the
extraction is irrelevant. The probability to win (when all tickets are
sold) is <span class="math notranslate nohighlight">\(1/\binom{90}{6}\)</span> =1.6 <span class="math notranslate nohighlight">\(10^{-9}\)</span>.</p>
</div>
<div class="section" id="combinations-with-repetitions">
<h3>Combinations with repetitions:<a class="headerlink" href="#combinations-with-repetitions" title="Permalink to this headline">¶</a></h3>
<p>pick r objects from a set of n and put them back. As in the case of
permutations with repetition but this time without considering the
order.</p>
<p><span class="math notranslate nohighlight">\(\frac{(n+r-1)!}{(n-1)!r!} =  \binom{n+r-1}{r}\)</span></p>
<p><strong>Example</strong>:
take r-scoops from n-icecream flavours. You can take them all the same
or repeat them as you like (assuming there is enough icecream…).</p>
<p><em>Derivation</em> [&#64;combinationsWithRepetitions]:</p>
<p>Start from an example: take 3 objects from a set of 5 (a,b,c,d,e).
Examples of those sequences are (a a b),<span class="math notranslate nohighlight">\(\;\)</span> (a b c),<span class="math notranslate nohighlight">\(\;\)</span> (c c c).
Now think about the sequences as ordered boxes filled with the letters:
one box for the a’s, one box for the b’s, etc… I will use a separator
“<span class="math notranslate nohighlight">\(|\)</span>” instead of drawing boxes (this trick will become very important in
a second):<br />
(a a b) <span class="math notranslate nohighlight">\(\rightarrow\)</span> a a <span class="math notranslate nohighlight">\(|\)</span> b <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> (the last three
are empty boxes corresponding to c, d, e)<br />
(a b c) <span class="math notranslate nohighlight">\(\rightarrow\)</span> a <span class="math notranslate nohighlight">\(|\)</span> b <span class="math notranslate nohighlight">\(|\)</span> c <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span><br />
(c c c) <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(\;\)</span><span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> c c c <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span><br />
We can also drop the letters and replace them with “x”, the position of
the box already tells which letter it correspnds to.<br />
e.g.: aab <span class="math notranslate nohighlight">\(\rightarrow\)</span> a a <span class="math notranslate nohighlight">\(|\)</span> b <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> x
x <span class="math notranslate nohighlight">\(|\)</span> x <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span> <span class="math notranslate nohighlight">\(\;\)</span> <span class="math notranslate nohighlight">\(|\)</span><br />
Considering “x” and “<span class="math notranslate nohighlight">\(|\)</span>” as objects (here is where the trick becomes
important), we can rephrase the problem as “in how many ways we can
place r = 3 “x” and n = 5-1 = 4 “<span class="math notranslate nohighlight">\(|\)</span>”.
This is the same as the combination w/o repetition “N pick R”, where in
this case:</p>
<p>N = n-1 + r (sum of all “<span class="math notranslate nohighlight">\(|\)</span>” and “x”)
R = r
<span class="math notranslate nohighlight">\(\;\;\;\;\;\Rightarrow\binom{n-1+r}{r}\)</span></p>
<p>For large <span class="math notranslate nohighlight">\(n\)</span>, the <em>Stirling formula</em> can be used to approximate <span class="math notranslate nohighlight">\(n!\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
n! &amp; \approx &amp; \left(\frac{n}{e}\right)^n\sqrt{2\pi n}\\
\ln n! &amp; \approx &amp;  (n+1/2)\ln n-n+ln\sqrt{2\pi}
\end{split}\]</div>
<p>The first term in the second line, <span class="math notranslate nohighlight">\((n/e)^{n}\)</span>, is called the zero-th
approximation, whereas the whole term in the above equation is the first
approximation. The factorial <span class="math notranslate nohighlight">\(n!\)</span> can be extended for non-integer
arguments <span class="math notranslate nohighlight">\(x\)</span> by the gamma function <span class="math notranslate nohighlight">\(\Gamma(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
% \nonumber to remove numbering (before each equation)
x!&amp;=&amp;\int_0^\infty u^xe^{-u}du=\Gamma(x+1)\\
\Gamma(x+1)&amp;=&amp;x\Gamma(x)\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="discrete-distributions">
<h2>Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bernoulli-trials">
<h3>Bernoulli trials<a class="headerlink" href="#bernoulli-trials" title="Permalink to this headline">¶</a></h3>
<p>A <em>Bernoulli trial</em> is an experiment with only two outcomes
(success/failure or 1/0) and where success will occur with constant
probability <span class="math notranslate nohighlight">\(p\)</span> and failure with constant probability <span class="math notranslate nohighlight">\(q=1-p\)</span>. Examples
are again the coin toss, or the decay of <span class="math notranslate nohighlight">\(K^{+}\)</span> into either
<span class="math notranslate nohighlight">\(\mu^{+}\nu\)</span> or any other channels. The random variable <span class="math notranslate nohighlight">\(r\in\{0,1\}\)</span> is
the outcome of the experiment and its p.d.f. (see Fig. <a class="reference external" href="#fig:Bernoulli">1.1</a>{reference-type=”ref”reference=”fig:Bernoulli”})
is:
$<span class="math notranslate nohighlight">\(f(r;p) = p^r~q^{(1-r)}\)</span>$</p>
<p>The p.d.f. is simply the probability for a single experiment to give success/failure.
The first two moments of the distribution are: $<span class="math notranslate nohighlight">\(\mu = p\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(V(r) = p(1-p)\)</span>$</p>
<p><img alt="[[fig:Bernoulli]]{#fig:Bernoulli label=&quot;fig:Bernoulli&quot;}Bernoullitrials distribution for a fixed  and 1000trials." src="Section2Bilder/bernoulli" />{#fig:Bernoulli width=”50%”}</p>
</div>
<div class="section" id="binomial">
<h3>Binomial<a class="headerlink" href="#binomial" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(n\)</span> Bernoulli trials with a probability of success <span class="math notranslate nohighlight">\(p\)</span>, the
binomial distribution gives the probability to observe <span class="math notranslate nohighlight">\(r\)</span> successes
and, consequently, <span class="math notranslate nohighlight">\(n-r\)</span> failures independently of the order with which
they appear. The random variable is again <span class="math notranslate nohighlight">\(r\)</span> but this time
<span class="math notranslate nohighlight">\(r\in\{0,n\}\)</span>, i.e. the maximum is given when all trials give a success.
The p.d.f. is:</p>
<div class="math notranslate nohighlight">
\[\label{binomial}
P(r;n,p)={n\choose r}\, p^r(1-p)^{n-r}.\]</div>
<p>Eq. <a class="reference external" href="#binomial">[binomial]</a>{reference-type=”ref” reference=”binomial”}
can be motivated in the following way: the probability that we get a
positive outcome in the first <span class="math notranslate nohighlight">\(r\)</span> attempts and negative outcome in the
last <span class="math notranslate nohighlight">\(n-r\)</span> attempts, is given by <span class="math notranslate nohighlight">\(p^{r} \cdot (1-p)^{n-r}\)</span>; but this
sequential arrangement is only one of a total of <span class="math notranslate nohighlight">\({n \choose r}\)</span>
possible arrangements. The distribution for different values of the
parameters is plotted in
Fig. <a class="reference external" href="#FigBinomialDist">1.2</a>{reference-type=”ref”
reference=”FigBinomialDist”}.</p>
<p>The important properties of the binomial distribution are:</p>
<ul class="simple">
<li><p>It is normalized to 1, i.e. <span class="math notranslate nohighlight">\(\sum_{r=0}^n P(r)=1.\)</span></p></li>
<li><p>The mean of <span class="math notranslate nohighlight">\(r\)</span> is <span class="math notranslate nohighlight">\(&lt;r&gt;=\sum_{r=0}^n r\cdot P(r)=np.\)</span></p></li>
<li><p>The variance of <span class="math notranslate nohighlight">\(r\)</span> is <span class="math notranslate nohighlight">\(V(r)=np(1-p).\)</span></p></li>
</ul>
<p>The binomial distribution (like several others we will encounter) has
the <strong>reproductive property</strong>. If <span class="math notranslate nohighlight">\(X\)</span> is binomially distributed as
<span class="math notranslate nohighlight">\(P(X;n,p)\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is binomially distributed (with the same probability
<span class="math notranslate nohighlight">\(p\)</span>) as <span class="math notranslate nohighlight">\(P(Y;m,p)\)</span>, then the sum is binomially distributed as
<span class="math notranslate nohighlight">\(P(X+Y; n+m, p)\)</span>.<br />
\</p>
<p><img alt="[[FigBinomialDist]]{#FigBinomialDist label=&quot;FigBinomialDist&quot;}Thebinomial distribution for a fixed  and different values for." src="Section2Bilder/BinomialDistJPG" />{#FigBinomialDist
width=”\textwidth”}\</p>
<p><strong>Example</strong>: what is the probability to get out of 10 coin tosses 3
times a “head”? Solution:
<span class="math notranslate nohighlight">\(P(3; 10, 0.5)={10 \choose 3}\,0.5^3\cdot (1-0.5)^{10-3}=\frac{10!}{3!7!}0.5^3\cdot 0.5^7=0.12\)</span><br />
<br />
<strong>Example</strong>: a detector with 4 layers has an efficiency per layer to
detect a traversing particle of 88%. To reconstruct the complete track
of the particle, we need at least three hits (i.e. three out of the four
layers have to detect the particle). What is the probability to
reconstruct the track?<br />
We need at least 3 hits (i.e. 3 or 4), so we have to sum the probability
to have 3 hits to the probability to have 4 hits:<br />
<span class="math notranslate nohighlight">\(P(r\ge 3; n=4, p=0.88) = P(r = 3; n=4, p=0.88) + P(r = 4; n=4, p=0.88) =\)</span><br />
<span class="math notranslate nohighlight">\(0.33 + 0.60 = 0.93\)</span><br />
<br />
What if we have 3 or 5 layers?<br />
For 3 layers <span class="math notranslate nohighlight">\(= P(r = 3; n=3, p=0.88) = 0.68\)</span><br />
For 5 layers <span class="math notranslate nohighlight">\(= P(r \ge 3; n=5, p=0.88) = 0.10 + 0.36 + 0.53 = 0.99\)</span></p>
</div>
<div class="section" id="multinomial-distribution">
<h3>Multinomial Distribution<a class="headerlink" href="#multinomial-distribution" title="Permalink to this headline">¶</a></h3>
<p>The precedent considerations can directly be generalized for the
multidimensional problem. Assume we have <span class="math notranslate nohighlight">\(n\)</span> objects of <span class="math notranslate nohighlight">\(k\)</span> different
types, and <span class="math notranslate nohighlight">\(r_{i}\)</span> is the number of objects of type <span class="math notranslate nohighlight">\(i\)</span>. The number of
distinguishable arrangements is then given by
<span class="math notranslate nohighlight">\(\frac{n!}{r_1!r_2!\cdots r_k!}\)</span>.
If we now choose randomly</p>
<div class="math notranslate nohighlight">
\[n = r_1 + r_2 + ... + r_k\]</div>
<p>objects (putting them back every time), then
the probability of getting an arrangement of <span class="math notranslate nohighlight">\(r_{1}\)</span> objects of type
<span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(r_{2}\)</span> objects of type <span class="math notranslate nohighlight">\(2\)</span>, etc… is given by
<span class="math notranslate nohighlight">\(p_1^{r_1}\cdot p_2^{r_2}\cdots p_k^{r_k}\)</span>. The overall probability is
therefore simply the probability of our arrangement, multiplied with the
number of possible distinguishable arrangements:</p>
<div class="math notranslate nohighlight">
\[P(r_1,..,r_k;p_1,p_k)=\left(\frac{n!}{r_1!r_2!r_3!\cdots r_k!}\right)p_1^{r_1}\cdot p_2^{r_2}\cdots p_k^{r_k}.\]</div>
<p>This distribution is called the multinomial distribution and it is what
describes the probability to have <span class="math notranslate nohighlight">\(r_i\)</span> events in bin i of a histogram
with <span class="math notranslate nohighlight">\(n\)</span> entries. The corresponding properties are:</p>
<div class="math notranslate nohighlight">
\[&lt;r_i&gt;=np_i \qquad \textrm{and} \qquad \; V(r_i)=np_i(1-p_i).\]</div>
<p>You can also compute the covariance among the bins of a histogram:</p>
<div class="math notranslate nohighlight">
\[\mbox{cov}(r_i, r_j) = -n p_i p_j\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\rho_{ij} = \frac{\mbox{cov}(r_i,r_j)}{\sigma_i \sigma_j} = -\sqrt{\frac{p_i}{1-p_i}\frac{p_j}{1-p_j}}\]</div>
<p>The correlation among bins comes from the fact that the total number of entries <span class="math notranslate nohighlight">\(n = r_1+\ldots+r_k\)</span> is fixed, i.e.</p>
<div class="math notranslate nohighlight">
\[r_i = n - r_41 - \ldots - r_{i-1} - r_{i+1} -\ldots - r_k .\]</div>
<p>If n is not fixed, i.e. n is another random variable, the bin entries are
uncorrelated and instead of having a multinomial we will have a Poisson
for each bin (see also extended maximum likelihood fit</p>
<p><a class="reference external" href="#sec:EMLF">[sec:EMLF]</a>{reference-type=”ref” reference=”sec:EMLF”})</p>
</div>
<div class="section" id="poisson-distribution">
<h3>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Poisson p.d.f. applies to the situations where we detect events but
do not know the total number of trials. An example is a radioactive
source where we detect the decays but do not detect the non-decays.<br />
The distribution can be obtained as a limit of the binomial: let
<span class="math notranslate nohighlight">\(\lambda\)</span> be the probability to observe a radioactive decay in a period
<span class="math notranslate nohighlight">\(T\)</span> of time. Now divide the period <span class="math notranslate nohighlight">\(T\)</span> in <span class="math notranslate nohighlight">\(n\)</span> time intervals
<span class="math notranslate nohighlight">\(\Delta T  = T/n\)</span> small enough that the probability to observe two
decays in an interval is negligible. The probability to observe a decay
in <span class="math notranslate nohighlight">\(\Delta T\)</span> is then <span class="math notranslate nohighlight">\(\lambda / n\)</span>, while the probability to observe
<span class="math notranslate nohighlight">\(r\)</span> decays in the period <span class="math notranslate nohighlight">\(T\)</span> is given by the binomial probability to
observe <span class="math notranslate nohighlight">\(r\)</span> events in <span class="math notranslate nohighlight">\(n\)</span> trials each of which has a probability
<span class="math notranslate nohighlight">\(\lambda / n\)</span>.</p>
<div class="math notranslate nohighlight">
\[P\left(r; n,\frac{\lambda}{n} \right) = \frac{n!}{(n-r)!}\frac{1}{r!} \left( \frac{\lambda}{n}\right)^r \left( 1- \frac{\lambda}{n}  \right) ^{n-r} \label{eq:bin}\]</div>
<p>Under the assumption that <span class="math notranslate nohighlight">\(n &gt;&gt; r\)</span> then:</p>
<div class="math notranslate nohighlight">
\[\frac{n!}{(n-r)!} = n(n-1)(n-2)\dots(n-r+1) \sim n^r\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\left( 1- \frac{\lambda}{n}  \right) ^{n-r} \sim \left( 1- \frac{\lambda}{n}  \right) ^{n} \to e^{-\lambda} \qquad \textrm{for} \qquad n \to \infty\]</div>
<p>and replacing this in the binomial expression in Eq. <a class="reference external" href="#eq:bin">[eq:bin]</a>{reference-type=”ref” reference=”eq:bin”} we
obtain the Poisson p.d.f:</p>
<div class="math notranslate nohighlight">
\[P(r;\lambda) = \frac{\lambda^r e^{-\lambda}}{r!}.\]</div>
<p>The Poisson distribution can so be seen as the limit of the binomial
distribution when the number <span class="math notranslate nohighlight">\(n\)</span> of trials becomes very large and the
probability <span class="math notranslate nohighlight">\(p\)</span> for a single event becomes very small, while the product
<span class="math notranslate nohighlight">\(pn = \lambda\)</span> remains a (finite) constant. It gives the probability of
getting <span class="math notranslate nohighlight">\(r\)</span> events if the expected number (mean) is <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Properties of the Poisson distribution:</p>
<ul class="simple">
<li><p>it is normalized to 1:
<span class="math notranslate nohighlight">\(\sum_{r=0}^{\infty}P(r)=e^{-\lambda}\sum_{r=0}^{\infty}\frac{\lambda^r}{r!}=e^{-\lambda}e^{+\lambda}=1\)</span></p></li>
<li><p>the mean <span class="math notranslate nohighlight">\(&lt;r&gt;\)</span> is <span class="math notranslate nohighlight">\(\lambda\)</span>:
<span class="math notranslate nohighlight">\(&lt;r&gt;=\sum_{r=0}^{\infty}r\cdot \frac{e^{-\lambda}\lambda^r}{r!}=\lambda\)</span></p></li>
<li><p>the variance is <span class="math notranslate nohighlight">\(V(r)=\lambda\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(r+s;\lambda_r, \lambda_s) = \frac{(\lambda_r+\lambda_s)^{(r+s)} e^{-(\lambda_r+\lambda_s)}}{(r+s)!}\)</span>:
the p.d.f. of the sum of two Poisson distributed random variables is
also Poisson with <span class="math notranslate nohighlight">\(\lambda\)</span> equal to the sum of the <span class="math notranslate nohighlight">\(\lambda\)</span>’s of
the individual Poissons</p></li>
</ul>
<p><strong>Example</strong> An historical example <a class="footnote-reference brackets" href="#id5" id="id1">1</a> is the number of deadly horse
accidents in the Prussian army. The fatal incidents were registered over
twenty years in ten different cavalry corps. There was a total of 122
fatal incidents, and therefore the expectation value per corps per year
is given by <span class="math notranslate nohighlight">\(\lambda = 122/200 =0.61\)</span>. The probability that no soldier
is killed per year and corps is
<span class="math notranslate nohighlight">\(P(0;0.61) = e^{-0.61} \cdot 0.61^{0} / 0! = 0.5434\)</span>. To get the total
events (of no incidents) in one year and per corps, we have to multiply
with the number of observed cases (here 200), which yields
<span class="math notranslate nohighlight">\(200 \cdot 0.5434 = 108.7\)</span>. The total statistics of the Prussian cavalry
is summarized in Tab. <a class="reference external" href="#tab1_sec2">1.1</a>{reference-type=”ref”
reference=”tab1_sec2”}, in agreement with the Poisson expectation.
{#tab1_sec2}</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Fatal incidents per corps and year</p></th>
<th class="head"><p>Reported incidents</p></th>
<th class="head"><p>Poisson distribution</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>109</p></td>
<td><p>108.7</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>65</p></td>
<td><p>66.3</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>22</p></td>
<td><p>20.2</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>3</p></td>
<td><p>4.1</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>1</p></td>
<td><p>0.6</p></td>
</tr>
</tbody>
</table>
<p>[[tab1_sec2]]{#tab1_sec2 label=”tab1_sec2”}The total statistics
of deadly accidents of Prussian soldiers.</p>
<p>The Poisson distribution is very often used in counting experiments:</p>
<ul class="simple">
<li><p>number of particles which are registered by a detector in the time
interval <span class="math notranslate nohighlight">\(t\)</span>, if the flux <span class="math notranslate nohighlight">\(\Phi\)</span> and the efficiency of the detector
are independent of time and the dead time of the detector <span class="math notranslate nohighlight">\(\tau\)</span> is
sufficiently small, such that <span class="math notranslate nohighlight">\(\phi \tau \ll 1\)</span></p></li>
<li><p>number of interactions caused by an intense beam of particles which
travel through a thin foil</p></li>
<li><p>number of entries in a histogram, if the data are taken during a
fixed time interval</p></li>
<li><p>number of flat tires when traveling a certain distance, if the
expectation value flats / distance is constant</p></li>
</ul>
<p>Some counter-examples, in which the Poisson distribution <em>cannot</em> be
used:</p>
<ul class="simple">
<li><p>the decay of a small amount of radioactive material in a certain
time interval, if this interval is comparable to the lifetime</p></li>
<li><p>the number of interactions of a beam of only a few particles which
pass through a thick foil</p></li>
</ul>
<p>In both cases we have the event rate is not constant (in the first it
decreases with time, in the second with distance) and therefore the
Poisson distribution cannot be applied.<br />
The Poisson p.d.f. requires that the events be independent. Consider the
case of a counter with a dead time of 1 <span class="math notranslate nohighlight">\(\mu sec\)</span>. This means that if a
second particle passes through the counter within 1 <span class="math notranslate nohighlight">\(\mu sec\)</span> after one
which was recorded, the counter is incapable of recording the second
particle. Thus the detection of a particle is not independent of the
detection of other particles. If the particle flux is low, the chance of
a second particle within the dead time is so small that it can be
neglected. However, if the flux is high it cannot be. No matter how high
the flux, the counter cannot count more than <span class="math notranslate nohighlight">\(10^6\)</span> particles per
second. In high fluxes, the number of particles detected in some time
interval will not be Poisson distributed.</p>
<p><img alt="[[FigPoissonDist]]{#FigPoissonDist label=&quot;FigPoissonDist&quot;}The Poisson distribution for different values of ." src="Section2Bilder/PoissonDistJPG" />{#FigPoissonDist
width=”\textwidth”}\</p>
<p>Fig. <a class="reference external" href="#FigPoissonDist">1.3</a>{reference-type=”ref”
reference=”FigPoissonDist”} shows the Poisson distribution for some
values of <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
</div>
<div class="section" id="continuous-distributions">
<h2>Continuous Distributions<a class="headerlink" href="#continuous-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="uniform-distribution">
<h3>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this headline">¶</a></h3>
<p>The probability density function of the uniform distribution in the
interval <span class="math notranslate nohighlight">\([a,b]\)</span> is given by (see Fig. <a class="reference external" href="#fig:Uniform">1.4</a>{reference-type=”ref” reference=”fig:Uniform”}):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x)= 
\begin{cases} \frac{1}{b-a} \quad &amp;\text{if } a \le x \le b \\
0 &amp;\text{else}.
\end{cases}\end{aligned}$$ The expectation value and the variance are
given by $$\begin{aligned}
&lt;x&gt;&amp;=&amp;\int_a^b\frac{x}{b-a}dx=\frac{1}{2}(a+b) \\
\mbox{Var}(x)&amp;=&amp;\frac{1}{12}(b-a)^2
\end{split}\]</div>
<p><img alt="[[fig:Uniform]]{#fig:Uniform label=&quot;fig:Uniform&quot;}A uniformdistribution." src="Section2Bilder/uniform" />{#fig:Uniform width=”50%”}\</p>
<p><strong>Example</strong>  Consider a detector built as a single strip of silicon with
a width of 1mm. If a charged particle hits it, the detector reads 1
otherwise zero (binary readout). What is the spacial resolution of the
detector? Estimating the resolution as the variance of the corresponding
uniform distribution, we get <span class="math notranslate nohighlight">\(\sim 290\mu m\)</span>.</p>
</div>
<div class="section" id="gaussian-or-normal-distribution">
<h3>Gaussian or Normal Distribution<a class="headerlink" href="#gaussian-or-normal-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Gaussian<a class="footnote-reference brackets" href="#id6" id="id2">2</a> or normal distribution is probably the most important
and useful distribution we know<a class="footnote-reference brackets" href="#id7" id="id3">3</a>. The probability density function is
$<span class="math notranslate nohighlight">\(f(x;\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.\)</span><span class="math notranslate nohighlight">\(
The Gaussian distribution is described by two parameters: the mean value
\)</span>\mu<span class="math notranslate nohighlight">\( and the variance \)</span>\sigma^{2}<span class="math notranslate nohighlight">\( or the standard deviation \)</span>\sigma<span class="math notranslate nohighlight">\(.
By substituting \)</span>z = (x-\mu) / \sigma<span class="math notranslate nohighlight">\( we obtain the so-called normal or
standardized Gaussian distribution:
\)</span><span class="math notranslate nohighlight">\(N(0,1)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}.\)</span>$ It has an mean of zero and
standard deviation 1.<br />
Properties of the normal distribution are:</p>
<ul class="simple">
<li><p>it is normalized to 1: <span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty}P(x;\mu,\sigma)dx=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the first moment of the distribution:
<span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty}xP(x;\mu,\sigma)dx=\mu\)</span></p></li>
<li><p>being a symmetric distribution, <span class="math notranslate nohighlight">\(\mu\)</span> is also its mode and median</p></li>
<li><p>its second central moment is <span class="math notranslate nohighlight">\(\sigma^2\)</span>:
<span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty}(x-\mu)^2P(x;\mu,\sigma)dx=\sigma^2\)</span></p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are two independent r.v.’s distributed as
<span class="math notranslate nohighlight">\(f(x;\mu_x,\sigma_x)\)</span> and <span class="math notranslate nohighlight">\(f(y;\mu_y,\sigma_y)\)</span> then Z = X + Y is
distributed as <span class="math notranslate nohighlight">\(f(z;\mu_z,\sigma_z)\)</span> with <span class="math notranslate nohighlight">\(\mu_z = \mu_x+\mu_y\)</span> and
<span class="math notranslate nohighlight">\(\sigma_z^2 = \sigma_x^2 + \sigma_y^2\)</span>.<br />
Some useful integrals, which are often used when working with the
Gaussian function: $<span class="math notranslate nohighlight">\(\begin{aligned}
\int_{-\infty}^{+\infty}e^{-ax^2}dx &amp; = &amp; \sqrt{\pi/a}\\
\int_{0}^{+\infty}xe^{-ax^2}dx &amp; = &amp; \frac{1}{2a}\\
\int_{-\infty}^{+\infty}x^2e^{-ax^2}dx &amp; = &amp; \frac{1}{2a}\sqrt{\pi/a}\\
\int_{0}^{+\infty}x^{2n+1}e^{-ax^2}dx &amp; = &amp; \frac{n!}{2a^{n+1}}\\
\int_{-\infty}^{+\infty}x^{2n+1}e^{-ax^2}dx &amp; = &amp;0, \, \mbox{for all odd values of \)</span>n<span class="math notranslate nohighlight">\(} \\\end{aligned}\)</span>$</p>
<p><img alt="[[FigGaussDist]]{#FigGaussDist label=&quot;FigGaussDist&quot;}The standardizedGaussian distribution. On the top graph the cumulative distributionfunction is shown, on the lower graph its probability distributionfunction." src="Section2Bilder/GaussDist" />{#FigGaussDist width=”75%”}\</p>
<p>Here are some numbers for the integrated Gaussian distribution:</p>
<ul class="simple">
<li><p>68.27% of the area lies within <span class="math notranslate nohighlight">\(\pm\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>95.45% lies within <span class="math notranslate nohighlight">\(\pm 2\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>99.73% lies within <span class="math notranslate nohighlight">\(\pm 3 \sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>90% of the area lies within <span class="math notranslate nohighlight">\(\pm 1.645\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>95% lies within <span class="math notranslate nohighlight">\(\pm 1.960\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>99% lies within <span class="math notranslate nohighlight">\(\pm 2.576\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
<li><p>99.9% lies within <span class="math notranslate nohighlight">\(\pm 3.290\sigma\)</span> around the mean <span class="math notranslate nohighlight">\(\mu\)</span></p></li>
</ul>
<p>The integrated Gaussian function <span class="math notranslate nohighlight">\(\Phi(x)\)</span> can also be expressed by the
so-called error function <span class="math notranslate nohighlight">\(erf(x)\)</span>: $<span class="math notranslate nohighlight">\(\begin{aligned}
\Phi(x)&amp;=&amp;\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x}e^{-(t-\mu)^2/2\sigma^2}dt \\
erf(x)&amp;=&amp;\frac{2}{\sqrt{\pi}}\int_0^xe^{-t^2}dt \\
=&gt;\Phi(x)&amp;=&amp;\frac{1}{2}\left(1+erf(\frac{x-\mu}{\sqrt{2}\sigma})\right)\end{aligned}\)</span>$</p>
<p>The <em>Full Width Half Maximum</em> (FWHM) is very useful to get a quick
estimate for the width of a distribution and for the specific case of
the Gaussian we have: $<span class="math notranslate nohighlight">\(FWHM=2\sigma\sqrt{2\ln 2}=2.355\sigma.\)</span>$ The
Gaussian distribution is the limiting case for several other p.d.f.’s we
will encounter later (see Fig. <a class="reference external" href="#fig:limits">1.6</a>{reference-type=”ref”
reference=”fig:limits”}). This is actually a consequence of the central
limit theorem (CLT) (discussed in
Sec. <a class="reference external" href="#sec:CLT">1.5</a>{reference-type=”ref” reference=”sec:CLT”}).</p>
<p><img alt="[[fig:limits]]{#fig:limits label=&quot;fig:limits&quot;}Limiting cases.[Metzger]" src="Section2Bilder/limits" />{#fig:limits width=”75%”}\</p>
<p><br />
<br />
<br />
The N-dimensional Gaussian distribution is defined by
$<span class="math notranslate nohighlight">\(f({\bf x};{\bf \mu}, V)=\frac{1}{(2\pi)^{N/2}|V|^{1/2}}\exp\left(-\frac{1}{2}({\bf x}-{\bf \mu})^TV^{-1}({\bf x}-{\bf \mu})\right).\)</span><span class="math notranslate nohighlight">\(
Here, \)</span>{\bf x}<span class="math notranslate nohighlight">\( and \)</span>{\bf \mu}<span class="math notranslate nohighlight">\( are column vectors with the components
\)</span>x_1,\ldots ,x_N<span class="math notranslate nohighlight">\( and \)</span>\mu_1,\ldots ,\mu_N<span class="math notranslate nohighlight">\(, respectively. The
transposed vectors \)</span>{\bf x}^T<span class="math notranslate nohighlight">\( and \)</span>{\bf \mu}^T<span class="math notranslate nohighlight">\( are the corresponding
row vectors and \)</span>|V|<span class="math notranslate nohighlight">\( is the determinant of the symmetric \)</span>N \times N<span class="math notranslate nohighlight">\(
covariance matrix \)</span>V$. The expectation values and the covariances are
given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\langle x_i \rangle=\mu_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{V}(x_i)=\mbox{V}_{ii}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{cov}(x_i,x_j)=\mbox{V}_{ij}\)</span></p></li>
</ul>
<p>In the simplified case of a two-dimensional Gaussian distribution we can
write $<span class="math notranslate nohighlight">\(\begin{aligned}
f(x_1,x_2;\mu_1\,\mu_2,\sigma_1,\sigma_2,\rho) &amp;=&amp;
\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \cdot \exp \left( -\frac{1}{2(1-\rho^2)}\right)\\ 
&amp;\cdot&amp; \left[\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2 + 
\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2
-2\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)
\left(\frac{x_2-\mu_2}{\sigma_2}\right)\right].\end{aligned}\)</span>$ We will
come back to the specific case of the gaussian distribution in multiple
dimensions in Ch. <a class="reference external" href="#ch:errors">[ch:errors]</a>{reference-type=”ref”
reference=”ch:errors”} when talking about the error matrix.</p>
</div>
<div class="section" id="chi-2-distribution-subsectionchi2">
<h3><span class="math notranslate nohighlight">\(\chi^2\)</span> Distribution {#SubSectionChi2}<a class="headerlink" href="#chi-2-distribution-subsectionchi2" title="Permalink to this headline">¶</a></h3>
<p>Assume that <span class="math notranslate nohighlight">\(x_{1}, x_{2}, \cdots, x_{n}\)</span> are independent random
variables, which obey a Gaussian distribution. Then the joint p.d.f. is:
$<span class="math notranslate nohighlight">\(\begin{aligned}
f({\bf x}; {\bf \mu}, {\bf \sigma}) &amp;=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi} \sigma_i}\exp\left[-\frac{1}{2} \left( \frac{x_i - \mu_i}{\sigma_i}\right)^2 \right]\\
                                    &amp;=&amp; \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \left( \frac{x_i - \mu_i}{\sigma_i}\right)^2 \right]  \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi} \sigma_i}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Then the variable \)</span>\chi^{2}(n)<span class="math notranslate nohighlight">\( defined as
\)</span><span class="math notranslate nohighlight">\(\chi^2(n)=\sum_{i=1}^{n}\left(\frac{x_i-\mu_i}{\sigma_i}\right)^2\)</span><span class="math notranslate nohighlight">\(
being a function of random variables is itself a random variable
distributed as a \)</span>\chi^{2}<span class="math notranslate nohighlight">\( distribution with \)</span>n<span class="math notranslate nohighlight">\( degrees of freedom.
The probability density is given by (see
Fig. [1.7](#fig:chi2){reference-type=&quot;ref&quot; reference=&quot;fig:chi2&quot;}):
\)</span><span class="math notranslate nohighlight">\(\chi^2(n) = f(\chi^2;n) = \frac{(\chi^2)^{n/2-1}e^{-\chi^2/2}}{\Gamma(n/2)2^{n/2}}.\)</span>$</p>
<p><img alt="[[fig:chi2]]{#fig:chi2 label=&quot;fig:chi2&quot;}The  distributionfor different degrees offreedom.[wiki]" src="Section2Bilder/chi2" />{#fig:chi2 width=”50%”}\</p>
<p>The <span class="math notranslate nohighlight">\(\chi^2(n)\)</span> p.d.f. has the properties:</p>
<ul class="simple">
<li><p>mean = <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>variance = <span class="math notranslate nohighlight">\(2n\)</span></p></li>
<li><p>mode = n-2 for <span class="math notranslate nohighlight">\(n\ge2\)</span> and 0 for <span class="math notranslate nohighlight">\(n\le2\)</span></p></li>
<li><p>reproductive property:
<span class="math notranslate nohighlight">\(\chi^2_{n_1+n_2} = \chi^2_{n_1} + \chi^2_{n_2} = \chi^2(n_1+n_2)\)</span></p></li>
</ul>
<p>Since the expectation of <span class="math notranslate nohighlight">\(\chi^2(n)\)</span> is <span class="math notranslate nohighlight">\(n\)</span>, the expectation of
<span class="math notranslate nohighlight">\(\chi^2(n)/n\)</span> is 1. The quantity <span class="math notranslate nohighlight">\(\chi^2(n)/n\)</span> is called “reduced
<span class="math notranslate nohighlight">\(\chi^2\)</span>”.<br />
For <span class="math notranslate nohighlight">\(n \to \infty\)</span>, <span class="math notranslate nohighlight">\(\chi^{2}(n)\to N(\chi^2;n,2n)\)</span> becomes a normal
distribution. When using the <span class="math notranslate nohighlight">\(\chi^{2}\)</span>-distribution in practice, the
approximation by a normal distribution is already sufficient for
<span class="math notranslate nohighlight">\(n \ge 30\)</span>.<br />
<br />
To understand the notation take the particular case of the <span class="math notranslate nohighlight">\(\chi^2\)</span> for
1 degree of freedom. Let <span class="math notranslate nohighlight">\(z = (x-\mu)/\sigma\)</span> so that the p.d.f. for <span class="math notranslate nohighlight">\(z\)</span>
is <span class="math notranslate nohighlight">\(N(z; 0, 1)\)</span> and the probability that <span class="math notranslate nohighlight">\(z \le Z \le z + dz\)</span> is:
$<span class="math notranslate nohighlight">\(f(z)dz = \frac{1}{\sqrt2\pi} e^{-\frac{1}{2} z^2 }dz\)</span><span class="math notranslate nohighlight">\( Let \)</span>Q = Z^2<span class="math notranslate nohighlight">\(.
(We use Q here instead of \)</span>\chi^2<span class="math notranslate nohighlight">\( to emphasize that this is the
variable.) This is not a one-to-one transformation because both \)</span>+Z<span class="math notranslate nohighlight">\( and
\)</span>-Z<span class="math notranslate nohighlight">\( go into \)</span>+Q$.</p>
<p>The probability that <span class="math notranslate nohighlight">\(Q\)</span> is between <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(q + dq\)</span> is the sum of the
probability that <span class="math notranslate nohighlight">\(Z\)</span> is between <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(z + dz\)</span> around <span class="math notranslate nohighlight">\(z = \sqrt{q}\)</span>,
and the probability that <span class="math notranslate nohighlight">\(Z\)</span> is between <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(z -dz\)</span> around
<span class="math notranslate nohighlight">\(z = -\sqrt{q}\)</span>. The Jacobian is:
$<span class="math notranslate nohighlight">\(J_{\pm} = \frac{d(\pm z)}{dq} = \pm \frac{1}{2\sqrt{q}}\)</span><span class="math notranslate nohighlight">\( so
\)</span><span class="math notranslate nohighlight">\(f(q)dq=  \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}q}(|J_+| + |J_-|) dq  =  \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}q}(\frac{dq}{2\sqrt{q}} + \frac{dq}{2\sqrt{q}}) dq = \frac{1}{\sqrt{2\pi q}} e^{-\frac{1}{2}q} dq\)</span><span class="math notranslate nohighlight">\(
Replacing \)</span>\chi^2<span class="math notranslate nohighlight">\( for \)</span>Q<span class="math notranslate nohighlight">\( we have:
\)</span><span class="math notranslate nohighlight">\(\chi^2(1) = \frac{1}{\sqrt{2\pi\chi^2}}e^{-\frac{1}{2} \chi^2}\)</span>$
(careful! the same symbol is used for the random variable and the
p.d.f.).</p>
</div>
<div class="section" id="log-normal-distribution">
<h3>Log-Normal Distribution<a class="headerlink" href="#log-normal-distribution" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(y\)</span> obeys a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard
deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, then it follows that <span class="math notranslate nohighlight">\(x=e^{y}\)</span> obeys a log-normal
distribution. This means that <span class="math notranslate nohighlight">\(\ln(x)\)</span> is normal distributed (see
Fig. <a class="reference external" href="#fig:logNorm">1.8</a>{reference-type=”ref” reference=”fig:logNorm”}):
$<span class="math notranslate nohighlight">\(f(x; \mu, \sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}\frac{1}{x}e^{-(\ln x-\mu)^2/2\sigma^2}.\)</span>$</p>
<p><img alt="[[fig:logNorm]]{#fig:logNorm label=&quot;fig:logNorm&quot;}The log-normaldistribution for different values of the definingparameters.[wiki]" src="Section2Bilder/logNorm" />{#fig:logNorm
width=”50%”}\</p>
<p>The expectation value and the variance are given by: $<span class="math notranslate nohighlight">\(\begin{aligned}
&lt;x&gt;&amp;=&amp;e^{(\mu+\frac{1}{2}\sigma^2)} \\
\mbox{Var}(x)&amp;=&amp;e^{(2\mu+\sigma^2)}(e^{\sigma^2}-1)\end{aligned}\)</span><span class="math notranslate nohighlight">\( The
log-normal distribution is typically used when the resolution of a
measurement apparatus is composed by different sources, each
contributing a (multiplicative) amount to the overall resolution. As the
*sum* of many small contributions of any random distribution converges
by the central limit theorem to a Gaussian distribution, so the
*product* of many small contributions is distributed according to a
log-normal distribution.\
\
**Example** Consider the signal of a photomultiplier (PMT), which
converts light signals into electric signals. Each photon hitting the
photo-cathode emits an electron, which gets accelerated by an electric
field generated by an electrode (dynode) behind. The electron hits the
dynode and emits other secondary electrons which gets accelerated to the
next dynode. This process if repeated several times (as many as the
number of dynodes in the PMT). At every stage the number of secondary
electrons emitted depends on the voltage applied. If the amplification
per step is \)</span>a_{i}<span class="math notranslate nohighlight">\(, then the number of electrons after the \)</span>k^{th}<span class="math notranslate nohighlight">\(
step, \)</span>n_{k} = \Pi_{i=0}^{k} a_{i}$, is approximately log-normal
distributed.</p>
</div>
<div class="section" id="exponential-distribution">
<h3>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Permalink to this headline">¶</a></h3>
<p>The exponential distribution (see
Fig. <a class="reference external" href="#fig:expo">1.10</a>{reference-type=”ref” reference=”fig:expo”}) is
defined for a continuous variable <span class="math notranslate nohighlight">\(t\)</span> (<span class="math notranslate nohighlight">\(0 \le t \le \infty\)</span>) by:
$<span class="math notranslate nohighlight">\(f(t,\tau)=\frac{1}{\tau}e^{-t/\tau}.\)</span>$</p>
<p><img alt="[[fig:expo]]{#fig:expo label=&quot;fig:expo&quot;}The exponential distributionfor different values of  (right: linear, left: logarithmicscale)." src="Section2Bilder/exponential" />{#fig:expo width=”40%”}
<img alt="[[fig:expo]]{#fig:expo label=&quot;fig:expo&quot;}The exponential distributionfor different values of  (right: linear, left: logarithmicscale)." src="Section2Bilder/exponentialLog" />{#fig:expo width=”40%”}\</p>
<p>The probability density is characterized by one single parameter <span class="math notranslate nohighlight">\(\tau\)</span>.</p>
<ul class="simple">
<li><p>The expectation value is
$<span class="math notranslate nohighlight">\(\langle t \rangle=\frac{1}{\tau}\int_0^{\infty}te^{-t/\tau}dt=\tau.\)</span>$</p></li>
<li><p>The variance is <span class="math notranslate nohighlight">\(\mbox{Var}(t)=\tau^2\)</span>.</p></li>
</ul>
<p>An example for the application of the exponential distribution is the
description of the proper-decay-time decay <span class="math notranslate nohighlight">\((t)\)</span> of an unstable
particles. The parameter <span class="math notranslate nohighlight">\(\tau\)</span> corresponds in this case to the mean
lifetime of the particle.</p>
</div>
<div class="section" id="gamma-distribution">
<h3>Gamma Distribution<a class="headerlink" href="#gamma-distribution" title="Permalink to this headline">¶</a></h3>
<p>The gamma distribution (see Fig. <a class="reference external" href="#fig:gamma">1.11</a>{reference-type=”ref”
reference=”fig:gamma”}) is given by:
$<span class="math notranslate nohighlight">\(f(x;k,\lambda)=\lambda^{k}\frac{x^{k-1}e^{-\lambda x}}{\Gamma(k)}.\)</span>$</p>
<p><img alt="[[fig:gamma]]{#fig:gamma label=&quot;fig:gamma&quot;}The gamma distributionfor different values of theparameters.[wiki]" src="Section2Bilder/gamma" />{#fig:gamma
width=”50%”}\</p>
<ul class="simple">
<li><p>The expectation value is <span class="math notranslate nohighlight">\(&lt;x&gt; = k / \lambda\)</span>.</p></li>
<li><p>The variance is <span class="math notranslate nohighlight">\(\sigma ^{2} = k / \lambda^{2}\)</span>.</p></li>
</ul>
<p>Special cases:</p>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(\lambda = 1/2\)</span> and <span class="math notranslate nohighlight">\(k=n/2\)</span> the gamma distribution has the form
of a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with n degrees of freedom.</p></li>
</ul>
<p>The Gamma distribution describes the distribution of the time <span class="math notranslate nohighlight">\(t=x\)</span>
between the first and the <span class="math notranslate nohighlight">\(k^{th}\)</span> event in a Poisson process with mean
<span class="math notranslate nohighlight">\(\lambda\)</span>. The parameter <span class="math notranslate nohighlight">\(k\)</span> influences the shape of the distribution,
whereas <span class="math notranslate nohighlight">\(\lambda\)</span> is just a scale parameter.</p>
</div>
<div class="section" id="student-s-t-distribution-sectionstudentt">
<h3>Student’s <span class="math notranslate nohighlight">\(t\)</span>-distribution {#SectionStudentT}<a class="headerlink" href="#student-s-t-distribution-sectionstudentt" title="Permalink to this headline">¶</a></h3>
<p>The Student’s <span class="math notranslate nohighlight">\(t\)</span> distribution is used when the standard deviation
<span class="math notranslate nohighlight">\(\sigma\)</span> of the parent distribution is unknown and the one evaluated
from the sample <span class="math notranslate nohighlight">\(s\)</span> is used. Suppose that <span class="math notranslate nohighlight">\(x\)</span> is a random variable
distributed normally (mean <span class="math notranslate nohighlight">\(\mu\)</span>, variance <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>), and a
measurement of <span class="math notranslate nohighlight">\(x\)</span> yields the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and the sample
variance <span class="math notranslate nohighlight">\(s^{2}\)</span>. Then the variable <span class="math notranslate nohighlight">\(t\)</span> is defined as
$<span class="math notranslate nohighlight">\(t = \frac{(\bar{x} - \mu) / \sigma}{s / \sigma} = \frac{\bar{x} - \mu}{s}.
\label{DefiningEquationVariableT}\)</span><span class="math notranslate nohighlight">\( You can see that we have canceled
our ignorance about the standard deviation of the parent distribution by
taking the ratio. The net effect is to substitute \)</span>\sigma<span class="math notranslate nohighlight">\( with the
estimated \)</span>s<span class="math notranslate nohighlight">\(.\
The quantity \)</span>t<span class="math notranslate nohighlight">\( follows the p.d.f. \)</span>f_n(t)<span class="math notranslate nohighlight">\( with \)</span>n<span class="math notranslate nohighlight">\( degrees of freedom
(see Fig. [1.12](#DistributionPlotStudentT){reference-type=&quot;ref&quot;
reference=&quot;DistributionPlotStudentT&quot;}):
\)</span><span class="math notranslate nohighlight">\(f_n(t)=\frac{1}{\sqrt{n\pi}}\frac{\Gamma((n+1)/2)}{\Gamma(n/2)}\left(1+\frac{t^2}{n}\right)^{-(n+1)/2}\)</span><span class="math notranslate nohighlight">\(
If the true mean \)</span>\mu<span class="math notranslate nohighlight">\( is known, then \)</span>n = N<span class="math notranslate nohighlight">\( with \)</span>N<span class="math notranslate nohighlight">\( being the number
of measurements, if the true mean is unknown then \)</span>n = N -1<span class="math notranslate nohighlight">\( because one
degree of freedom is used for the sample mean \)</span>\bar{x}$.</p>
<p><img alt="[[DistributionPlotStudentT]]{#DistributionPlotStudentTlabel=&quot;DistributionPlotStudentT&quot;}The Student's distribution fordifferent values of .[wiki]" src="Section2Bilder/student" />{#DistributionPlotStudentT
width=”50%”}\</p>
<p>The distribution depends only on the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and the
sample variance <span class="math notranslate nohighlight">\(s\)</span>.<br />
<br />
In general, the variable: $<span class="math notranslate nohighlight">\(T=\frac{Z}{\sqrt{S^2/n}}\)</span><span class="math notranslate nohighlight">\( is governed by
the Student's t distribution for n degrees of freedom if \)</span>Z<span class="math notranslate nohighlight">\( and \)</span>S^2<span class="math notranslate nohighlight">\(
are two independent random variables following respectively a normal
distribution \)</span>\mathcal{N}(0,1)<span class="math notranslate nohighlight">\( and the \)</span>\chi^2<span class="math notranslate nohighlight">\( distribution
\)</span>\chi^2(n)$ with n degrees of freedom.</p>
</div>
<div class="section" id="f-distribution-sectionfdistributionchapter3">
<h3><span class="math notranslate nohighlight">\(F\)</span> Distribution {#SectionFDistributionChapter3}<a class="headerlink" href="#f-distribution-sectionfdistributionchapter3" title="Permalink to this headline">¶</a></h3>
<p>Consider two random variables, <span class="math notranslate nohighlight">\(\chi_1^2\)</span> and <span class="math notranslate nohighlight">\(\chi_2^2\)</span>, distributed as
<span class="math notranslate nohighlight">\(\chi^2\)</span> with <span class="math notranslate nohighlight">\(\nu_1\)</span> and <span class="math notranslate nohighlight">\(\nu_2\)</span> degrees of freedom, respectively. We
define a new random variable <span class="math notranslate nohighlight">\(F\)</span> as:
$<span class="math notranslate nohighlight">\(F = \frac{\chi_1^2/\nu_1}{\chi_2^2/\nu_2}\)</span><span class="math notranslate nohighlight">\( The random variable \)</span>F<span class="math notranslate nohighlight">\(
follows the distribution (see
Fig. [1.13](#fig:Fdist){reference-type=&quot;ref&quot; reference=&quot;fig:Fdist&quot;}):
\)</span><span class="math notranslate nohighlight">\(f(F)=\left(\frac{n_1}{n_2}\right)^{n_1/2}\cdot \frac{\Gamma((n_1+n_2)/2)}{\Gamma(n_1/2)\Gamma(n_2/2)}\cdot F^{(n_1-2)/2}\left(1+\frac{n_1}{n_2}F\right)^{-(n_1+n_2)/2}.\)</span>$</p>
<p><img alt="[[fig:Fdist]]{#fig:Fdist label=&quot;fig:Fdist&quot;}The F-distributiondistribution for different values of the parameters.[wiki]" src="Section2Bilder/Fdist" />{#fig:Fdist width=”50%”}\</p>
<p>This distribution is known by many names: Fisher-Snedecor distribution,
Fisher distribution, Snedecor distribution, variance ratio distribution,
and <span class="math notranslate nohighlight">\(F\)</span>-distribution. By convention, one usually puts the larger value
on top so that <span class="math notranslate nohighlight">\(F \ge 1\)</span>.<br />
The <span class="math notranslate nohighlight">\(F\)</span> distribution is used to test the statistical compatibility
between the variances of two different samples, which are obtained form
the same underlying distribution (more in
Sec. <a class="reference external" href="#SectionDistributionDependingTests">[SectionDistributionDependingTests]</a>{reference-type=”ref”
reference=”SectionDistributionDependingTests”}).</p>
</div>
<div class="section" id="weibull-distribution">
<h3>Weibull Distribution<a class="headerlink" href="#weibull-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Weibull distribution (see
Fig. <a class="reference external" href="#fig:weibull">1.14</a>{reference-type=”ref” reference=”fig:weibull”})
was originally invented to describe the rate of failures of light bulbs:
$<span class="math notranslate nohighlight">\(P(x;\alpha,\lambda)=\alpha\lambda(\lambda x)^{\alpha-1}e^{-(\lambda x)^{\alpha}}.\)</span>$</p>
<p><img alt="[[fig:weibull]]{#fig:weibull label=&quot;fig:weibull&quot;}The Weibulldistribution for different values of the parameters.[wiki]" src="Section2Bilder/weibull" />{#fig:weibull width=”50%”}\</p>
<p>with <span class="math notranslate nohighlight">\(x\ge 0\)</span> and <span class="math notranslate nohighlight">\(\alpha, \lambda &gt; 0\)</span>. The parameter <span class="math notranslate nohighlight">\(\alpha\)</span> is just
a scale factor and <span class="math notranslate nohighlight">\(\lambda\)</span> describes the width of the maximum. The
exponential distribution is a special case <span class="math notranslate nohighlight">\((\alpha = 1)\)</span>, when the
probability of failure at time t is independent of t. The Weibull
distribution is very useful to describe the reliability and to predict
failure rates. The expectation value of the Weibull distribution is
<span class="math notranslate nohighlight">\(\Gamma(\frac{1}{\alpha}+1) \frac{1}{\lambda}\)</span> and the variance is
<span class="math notranslate nohighlight">\(\frac{1}{\lambda^2} \left( \Gamma\left(\frac{2}{\alpha} +1\right) - \Gamma^2\left(\frac{1}{\alpha} +1\right) \right)\)</span></p>
</div>
<div class="section" id="cauchy-breit-wigner-distribution-subsectioncauchydistribution">
<h3>Cauchy (Breit-Wigner) Distribution {#SubsectionCauchyDistribution}<a class="headerlink" href="#cauchy-breit-wigner-distribution-subsectioncauchydistribution" title="Permalink to this headline">¶</a></h3>
<p>The Cauchy probability density function is:
$<span class="math notranslate nohighlight">\(f(x)=\frac{1}{\pi}\frac{1}{1+x^2}.\)</span><span class="math notranslate nohighlight">\( For large values of \)</span>x<span class="math notranslate nohighlight">\( it
decreases only slowly. Neither the mean nor the variance are defined,
because the corresponding integrals are divergent. The particular Cauchy
distribution of the form
\)</span><span class="math notranslate nohighlight">\(f(m;M,\Gamma)=\frac{1}{2\pi}\frac{\Gamma}{(m-M)^2+(\Gamma/2)^2}\)</span>$</p>
<p><img alt="[[fig:BW]]{#fig:BW label=&quot;fig:BW&quot;}The Breit-Wigner distribution fordifferent values of the parameters.[wiki]" src="Section2Bilder/BW" />{#fig:BW width=”50%”}\</p>
<p>is also called Breit-Wigner function (see
Fig. <a class="reference external" href="#fig:BW">1.15</a>{reference-type=”ref” reference=”fig:BW”}), and is
used in particle physics to describe cross sections near a resonance
with mass <span class="math notranslate nohighlight">\(M\)</span> and width <span class="math notranslate nohighlight">\(\Gamma\)</span><a class="footnote-reference brackets" href="#id8" id="id4">4</a>. The Breit-Wigner comes as the
Fourier transformation of the wave function of an unstable particle:
$<span class="math notranslate nohighlight">\(\psi(t) \propto e^{-i E_i t /\hbar} e^{-\Gamma t /2}\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(\phi(\omega)\propto \int_0^\infty \psi(t) e^{i \omega t} dt = \frac{i}{(\omega - \omega_0 + i \frac{\Gamma}{2}) }\)</span><span class="math notranslate nohighlight">\(
which squared gives:
\)</span><span class="math notranslate nohighlight">\(|\phi(\omega)|^2 = \frac{1}{(\omega-\omega_0)^2 + \frac{\Gamma^2}{4}}\)</span>$</p>
</div>
<div class="section" id="landau-distribution">
<h3>Landau Distribution<a class="headerlink" href="#landau-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Landau distribution (see
Fig. <a class="reference external" href="#fig:Landau">1.16</a>{reference-type=”ref” reference=”fig:Landau”})
is used to describe the distribution of the energy loss <span class="math notranslate nohighlight">\(x\)</span>
(dimensionless quantity proportional to <span class="math notranslate nohighlight">\(dE\)</span>) of a charged particle (by
ionisation) passing through a thin layer of matter:
$<span class="math notranslate nohighlight">\(p(x) = \frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty} \exp(s\log s + xs)ds \qquad  (c &gt; 0).\)</span><span class="math notranslate nohighlight">\(
The long tail towards large energies models the large energy loss
fluctuations in thin layers. The mean and the variance of the
distribution are not defined.\
Often it can be found approximated as:
\)</span><span class="math notranslate nohighlight">\(p(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} \left( x+ e^{-x} \right) }\)</span>$</p>
<p><img alt="[[fig:Landau]]{#fig:Landau label=&quot;fig:Landau&quot;}Straggling functionsin silicon for 500 MeV pions, normalized to unity at the most probablevalue . The width  is the full width at halfmaximum. [&#64;PDGStatistics]" src="Section2Bilder/landau" />{#fig:Landau
width=”50%”}\</p>
</div>
<div class="section" id="crystal-ball">
<h3>Crystal Ball<a class="headerlink" href="#crystal-ball" title="Permalink to this headline">¶</a></h3>
<p>The Crystal Ball is typically used to describe the invariant mass
distribution of a resonance in which the decay products are affected by
energy losses. An example could be a <span class="math notranslate nohighlight">\(Z\to e^+ e^-\)</span> where the electrons
in the final state lose energy by brehmstralung. The distribution is
characterized by a Gaussian core with a power law tail (see
Fig.<a class="reference external" href="#fig:crystalball">1.17</a>{reference-type=”ref”
reference=”fig:crystalball”} $<span class="math notranslate nohighlight">\(f(x) = N \cdot
\begin{cases}
    e^{-\frac{(x-\mu)^2}{2\sigma^2}} ,                    &amp; \text{if } \frac{x-\mu}{\sigma} &gt; -\alpha \\
    A \cdot \left( B - \frac{x-\mu}{\sigma}\right)^{-n},  &amp;  \text{if } \frac{x-\mu}{\sigma} \leq -\alpha
\end{cases}\)</span><span class="math notranslate nohighlight">\( where: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 A &amp;=&amp; \left( \frac{n}{|\alpha|} \right)^n \cdot \exp\left( -\frac{|\alpha|^2}{2}\right)     \\
 B &amp;=&amp; \frac{n}{|\alpha|} - |\alpha|\end{aligned}\)</span><span class="math notranslate nohighlight">\( A trivial extension
is given by the &quot;Double Crystal Ball&quot; where a second exponential tail is
added on the high side. \)</span><span class="math notranslate nohighlight">\(f(x) = N \cdot
\begin{cases}
    e^{-\frac{(x-\mu)^2}{2\sigma^2}} ,                      &amp;  \text{if } \alpha_1 &lt; \frac{x-\mu}{\sigma} &lt; \alpha_2 \\
    A \cdot \left( B - \frac{x-\mu}{\sigma}\right)^{-n_1},  &amp;  \text{if } \frac{x-\mu}{\sigma} \leq \alpha_1\\
    C \cdot \left( D - \frac{x-\mu}{\sigma}\right)^{-n_2},  &amp;  \text{if } \frac{x-\mu}{\sigma} \geq \alpha_2
\end{cases}\)</span><span class="math notranslate nohighlight">\( where: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 A &amp;=&amp; \left( \frac{n_1}{|\alpha_1|} \right)^{n_1} \cdot \exp\left( -\frac{|\alpha_1|^2}{2}\right)     \\
 B &amp;=&amp; \frac{n_1}{|\alpha_1|} - |\alpha_1| \\
 C &amp;=&amp; \left( \frac{n_2}{|\alpha_2|} \right)^{n_2} \cdot \exp\left( -\frac{|\alpha_2|^2}{2}\right)     \\
 D &amp;=&amp; \frac{n_2}{|\alpha_2|} - |\alpha_2|\end{aligned}\)</span>$ In case you
are wondering, the name comes from the collaboration that first used
this function to describe a signal distribution. [&#64;crystalBall].</p>
<p><img alt="[[fig:crystalball]]{#fig:crystalball label=&quot;fig:crystalball&quot;}Crystalball distribution.[wiki]" src="Section2Bilder/crystalball" />{#fig:crystalball
width=”50%”}\</p>
</div>
</div>
<div class="section" id="characteristic-function-sec-characteristic">
<h2>Characteristic Function {#sec:characteristic}<a class="headerlink" href="#characteristic-function-sec-characteristic" title="Permalink to this headline">¶</a></h2>
<p>The <em>characteristic function</em> <span class="math notranslate nohighlight">\(\Phi(t)\)</span> is defined as the expectation
value of <span class="math notranslate nohighlight">\(e^{itx}\)</span> for a p.d.f. <span class="math notranslate nohighlight">\(f(x)\)</span>:
$<span class="math notranslate nohighlight">\(\Phi(t):=&lt;e^{itx}&gt;=\int e^{itx}\cdot f(x)dx\)</span><span class="math notranslate nohighlight">\( i.e \)</span>\Phi(t)<span class="math notranslate nohighlight">\( is the
Fourier integral of f(x). The characteristic function completely
determines the p.d.f., since by inverting the Fourier transformation we
regain \)</span>f(x)<span class="math notranslate nohighlight">\(: \)</span><span class="math notranslate nohighlight">\(f(x)=\frac{1}{2\pi}\int e^{-itx}\cdot \Phi(t)dt\)</span><span class="math notranslate nohighlight">\( The
characteristic function as well as its first and second derivative are
readily calculated for the special case where \)</span>t=0<span class="math notranslate nohighlight">\(: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\Phi(0)&amp;=&amp;1 \\
\frac{d\Phi(0)}{dt}&amp;=&amp;i&lt;x&gt; \\
\frac{d^2\Phi(0)}{dt^2}&amp;=&amp;-(\sigma^2+&lt;x&gt;^2)\end{aligned}\)</span><span class="math notranslate nohighlight">\( What do we
need characteristic functions for? They may be useful when performing
calculations with probability densities, for example if the convolution
of two probability densities \)</span>f_{1}<span class="math notranslate nohighlight">\( and \)</span>f_{2}<span class="math notranslate nohighlight">\( for two random
variables \)</span>x_{1}<span class="math notranslate nohighlight">\( and \)</span>x_{2}<span class="math notranslate nohighlight">\( should be calculated. A convolution of
\)</span>f_{1}<span class="math notranslate nohighlight">\( and \)</span>f_{2}<span class="math notranslate nohighlight">\( yields a new probability density \)</span>g(y)<span class="math notranslate nohighlight">\(, according
to which the sum of the random variable \)</span>y = x_{1} + x_{2}<span class="math notranslate nohighlight">\( is
distributed:
\)</span><span class="math notranslate nohighlight">\(g(y)=\int \int f_1(x_1)f_2(x_2)\delta(y-x_1-x_2)dx_1dx_2=\int f_1(x_1)f_2(y-x_1)dx_1=\int f_2(x_2)f_1(y-x_2)dx_2\)</span><span class="math notranslate nohighlight">\(
The convolution integral can now be transformed with the help of the
characteristic functions: \)</span><span class="math notranslate nohighlight">\(\Phi_g(t)=\Phi_{f_1}(t)\cdot\Phi_{f_2}(t)\)</span><span class="math notranslate nohighlight">\(
In words: the characteristic function of the convolution of two
variables is obtained by the product of their characteristic functions.
Thus it can be easily shown that the convolution of two Gaussian
distributions with \)</span>\mu_{1,2}<span class="math notranslate nohighlight">\( and \)</span>\sigma_{1,2}<span class="math notranslate nohighlight">\( is again a Gaussian
distribution with \)</span>\mu = \mu_{1} + \mu_{2}<span class="math notranslate nohighlight">\( and
\)</span>\sigma^{2} = \sigma_{1}^{2} + \sigma_{2}^{2}$. Furthermore, the
convolution of two Poisson distributions is again a Poisson
distribution. The characteristic functions of some probability densities
are given in Tab. <a class="reference external" href="#tab2_sec2">1.2</a>{reference-type=”ref”
reference=”tab2_sec2”}.</p>
</div>
<div class="section" id="the-central-limit-theorem-sec-clt">
<h2>The Central Limit Theorem {#sec:CLT}<a class="headerlink" href="#the-central-limit-theorem-sec-clt" title="Permalink to this headline">¶</a></h2>
<p>The “Central Limit Theorem” (CLT) is probably the most important theorem
in statistics and it is the reason why the Gaussian distribution is so
important.<br />
Take <span class="math notranslate nohighlight">\(n\)</span> independent variables <span class="math notranslate nohighlight">\(x_i\)</span>, distributed according to p.d.f.’s
<span class="math notranslate nohighlight">\(f_i\)</span> having mean <span class="math notranslate nohighlight">\(\mu_i\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, then the p.d.f. of
the sum of the <span class="math notranslate nohighlight">\(x_i\)</span>, <span class="math notranslate nohighlight">\(S=\sum x_i\)</span>, has mean <span class="math notranslate nohighlight">\(\sum \mu_i\)</span> and variance
<span class="math notranslate nohighlight">\(\sum\sigma_i^2\)</span> and it approaches the normal p.d.f.
<span class="math notranslate nohighlight">\(N(S; \sum \mu_i, \sum \sigma_i^2)\)</span> as <span class="math notranslate nohighlight">\(n\to \infty\)</span>.<br />
The CLT holds under pretty general conditions:</p>
<ul class="simple">
<li><p>both mean and variance have to exist for each of the random
variables in the sum</p></li>
<li><p>Lindeberg criteria: $<span class="math notranslate nohighlight">\(\begin{aligned}
&amp;y_k =  x_k,\quad {\rm if}\, |x_k-\mu_k|\le \epsilon_k\sigma_k \\
&amp;y_k =   0,\quad {\rm if}\, |x_k-\mu_k|&gt; \epsilon_k\sigma_k.\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Here, \)</span>\epsilon_{k}<span class="math notranslate nohighlight">\( is an arbitrary number. If the variance
\)</span>(y_{1}+y_{2}+\cdots y_{n})/ \sigma_{y}^{2} \to 1<span class="math notranslate nohighlight">\( for
\)</span>n \to \infty$, then this condition is fulfilled for the CLT. In
plain English: The Lindeberg criteria ensures that fluctuations of a
single variable does not dominate its sum.</p></li>
</ul>
<p>An example of convergence for the CLT is given in
Fig. <a class="reference external" href="#fig:CLT">1.18</a>{reference-type=”ref” reference=”fig:CLT”} where a
uniform distribution is used for 4 iterations.</p>
<p><img alt="[[fig:CLT]]{#fig:CLT label=&quot;fig:CLT&quot;}Sum of random variables from auniform distribution in  after the iterations 1 to4." src="Section2Bilder/CLT" />{#fig:CLT width=”50%”}\</p>
<p>When performing measurements the value obtained is usually affected by a
large number of (hopefully) small uncertainties. If this number of small
contributions is large the C.L.T. tells us that their total sum is
Gaussian distributed. This is often the case and is the reason
resolution functions are usually Gaussian. But if there are only a few
contributions, or if a few of the contributions are much larger than the
rest, the C.L.T. is not applicable, and the sum is not necessarily
Gaussian.<br />
<br />
<strong>Example</strong>The inverse transverse momentum <span class="math notranslate nohighlight">\(q/p_T\)</span>, with q the charge of
the particle, can be deduced from the measured sagitta. <span class="math notranslate nohighlight">\(Q/p_T\)</span> has
Gaussian errors, not <span class="math notranslate nohighlight">\(p_T\)</span> !</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Pretty much every book about statistics/probability will cover the
material of this chapter. Here are a few examples:</p>
<ul class="simple">
<li><p>L. Lyons [&#64;Lyons], “Statistics for Nuclear and Particle Physicist”:
Ch. 3</p></li>
<li><p>W. Metzger [&#64;Metzger], “Statistical Methods in Data Analysis”: Ch.2</p></li>
<li><p>PDG [&#64;PDG], Probability</p></li>
<li><p>PDG [&#64;PDG], Passage of particles through matter</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>This example is mentioned for the first time in the book from L.
von Bortkiewicz in the year 1898: “Das Gesetz der kleinen Zahlen.”</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>C.F. Gauss did not discovery it all alone. Independently, Laplace
and de Moivre knew about this distribution.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>A legend says that Gauss did describe the size of bread loaves in
the city of Königsberg with the normal distribution.</p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Even if the mean does not exist, noting that the distribution is
symmetric, we can define it to be <span class="math notranslate nohighlight">\(M\)</span>; <span class="math notranslate nohighlight">\(\Gamma\)</span> is the FWHM. In
actual physical problems the distribution is truncated, e.g., by
energy conservation, and the resulting distribution is well-behaved.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="probability.html" title="previous page">Probability</a>
    <a class='right-next' id="next-link" href="notebooks.html" title="next page">Jupyter Notebook files</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mauro Donega<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>